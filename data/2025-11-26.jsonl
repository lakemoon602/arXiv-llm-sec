{"id": "2511.17762", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.17762", "abs": "https://arxiv.org/abs/2511.17762", "authors": ["Henning Femmer", "Ivan Esau"], "title": "The Software Engineering Simulations Lab: Agentic AI for RE Quality Simulations", "comment": null, "summary": "Context and motivation. Quality in Requirements Engineering (RE) is still predominantly anecdotal and intuition-driven. Creating a solid requirements quality model requires broad sets of empirical evidence to evaluate quality factors and their context. Problem. However, empirical data on the detailed effects of requirements quality defects is scarce, since it is costly to obtain. Furthermore, with the advent of AI-based development, the requirements quality factors may change: Requirements are no longer only consumed by humans, but increasingly also by AI agents, which might lead to a different efficient and effective requirements style. Principal ideas. We propose to extend the RE research toolbox with Agentic AI simulations, in which software engineering (SE) processes are replicated by standardized agents in stochastic, dynamic, event-driven, qualitative simulations. We argue that their speed and simplicity makes them a valuable addition to RE research, although limitations in replicating human behavior need to be studied and understood. Contribution. This paper contributes a first concept, a research roadmap, a prototype, and a first feasibility study for RE simulations with agentic AI. Study results indicate that even a naive implementation leads to executable simulations, encouraging technical improvements along with broader application in RE research."}
{"id": "2511.17836", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.17836", "abs": "https://arxiv.org/abs/2511.17836", "authors": ["Edwin Sundberg", "Thea Ekmark", "Workneh Yilma Ayele"], "title": "Validating API Design Requirements for Interoperability: A Static Analysis Approach Using OpenAPI", "comment": "11 pages, 3 tables, 2 figures. Preprint. To appear in: PoEM2025: Companion Proceedings of the 18th IFIP Working Conference on the Practice of Enterprise Modeling: PoEM Forum, Doctoral Consortium, Business Case and Tool Forum, Workshops, December 3-5, 2025, Geneva, Switzerland", "summary": "RESTful APIs are central in developing interoperable, modular, and maintainable software systems in enterprises today. Also, it is essential to support system evolution, service interoperability, and governance across organizational boundaries to ensure good quality and consistency of these APIs. However, evaluating API design quality, which is part of non-functional requirement tasks, remains a largely manual and ad hoc process, particularly during early development. Using a Design Science Research (DSR) methodology, we elicited user needs, identified 75 API design rules using a literature review, and implemented a configurable rule engine to detect structural violations in OpenAPI specifications. The proposed tool supports organizational adaptability by allowing rules to be customized, enabled, or disabled, enabling integration of domain-specific standards. The evaluation was conducted through structured experiments and thematic analysis involving industry experts. API quality validation contributes to aligning technical designs with requirements and enterprise architecture by strengthening interoperability and governance between enterprise systems. The results show that S.E.O.R.A facilitates early validation of non-functional API requirements, provides actionable and traceable feedback, and aligns well with requirements elicitation and quality assurance processes. It improves the API design process by automating checks that would otherwise require manual inspection, thus supporting consistent and reusable conformance practices. This work contributes to requirements engineering by operationalizing design principles as verifiable constraints and embedding them into a practical validation tool. Future directions include IDE integration, expanded rule coverage, and real-world deployment to support continuous compliance in agile API development lifecycles."}
{"id": "2511.17853", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17853", "abs": "https://arxiv.org/abs/2511.17853", "authors": ["SunMin Moon", "Jangwon Gim", "Chaerin Kim", "Yeeun Kim", "YoungJoo Kim", "Kang Choi"], "title": "A Low-Code Methodology for Developing AI Kiosks: a Case Study with the DIZEST Platform", "comment": "5 pages, 2 figures, conference, 2 tables", "summary": "This paper presents a comprehensive study on enhancing kiosk systems through a low-code architecture, with a focus on AI-based implementations. Modern kiosk systems are confronted with significant challenges, including a lack of integration, structural rigidity, performance bottlenecks, and the absence of collaborative frameworks. To overcome these limitations, we propose a DIZEST-based approach methodology, a specialized low-code platform that enables intuitive workflow design and seamless AI integration. Through a comparative analysis with existing platforms, including Jupyter Notebook, ComfyUI, and Orange3, we demonstrate that DIZEST delivers superior performance across key evaluation criteria. Our photo kiosk case study further validates the effectiveness of this approach in improving interoperability, enhancing user experience, and increasing deployment flexibility."}
{"id": "2511.17977", "categories": ["cs.SE", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.17977", "abs": "https://arxiv.org/abs/2511.17977", "authors": ["Kuangxiangzi Liu", "Dhiman Chakraborty", "Alexander Liggesmeyer", "Andreas Zeller"], "title": "Synthesizing Precise Protocol Specs from Natural Language for Effective Test Generation", "comment": null, "summary": "Safety- and security-critical systems have to be thoroughly tested against their specifications. The state of practice is to have _natural language_ specifications, from which test cases are derived manually - a process that is slow, error-prone, and difficult to scale. _Formal_ specifications, on the other hand, are well-suited for automated test generation, but are tedious to write and maintain. In this work, we propose a two-stage pipeline that uses large language models (LLMs) to bridge the gap: First, we extract _protocol elements_ from natural-language specifications; second, leveraging a protocol implementation, we synthesize and refine a formal _protocol specification_ from these elements, which we can then use to massively test further implementations.\n  We see this two-stage approach to be superior to end-to-end LLM-based test generation, as 1. it produces an _inspectable specification_ that preserves traceability to the original text; 2. the generation of actual test cases _no longer requires an LLM_; 3. the resulting formal specs are _human-readable_, and can be reviewed, version-controlled, and incrementally refined; and 4. over time, we can build a _corpus_ of natural-language-to-formal-specification mappings that can be used to further train and refine LLMs for more automatic translations.\n  Our prototype, AUTOSPEC, successfully demonstrated the feasibility of our approach on five widely used _internet protocols_ (SMTP, POP3, IMAP, FTP, and ManageSieve) by applying its methods on their _RFC specifications_ written in natural-language, and the recent _I/O grammar_ formalism for protocol specification and fuzzing. In its evaluation, AUTOSPEC recovers on average 92.8% of client and 80.2% of server message types, and achieves 81.5% message acceptance across diverse, real-world systems."}
{"id": "2511.17569", "categories": ["cs.SI", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2511.17569", "abs": "https://arxiv.org/abs/2511.17569", "authors": ["Huilin Wang Wenjun Zhang Weibing Deng"], "title": "Diffusion Signals Reveal Hidden Connections: A Physics-Inspired Framework for Link Prediction via Personalized PageRank Signals", "comment": null, "summary": "Link prediction in complex networks--identifying the missing or future connections--remains a cornerstone problem for understanding network evolution and function, yet existing methods struggle to balance computational efficiency with theoretical rigor across heterogeneous topologies. This work introduces a physically principled framework, Diffusion Distance with Personalized PageRank (D-PPR), which unifies static topology with dynamic information flow by modeling nodes as signal sources propagating through the network via Personalized PageRank (PPR) vectors. The method quantifies node-pair similarity through the graph Laplacian-governed diffusion distance between their topology-aware signal distributions, thereby bridging microscopic interactions with macroscopic network dynamics. Systematic benchmarking on synthetic (Barabási-Albert, LFR) and seven large-scale real-world networks spanning technology, biology, and social domains demonstrates that D-PPR achieves highly competitive performance, yielding favorable results when compared to representative local and global heuristics, particularly in sparse and modular networks. These findings establish a rigorous foundation for physics-inspired link prediction by revealing that incorporating dynamical processes into structural similarity metrics enables deeper insights into network connectivity patterns, offering both methodological advances and new theoretical perspectives on the interplay between topology and dynamics."}
{"id": "2511.17554", "categories": ["cs.CY", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.17554", "abs": "https://arxiv.org/abs/2511.17554", "authors": ["Sumon Kanti Dey", "Manvi S", "Zeel Mehta", "Meet Shah", "Unnati Agrawal", "Suhani Jalota", "Azra Ismail"], "title": "Beyond the Rubric: Cultural Misalignment in LLM Benchmarks for Sexual and Reproductive Health", "comment": "https://github.com/Sumon/healthbench-srh-eval/", "summary": "Large Language Models (LLMs) have been positioned as having the potential to expand access to health information in the Global South, yet their evaluation remains heavily dependent on benchmarks designed around Western norms. We present insights from a preliminary benchmarking exercise with a chatbot for sexual and reproductive health (SRH) for an underserved community in India. We evaluated using HealthBench, a benchmark for conversational health models by OpenAI. We extracted 637 SRH queries from the dataset and evaluated on the 330 single-turn conversations. Responses were evaluated using HealthBench's rubric-based automated grader, which rated responses consistently low. However, qualitative analysis by trained annotators and public health experts revealed that many responses were actually culturally appropriate and medically accurate. We highlight recurring issues, particularly a Western bias, such as for legal framing and norms (e.g., breastfeeding in public), diet assumptions (e.g., fish safe to eat during pregnancy), and costs (e.g., insurance models). Our findings demonstrate the limitations of current benchmarks in capturing the effectiveness of systems built for different cultural and healthcare contexts. We argue for the development of culturally adaptive evaluation frameworks that meet quality standards while recognizing needs of diverse populations."}
{"id": "2511.17666", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17666", "abs": "https://arxiv.org/abs/2511.17666", "authors": ["Tom Perel"], "title": "Evaluating Adversarial Vulnerabilities in Modern Large Language Models", "comment": null, "summary": "The recent boom and rapid integration of Large Language Models (LLMs) into a wide range of applications warrants a deeper understanding of their security and safety vulnerabilities. This paper presents a comparative analysis of the susceptibility to jailbreak attacks for two leading publicly available LLMs, Google's Gemini 2.5 Flash and OpenAI's GPT-4 (specifically the GPT-4o mini model accessible in the free tier). The research utilized two main bypass strategies: 'self-bypass', where models were prompted to circumvent their own safety protocols, and 'cross-bypass', where one model generated adversarial prompts to exploit vulnerabilities in the other. Four attack methods were employed - direct injection, role-playing, context manipulation, and obfuscation - to generate five distinct categories of unsafe content: hate speech, illegal activities, malicious code, dangerous content, and misinformation. The success of the attack was determined by the generation of disallowed content, with successful jailbreaks assigned a severity score. The findings indicate a disparity in jailbreak susceptibility between 2.5 Flash and GPT-4, suggesting variations in their safety implementations or architectural design. Cross-bypass attacks were particularly effective, indicating that an ample amount of vulnerabilities exist in the underlying transformer architecture. This research contributes a scalable framework for automated AI red-teaming and provides data-driven insights into the current state of LLM safety, underscoring the complex challenge of balancing model capabilities with robust safety mechanisms."}
{"id": "2511.17553", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.17553", "abs": "https://arxiv.org/abs/2511.17553", "authors": ["Jason M. Pittman", "Anton Phillips", "Yesenia Medina-Santos", "Brielle C. Stark"], "title": "Practical Machine Learning for Aphasic Discourse Analysis", "comment": "14 pages, 4 tables, 2 figures", "summary": "Analyzing spoken discourse is a valid means of quantifying language ability in persons with aphasia. There are many ways to quantify discourse, one common way being to evaluate the informativeness of the discourse. That is, given the total number of words produced, how many of those are context-relevant and accurate. This type of analysis is called Correct Information Unit (CIU) analysis and is one of the most prevalent discourse analyses used by speech-language pathologists (SLPs). Despite this, CIU analysis in the clinic remains limited due to the manual labor needed by SLPs to code and analyze collected speech. Recent advances in machine learning (ML) seek to augment such labor by automating modeling of propositional, macrostructural, pragmatic, and multimodal dimensions of discourse. To that end, this study evaluated five ML models for reliable identification of Correct Information Units (CIUs, Nicholas & Brookshire, 1993), during a picture description task. The five supervised ML models were trained using randomly selected human-coded transcripts and accompanying words and CIUs from persons with aphasia. The baseline model training produced a high accuracy across transcripts for word vs non-word, with all models achieving near perfect performance (0.995) with high AUC range (0.914 min, 0.995 max). In contrast, CIU vs non-CIU showed a greater variability, with the k-nearest neighbor (k-NN) model the highest accuracy (0.824) and second highest AUC (0.787). These findings indicate that while the supervised ML models can distinguish word from not word, identifying CIUs is challenging."}
{"id": "2511.17541", "categories": ["cs.AI", "cs.IT", "cs.LO"], "pdf": "https://arxiv.org/pdf/2511.17541", "abs": "https://arxiv.org/abs/2511.17541", "authors": ["Seyma Yaman Kayadibi"], "title": "Leibniz's Monadology as Foundation for the Artificial Age Score: A Formal Architecture for Al Memory Evaluation", "comment": null, "summary": "This paper develops a mathematically rigorous, philosophically grounded framework for evaluating artificial memory systems, rooted in the metaphysical structure of Leibniz's Monadology. Building on a previously formalized metric, the Artificial Age Score (AAS), the study maps twenty core propositions from the Monadology to an information-theoretic architecture. In this design, each monad functions as a modular unit defined by a truth score, a redundancy parameter, and a weighted contribution to a global memory penalty function. Smooth logarithmic transformations operationalize these quantities and yield interpretable, bounded metrics for memory aging, representational stability, and salience. Classical metaphysical notions of perception, apperception, and appetition are reformulated as entropy, gradient dynamics, and internal representation fidelity. Logical principles, including the laws of non-contradiction and sufficient reason, are encoded as regularization constraints guiding memory evolution. A central contribution is a set of first principles proofs establishing refinement invariance, structural decomposability, and monotonicity under scale transformation, aligned with the metaphysical structure of monads. The framework's formal organization is structured into six thematic bundles derived from Monadology, aligning each mathematical proof with its corresponding philosophical domain. Beyond evaluation, the framework offers a principled blueprint for building Al memory architectures that are modular, interpretable, and provably sound."}
{"id": "2511.18001", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.18001", "abs": "https://arxiv.org/abs/2511.18001", "authors": ["Jiaolong Kong", "Xiaofei Xie", "Yiheng Xiong", "Yuekun Wang", "Jian Wang"], "title": "Enhancing Automated Program Repair via Faulty Token Localization and Quality-Aware Patch Refinement", "comment": null, "summary": "Large language models (LLMs) have recently demonstrated strong potential for automated program repair (APR). However, existing LLM-based techniques primarily rely on coarse-grained external feedback (e.g.,test results) to guide iterative patch generation, while lacking fine-grained internal signals that reveal why a patch fails or which parts of the generated code are likely incorrect. This limitation often leads to inefficient refinement, error propagation, and suboptimal repair performance. In this work, we propose TokenRepair, a novel two-level refinement framework that enhances APR by integrating internal reflection for localizing potentially faulty tokens with external feedback for quality-aware patch refinement. Specifically, TokenRepair first performs internal reflection by analyzing context-aware token-level uncertainty fluctuations to identify suspicious or low-confidence tokens within a patch. It then applies Chain-of-Thought guided rewriting to refine only these localized tokens, enabling targeted and fine-grained correction. To further stabilize the iterative repair loop, TokenRepair incorporates a quality-aware external feedback mechanism that evaluates patch quality and filters out low-quality candidates before refinement. Experimental results show that TokenRepair achieves new state-of-the-art repair performance, correctly fixing 88 bugs on Defects4J 1.2 and 139 bugs on HumanEval-Java, demonstrating substantial improvements ranging from 8.2% to 34.9% across all models on Defects4J 1.2 and from 3.3% to 16.1% on HumanEval-Java."}
{"id": "2511.17574", "categories": ["cs.SI", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.17574", "abs": "https://arxiv.org/abs/2511.17574", "authors": ["Eamon Earl", "Chen Ding", "Richard Valenzano", "Drai Paulen-Patterson"], "title": "Constructing Political Coordinates: Aggregating Over the Opposition for Diverse News Recommendation", "comment": "Due to appear in the proceedings of the 2025 IEEE International Conference on Big Data", "summary": "In the past two decades, open access to news and information has increased rapidly, empowering educated political growth within democratic societies. News recommender systems (NRSs) have shown to be useful in this process, minimizing political disengagement and information overload by providing individuals with articles on topics that matter to them. Unfortunately, NRSs often conflate underlying user interest with the partisan bias of the articles in their reading history and with the most popular biases present in the coverage of their favored topics. Over extended interaction, this can result in the formation of filter bubbles and the polarization of user partisanship. In this paper, we propose a novel embedding space called Constructed Political Coordinates (CPC), which models the political partisanship of users over a given topic-space, relative to a larger sample population. We apply a simple collaborative filtering (CF) framework using CPC-based correlation to recommend articles sourced from oppositional users, who have different biases from the user in question. We compare against classical CF methods and find that CPC-based methods promote pointed bias diversity and better match the true political tolerance of users, while classical methods implicitly exploit biases to maximize interaction."}
{"id": "2511.17591", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2511.17591", "abs": "https://arxiv.org/abs/2511.17591", "authors": ["Quan-Hoang Vuong", "Fatemeh Kianfar", "Thi Mai Anh Tran", "Ni Putu Wulan Purnama Sari", "Cresensia Dina Candra Kumaladewi", "Viet-Phuong La", "Minh-Hoang Nguyen"], "title": "Do Environment-Modification Behaviors and Gamers' Immersiveness Shape Exceptionalism Beliefs?", "comment": null, "summary": "Human exceptionalism strongly shapes human-nature perceptions, thinking, values, and behaviors. Yet little is known about how virtual ecological environments influence this mindset. As digital worlds become increasingly immersive and ecologically sophisticated, they provide novel contexts for examining how human value systems are formed and transformed. This study investigates how virtual environment-modification behaviors and players' sense of immersiveness jointly shape exceptionalism, drawing on worldviews from quantum mechanics and mathematical logic. Using Granular Interaction Thinking Theory (GITT) and the Bayesian Mindsponge Framework (BMF analytics), we analyze five key activities--tree planting, flower planting, flower crossbreeding, terraforming, and creating conditions for bug respawn--based on a multinational dataset of 640 Animal Crossing: New Horizons players from 29 countries. Results reveal two behavioral clusters distinguished by controllability. High-controllability behaviors (i.e., flower planting and terraforming) predict higher exceptionalism, whereas the flower-planting effect reverses among highly immersed players. Low-controllability behaviors (i.e., flower crossbreeding and manipulating bug spawning) predict lower exceptionalism, but these associations weaken or reverse under high immersiveness, respectively. These findings offer insights into leveraging virtual worlds to cultivate Nature Quotient (NQ), mitigate exceptionalist tendencies, and foster eco-surplus cultural orientations."}
{"id": "2511.17671", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.17671", "abs": "https://arxiv.org/abs/2511.17671", "authors": ["Atharv Singh Patlan", "Peiyao Sheng", "S. Ashwin Hebbar", "Prateek Mittal", "Pramod Viswanath"], "title": "MURMUR: Using cross-user chatter to break collaborative language agents in groups", "comment": "20 pages, 7 figures", "summary": "Language agents are rapidly expanding from single-user assistants to multi-user collaborators in shared workspaces and groups. However, today's language models lack a mechanism for isolating user interactions and concurrent tasks, creating a new attack vector inherent to this new setting: cross-user poisoning (CUP). In a CUP attack, an adversary injects ordinary-looking messages that poison the persistent, shared state, which later triggers the agent to execute unintended, attacker-specified actions on behalf of benign users. We validate CUP on real systems, successfully attacking popular multi-user agents. To study the phenomenon systematically, we present MURMUR, a framework that composes single-user tasks into concurrent, group-based scenarios using an LLM to generate realistic, history-aware user interactions. We observe that CUP attacks succeed at high rates and their effects persist across multiple tasks, thus posing fundamental risks to multi-user LLM deployments. Finally, we introduce a first-step defense with task-based clustering to mitigate this new class of vulnerability"}
{"id": "2511.17564", "categories": ["cs.LG", "astro-ph.IM", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17564", "abs": "https://arxiv.org/abs/2511.17564", "authors": ["Guilherme Grancho D. Fernandes", "Marco A. Barroca", "Mateus dos Santos", "Rafael S. Oliveira"], "title": "Classification of Transient Astronomical Object Light Curves Using LSTM Neural Networks", "comment": "12 pages, 11 figures, 2 tables", "summary": "This study presents a bidirectional Long Short-Term Memory (LSTM) neural network for classifying transient astronomical object light curves from the Photometric LSST Astronomical Time-series Classification Challenge (PLAsTiCC) dataset. The original fourteen object classes were reorganized into five generalized categories (S-Like, Fast, Long, Periodic, and Non-Periodic) to address class imbalance. After preprocessing with padding, temporal rescaling, and flux normalization, a bidirectional LSTM network with masking layers was trained and evaluated on a test set of 19,920 objects. The model achieved strong performance for S-Like and Periodic classes, with ROC area under the curve (AUC) values of 0.95 and 0.99, and Precision-Recall AUC values of 0.98 and 0.89, respectively. However, performance was significantly lower for Fast and Long classes (ROC AUC of 0.68 for Long class), and the model exhibited difficulty distinguishing between Periodic and Non-Periodic objects. Evaluation on partial light curve data (5, 10,and 20 days from detection) revealed substantial performance degradation, with increased misclassification toward the S-Like class. These findings indicate that class imbalance and limited temporal information are primary limitations, suggesting that class balancing strategies and preprocessing techniques focusing on detection moments could improve performance."}
{"id": "2511.17643", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17643", "abs": "https://arxiv.org/abs/2511.17643", "authors": ["Yayan Qiu", "Sean Hanna"], "title": "Fluid Grey 2: How Well Does Generative Adversarial Network Learn Deeper Topology Structure in Architecture That Matches Images?", "comment": null, "summary": "Taking into account the regional characteristics of intrinsic and extrinsic properties of space is an essential issue in architectural design and urban renewal, which is often achieved step by step using image and graph-based GANs. However, each model nesting and data conversion may cause information loss, and it is necessary to streamline the tools to facilitate architects and users to participate in the design. Therefore, this study hopes to prove that I2I GAN also has the potential to recognize topological relationships autonomously. Therefore, this research proposes a method for quickly detecting the ability of pix2pix to learn topological relationships, which is achieved by adding two Grasshopper-based detection modules before and after GAN. At the same time, quantitative data is provided and its learning process is visualized, and changes in different input modes such as greyscale and RGB affect its learning efficiency. There are two innovations in this paper: 1) It proves that pix2pix can automatically learn spatial topological relationships and apply them to architectural design. 2) It fills the gap in detecting the performance of Image-based Generation GAN from a topological perspective. Moreover, the detection method proposed in this study takes a short time and is simple to operate. The two detection modules can be widely used for customizing image datasets with the same topological structure and for batch detection of topological relationships of images. In the future, this paper may provide a theoretical foundation and data support for the application of architectural design and urban renewal that use GAN to preserve spatial topological characteristics."}
{"id": "2511.18038", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18038", "abs": "https://arxiv.org/abs/2511.18038", "authors": ["Xiaoke Han", "Hong Zhu"], "title": "MASTEST: A LLM-Based Multi-Agent System For RESTful API Tests", "comment": "14 Page of main text plus 4 pages of appendix", "summary": "Testing RESTful API is increasingly important in quality assurance of cloud-native applications. Recent advances in machine learning (ML) techniques have demonstrated that various testing activities can be performed automatically by large language models (LLMs) with reasonable accuracy. This paper develops a multi-agent system called MASTEST that combines LLM-based and programmed agents to form a complete tool chain that covers the whole workflow of API test starting from generating unit and system test scenarios from API specification in the OpenAPI Swagger format, to generating of Pytest test scripts, executing test scripts to interact with web services, to analysing web service response messages to determine test correctness and calculate test coverage. The system also supports the incorporation of human testers in reviewing and correcting LLM generated test artefacts to ensure the quality of testing activities. MASTEST system is evaluated on two LLMs, GPT-4o and DeepSeek V3.1 Reasoner with five public APIs. The performances of LLMs on various testing activities are measured by a wide range of metrics, including unit and system test scenario coverage and API operation coverage for the quality of generated test scenarios, data type correctness, status code coverage and script syntax correctness for the quality of LLM generated test scripts, as well as bug detection ability and usability of LLM generated test scenarios and scripts. Experiment results demonstrated that both DeepSeek and GPT-4o achieved a high overall performance. DeepSeek excels in data type correctness and status code detection, while GPT-4o performs best in API operation coverage. For both models, LLM generated test scripts maintained 100\\% syntax correctness and only required minimal manual edits for semantic correctness. These findings indicate the effectiveness and feasibility of MASTEST."}
{"id": "2511.17657", "categories": ["cs.SI"], "pdf": "https://arxiv.org/pdf/2511.17657", "abs": "https://arxiv.org/abs/2511.17657", "authors": ["Imran Ansari", "Pawanesh Pawanesh"], "title": "A Comprehensive Review of Core-Periphery and Community Detection Paradigms", "comment": null, "summary": "Meso-scale structures, such as core-periphery (CP) and community structure, have attracted significant attention in modern network science. While communities are characterized by dense intra-group and sparse inter-group connections, CP structures consist of a densely interconnected core and a loosely connected periphery, where peripheral nodes are typically linked to the core. Despite growing interest, identifying CP structures remains an ill-posed problem, with no universally accepted definition or standardized detection methodology. This ambiguity has led to conceptual overlaps, inconsistent evaluation metrics and slowed methodological progress. In this review, we provide a structured overview of foundational concepts, recent advances, key challenges and comparative evaluations of CP detection approaches, along with a discussion of their interplay with community structure and applications in real-world networks."}
{"id": "2511.17646", "categories": ["cs.CY", "econ.GN"], "pdf": "https://arxiv.org/pdf/2511.17646", "abs": "https://arxiv.org/abs/2511.17646", "authors": ["Quan-Hoang Vuong", "Viet-Phuong La", "Minh-Hoang Nguyen"], "title": "Bayesian probabilistic exploration of Bitcoin informational quanta and interactions under the GITT-VT paradigm", "comment": null, "summary": "This study explores Bitcoin's value formation through the Granular Interaction Thinking Theory-Value Theory (GITT-VT). Rather than stemming from material utility or cash flows, Bitcoin's value arises from informational attributes and interactions of multiple factors, including cryptographic order, decentralization-enabled autonomy, trust embedded in the consensus mechanism, and socio-narrative coherence that reduce entropy within decentralized value-exchange processes. To empirically assess this perspective, a Bayesian linear model was estimated using daily data from 2022 to 2025, operationalizing four informational value dimensions: Store-of-Value (SOV), Autonomy (AUT), Social-Signal Value (SSV), and Hedonic-Sentiment Value (HSV). Results indicate that only SSV exerts a highly credible positive effect on next-day returns, highlighting the dominant role of high-entropy social information in short-term pricing dynamics. In contrast, SOV and AUT show moderately reliable positive associations, reflecting their roles as low-entropy structural anchors of long-term value. HSV displays no credible predictive effect. The study advances interdisciplinary value theory and demonstrates Bitcoin as a dual-layer entropy-regulating socio-technological ecosystem. The findings offer implications for digital asset valuation, investment education, and future research on entropy dynamics across non-cash-flow digital assets."}
{"id": "2511.17692", "categories": ["cs.CR", "quant-ph"], "pdf": "https://arxiv.org/pdf/2511.17692", "abs": "https://arxiv.org/abs/2511.17692", "authors": ["Osamah N. Neamah"], "title": "QDNA-ID Quantum Device Native Authentication", "comment": null, "summary": "QDNA-ID is a trust-chain framework that links physical quantum behavior to digitally verified records. The system first executes standard quantum circuits with random shot patterns across different devices to generate entropy profiles and measurement data that reveal device-specific behavior. A Bell or CHSH test is then used to confirm that correlations originate from genuine non classical processes rather than classical simulation. The verified outcomes are converted into statistical fingerprints using entropy, divergence, and bias features to characterize each device. These features and metadata for device, session, and random seed parameters are digitally signed and time stamped to ensure integrity and traceability. Authenticated artifacts are stored in a hierarchical index for reproducible retrieval and long term auditing. A visualization and analytics interface monitors drift, policy enforcement, and device behavior logs. A machine learning engine tracks entropy drift, detects anomalies, and classifies devices based on evolving patterns. An external verification API supports independent recomputation of hashes, signatures, and CHSH evidence. QDNA-ID operates as a continuous feedback loop that maintains a persistent chain of trust for quantum computing environments."}
{"id": "2511.17566", "categories": ["cs.LG", "cs.DC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.17566", "abs": "https://arxiv.org/abs/2511.17566", "authors": ["Shuaiyu Xie", "Hanbin He", "Jian Wang", "Bing Li"], "title": "Root Cause Analysis for Microservice Systems via Cascaded Conditional Learning with Hypergraphs", "comment": null, "summary": "Root cause analysis in microservice systems typically involves two core tasks: root cause localization (RCL) and failure type identification (FTI). Despite substantial research efforts, conventional diagnostic approaches still face two key challenges. First, these methods predominantly adopt a joint learning paradigm for RCL and FTI to exploit shared information and reduce training time. However, this simplistic integration neglects the causal dependencies between tasks, thereby impeding inter-task collaboration and information transfer. Second, these existing methods primarily focus on point-to-point relationships between instances, overlooking the group nature of inter-instance influences induced by deployment configurations and load balancing. To overcome these limitations, we propose CCLH, a novel root cause analysis framework that orchestrates diagnostic tasks based on cascaded conditional learning. CCLH provides a three-level taxonomy for group influences between instances and incorporates a heterogeneous hypergraph to model these relationships, facilitating the simulation of failure propagation. Extensive experiments conducted on datasets from three microservice benchmarks demonstrate that CCLH outperforms state-of-the-art methods in both RCL and FTI."}
{"id": "2511.17644", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17644", "abs": "https://arxiv.org/abs/2511.17644", "authors": ["Chaitanya Kumar Kolli"], "title": "Hybrid Neuro-Symbolic Models for Ethical AI in Risk-Sensitive Domains", "comment": "6 pages, 6 figures", "summary": "Artificial intelligence deployed in risk-sensitive domains such as healthcare, finance, and security must not only achieve predictive accuracy but also ensure transparency, ethical alignment, and compliance with regulatory expectations. Hybrid neuro symbolic models combine the pattern-recognition strengths of neural networks with the interpretability and logical rigor of symbolic reasoning, making them well-suited for these contexts. This paper surveys hybrid architectures, ethical design considerations, and deployment patterns that balance accuracy with accountability. We highlight techniques for integrating knowledge graphs with deep inference, embedding fairness-aware rules, and generating human-readable explanations. Through case studies in healthcare decision support, financial risk management, and autonomous infrastructure, we show how hybrid systems can deliver reliable and auditable AI. Finally, we outline evaluation protocols and future directions for scaling neuro symbolic frameworks in complex, high stakes environments."}
{"id": "2511.18092", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.18092", "abs": "https://arxiv.org/abs/2511.18092", "authors": ["Sebastian Dingler", "Philip Rehkop", "Florian Mayer", "Ralf Muenzenberger"], "title": "Event-Chain Analysis for Automated Driving and ADAS Systems: Ensuring Safety and Meeting Regulatory Timing Requirements", "comment": null, "summary": "Automated Driving Systems (ADS), including Advanced Driver Assistance Systems (ADAS), must fulfill not only high functional expectations but also stringent timing constraints mandated by international regulations and standards. Regulatory frameworks such as UN regulations, NCAP standards, ISO norms, and NHTSA guidelines impose strict bounds on system reaction times to ensure safe vehicle operation. This paper presents a structured, White-Box methodology based on Event-Chain Modeling to address these timing challenges. Unlike Black-Box approaches, Event-Chain Analysis offers transparent insights into the timing behavior of each functional component - from perception and planning to actuation and human interaction. This perspective is also aligned with multiple regulations, which require that homologation dossiers provide evidence that the chosen system architecture is suitable to ensure compliance with the specified requirements. Our methodology enables the derivation, modeling, and validation of end-to-end timing constraints at the architectural level and facilitates early verification through simulation. Through a detailed case study, we demonstrate how this Event-Chain-centric approach enhances regulatory compliance, optimizes system design, and supports model-based safety analysis techniques, with results showing early identification of compliance issues, systematic parameter optimization, and quantitative evidence generation through probabilistic analysis."}
{"id": "2511.18220", "categories": ["cs.SI", "cs.IT"], "pdf": "https://arxiv.org/pdf/2511.18220", "abs": "https://arxiv.org/abs/2511.18220", "authors": ["Ben Prystawski", "Dilip Arumugam", "Noah D. Goodman"], "title": "Lossy communication constrains iterated learning", "comment": "22 pages, 8 figures", "summary": "Humans' distinctive role in the world can largely be attributed to our capacity for iterated learning, a process by which knowledge is expanded and refined over generations. A range of theories seek to explain why humans are so adept at iterated learning, many positing substantial evolutionary discontinuities in communication or cognition. Is it necessary to posit large differences in abilities between humans and other species, or could small differences in communication ability produce large differences in what a species can learn over generations? We investigate this question through a formal model based on information theory. We manipulate how much information individual learners can send each other and observe the effect on iterated learning performance. Incremental changes to the channel rate can lead to dramatic, non-linear changes to the eventual performance of the population. We complement this model with a theoretical result that describes how individual lossy communications constrain the global performance of iterated learning. Our results demonstrate that incremental, quantitative changes to communication abilities could be sufficient to explain large differences in what can be learned over many generations."}
{"id": "2511.17648", "categories": ["cs.CY", "cs.ET"], "pdf": "https://arxiv.org/pdf/2511.17648", "abs": "https://arxiv.org/abs/2511.17648", "authors": ["Florian Laronze", "Audrey Landuran", "Bernard N'kaoua"], "title": "Technologies to Support Self-determination for People with Intellectual Disability and ASD", "comment": null, "summary": "This article focuses on the concept of self-determination and the design and validation of digital tools intended to promote the self-determination of vulnerable people. Self-determination is an essential skill for carrying out daily activities. But in certain situations, and for certain populations, self-determination is lacking, which leads to the inability to live an independent life and in favorable conditions of well-being and health. In recent years, self-determination enhancing technologies have been developed and used to promote independent living among people with self-determination disorders. We will illustrate the main digital tools to support self-determination developed for two populations of people suffering from self-determination disorders: people with an intellectual disability and people with an autism spectrum disorder. The ability of these digital assistants to improve the comfort of life of these people will also be presented and discussed."}
{"id": "2511.17726", "categories": ["cs.CR", "cs.AR"], "pdf": "https://arxiv.org/pdf/2511.17726", "abs": "https://arxiv.org/abs/2511.17726", "authors": ["Subhash Sethumurugan", "Hari Cherupalli", "Kangjie Lu", "John Sartori"], "title": "Pre-cache: A Microarchitectural Solution to prevent Meltdown and Spectre", "comment": "17 pages; 19 figures", "summary": "Recent work has shown that out-of-order and speculative execution mechanisms used to increase performance in the majority of processors expose the processors to critical attacks. These attacks, called Meltdown and Spectre, exploit the side effects of performance-enhancing features in modern microprocessors to expose secret data through side channels in the microarchitecture. The well known implementations of these attacks exploit cache-based side channels since they are the least noisy channels to exfiltrate data. While some software patches attempted to mitigate these attacks, they are ad-hoc and only try to fix the side effects of the vulnerabilites. They may also impose a performance overhead of up to 30%. In this paper, we present a microarchitecture-based solution for Meltdown and Spectre that addresses the vulnerabilities exploited by the attacks. Our solution prevents flushed instructions from exposing data to the cache. Our approach can also be extended to other memory structures in the microarchitecture thereby preventing variants of the attacks which exploit these memory structures. We further identify two new variant attacks based on exploiting the side effects of speculative and out-of-order execution and show how our solution can be used to prevent these attacks. Evaluation results show that our microarchitectural solution not only restores secure out-of-order and speculative execution, but also has relatively low overhead and does not significantly impact performance for most applications."}
{"id": "2511.17568", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17568", "abs": "https://arxiv.org/abs/2511.17568", "authors": ["Le Xu", "Jiayu Chen"], "title": "Enhancing Robustness of Offline Reinforcement Learning Under Data Corruption via Sharpness-Aware Minimization", "comment": "Accepted as an Oral Presentation at the AAAI 2026 Student Abstract and Poster Program (SAPP)", "summary": "Offline reinforcement learning (RL) is vulnerable to real-world data corruption, with even robust algorithms failing under challenging observation and mixture corruptions. We posit this failure stems from data corruption creating sharp minima in the loss landscape, leading to poor generalization. To address this, we are the first to apply Sharpness-Aware Minimization (SAM) as a general-purpose, plug-and-play optimizer for offline RL. SAM seeks flatter minima, guiding models to more robust parameter regions. We integrate SAM into strong baselines for data corruption: IQL, a top-performing offline RL algorithm in this setting, and RIQL, an algorithm designed specifically for data-corruption robustness. We evaluate them on D4RL benchmarks with both random and adversarial corruption. Our SAM-enhanced methods consistently and significantly outperform the original baselines. Visualizations of the reward surface confirm that SAM finds smoother solutions, providing strong evidence for its effectiveness in improving the robustness of offline RL agents."}
{"id": "2511.17672", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17672", "abs": "https://arxiv.org/abs/2511.17672", "authors": ["Yinjie Zhao", "Heng Zhao", "Bihan Wen", "Joey Tianyi Zhou"], "title": "Cognitive Inception: Agentic Reasoning against Visual Deceptions by Injecting Skepticism", "comment": null, "summary": "As the development of AI-generated contents (AIGC), multi-modal Large Language Models (LLM) struggle to identify generated visual inputs from real ones. Such shortcoming causes vulnerability against visual deceptions, where the models are deceived by generated contents, and the reliability of reasoning processes is jeopardized. Therefore, facing rapidly emerging generative models and diverse data distribution, it is of vital importance to improve LLMs' generalizable reasoning to verify the authenticity of visual inputs against potential deceptions. Inspired by human cognitive processes, we discovered that LLMs exhibit tendency of over-trusting the visual inputs, while injecting skepticism could significantly improve the models visual cognitive capability against visual deceptions. Based on this discovery, we propose \\textbf{Inception}, a fully reasoning-based agentic reasoning framework to conduct generalizable authenticity verification by injecting skepticism, where LLMs' reasoning logic is iteratively enhanced between External Skeptic and Internal Skeptic agents. To the best of our knowledge, this is the first fully reasoning-based framework against AIGC visual deceptions. Our approach achieved a large margin of performance improvement over the strongest existing LLM baselines and SOTA performance on AEGIS benchmark."}
{"id": "2511.18165", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18165", "abs": "https://arxiv.org/abs/2511.18165", "authors": ["Israel Puerta-Merino", "Carlos Núñez-Molina", "Pablo Mesejo", "Juan Fernández-Olivares"], "title": "Towards a General Framework for HTN Modeling with LLMs", "comment": "10 pages, 5 figures, to be published in the Workshop on Planning in the Era of LLMs ( LM4Plan - https://llmforplanning.github.io ) and the Workshop on Hierarchical Planning ( HPlan - https://icaps25.icaps-conference.org/program/workshops/hplan/ ), both in the International Conference on Automated Planning and Scheduling (ICAPS) 2025", "summary": "The use of Large Language Models (LLMs) for generating Automated Planning (AP) models has been widely explored; however, their application to Hierarchical Planning (HP) is still far from reaching the level of sophistication observed in non-hierarchical architectures. In this work, we try to address this gap. We present two main contributions. First, we propose L2HP, an extension of L2P (a library to LLM-driven PDDL models generation) that support HP model generation and follows a design philosophy of generality and extensibility. Second, we apply our framework to perform experiments where we compare the modeling capabilities of LLMs for AP and HP. On the PlanBench dataset, results show that parsing success is limited but comparable in both settings (around 36\\%), while syntactic validity is substantially lower in the hierarchical case (1\\% vs. 20\\% of instances). These findings underscore the unique challenges HP presents for LLMs, highlighting the need for further research to improve the quality of generated HP models."}
{"id": "2511.19170", "categories": ["cs.SI"], "pdf": "https://arxiv.org/pdf/2511.19170", "abs": "https://arxiv.org/abs/2511.19170", "authors": ["Gaurav Kumar", "Akrati Saxena", "Chandrakala Meena"], "title": "Perplexity-Homophily Index: Homophily through Diversity in Hypergraphs", "comment": null, "summary": "Real-world complex systems are often better modeled as hypergraphs, where edges represent group interactions involving multiple entities. Understanding and quantifying homophily (similarity-driven association) in such networks is essential for analyzing community formation and information flow. We propose a hyperedge-centric framework to quantify homophily in hypergraphs. Each interaction is represented as a hyperedge, and its interaction perplexity measures the effective number of distinct attributes it contains. Comparing this observed perplexity with a degree-preserving random baseline defines the diversity gap, which quantifies how diverse an interaction is than expected by chance. The global homophily score for a network, called Perplexity-Homophily Index, is computed by averaging the normalized diversity gap across all hyperedges. Experiments on synthetic and real-world datasets show that the proposed index captures the full distribution of homophily and reveals how homophilic and heterophilic tendencies vary with interaction size in hypergraphs."}
{"id": "2511.17669", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17669", "abs": "https://arxiv.org/abs/2511.17669", "authors": ["Ashish", "Aparajita Jaiswal", "Sudip Vhaduri", "Niveditha Nerella", "Shubham Jha"], "title": "Empa: An AI-Powered Virtual Mentor for Developing Global Collaboration Skills in HPC Education", "comment": null, "summary": "High-performance computing (HPC) and parallel computing increasingly rely on global collaboration among diverse teams, yet traditional computing curricula inadequately prepare students for cross-cultural teamwork essential in modern computational research environments. This paper presents Empa, an AI-powered virtual mentor that integrates intercultural collaboration training into undergraduate computing education. Built using large language models and deployed through a progressive web application, Empa guides students through structured activities covering cultural dimensions, communication styles, and conflict resolution that are critical for effective multicultural teamwork. Our system addresses the growing need for culturally competent HPC professionals by helping computing students develop skills to collaborate effectively in international research teams, contribute to global computational projects, and navigate the cultural complexities inherent in distributed computing environments. Pilot preparation for deployment in computing courses demonstrates the feasibility of AI-mediated intercultural training and provides insights into scalable approaches for developing intercultural collaboration skills essential for HPC workforce development."}
{"id": "2511.17748", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.17748", "abs": "https://arxiv.org/abs/2511.17748", "authors": ["Daniel Myrén", "Zeeshan Afzal", "Mikael Asplund"], "title": "The Dark Side of Flexibility: How Aggregated Cyberattacks Threaten the Power Grid", "comment": "Accepted for publication at the 20th International Conference on Critical Information Infrastructures Security (CRITIS)", "summary": "Flexible energy resources are increasingly becoming common in smart grids. These resources are typically managed and controlled by aggregators that coordinate many resources to provide flexibility services. However, these aggregators and flexible energy resources are vulnerable, which could allow attackers to remotely control flexible energy resources to launch large-scale attacks on the grid. This paper investigates and evaluates the potential attack strategies that can be used to manipulate flexible energy resources to challenge the effectiveness of traditional grid stability measures and disrupt the first-swing stability of the power grid. Our work shows that although a large amount of power is required, the current flexibility capacities could potentially be sufficient to disrupt the grid on a national level."}
{"id": "2511.17573", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.17573", "abs": "https://arxiv.org/abs/2511.17573", "authors": ["Michael J. Bommarito"], "title": "Binary BPE: A Family of Cross-Platform Tokenizers for Binary Analysis", "comment": "17 pages, 3 figures, 9 tables. Paper source available at https://github.com/mjbommar/binary-tokenizer-paper ; tokenizers available at https://huggingface.co/mjbommar - mjbommar/binary-tokenizer-001-{4k,8k,16k,32k,64k}", "summary": "Sequence models for binary analysis are bottlenecked by byte-level tokenization: raw bytes waste precious context window capacity for transformers and other neural network architectures, and many existing text-oriented tokenizers fail on arbitrary 0x00--0xFF sequences. To address this issue, we introduce the Binary BPE tokenizer family, a set of cross-platform Byte Pair Encoding (BPE) tokenizers for executables trained on a large corpus of binaries spanning multiple platforms, architectures, and operating systems, including Linux, Windows, macOS, Android, and malware sources. We release trained tokenizers with vocabularies of 4K, 8K, 16K, 32K, and 64K tokens, enabling both systematic scaling studies and practical deployment from resource-constrained edge devices to high-throughput datacenters. These tokenizers discover interpretable patterns (ELF/PE headers, instruction sequences, cross-platform strings) while yielding multi-byte compression per token. On representative uncompressed executables (e.g., ELF/PE/Mach-O rather than compressed APKs), the Binary BPE tokenizers typically allow for roughly 2-3x more binary content per fixed-length transformer context window than raw bytes, enabling more efficient research and practical deployment for content identification, malware detection, reverse engineering, and optimization. We release the trained Binary BPE tokenizers on HuggingFace, providing a drop-in, open-source foundation for binary-focused language models and context-efficient agentic tools."}
{"id": "2511.17673", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.17673", "abs": "https://arxiv.org/abs/2511.17673", "authors": ["Myung Ho Kim"], "title": "Bridging Symbolic Control and Neural Reasoning in LLM Agents: The Structured Cognitive Loop", "comment": "27 pages", "summary": "Large language model agents suffer from fundamental architectural problems: entangled reasoning and execution, memory volatility, and uncontrolled action sequences. We introduce Structured Cognitive Loop (SCL), a modular architecture that explicitly separates agent cognition into five phases: Retrieval, Cognition, Control, Action, and Memory (R-CCAM). At the core of SCL is Soft Symbolic Control, an adaptive governance mechanism that applies symbolic constraints to probabilistic inference, preserving neural flexibility while restoring the explainability and controllability of classical symbolic systems. Through empirical validation on multi-step conditional reasoning tasks, we demonstrate that SCL achieves zero policy violations, eliminates redundant tool calls, and maintains complete decision traceability. These results address critical gaps in existing frameworks such as ReAct, AutoGPT, and memory-augmented approaches. Our contributions are threefold: (1) we situate SCL within the taxonomy of hybrid intelligence, differentiating it from prompt-centric and memory-only approaches; (2) we formally define Soft Symbolic Control and contrast it with neuro-symbolic AI; and (3) we derive three design principles for trustworthy agents: modular decomposition, adaptive symbolic governance, and transparent state management. We provide a complete open-source implementation demonstrating the R-CCAM loop architecture, alongside a live GPT-4o-powered travel planning agent. By connecting expert system principles with modern LLM capabilities, this work offers a practical and theoretically grounded path toward reliable, explainable, and governable AI agents. Code: https://github.com/enkiluv/scl-core-experiment Demo: https://scl-travel-planner.streamlit.app/"}
{"id": "2511.18187", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.18187", "abs": "https://arxiv.org/abs/2511.18187", "authors": ["Sristy Sumana Nath", "Banani Roy", "Munima Jahan"], "title": "Establishing Traceability Links between Release Notes & Software Artifacts: Practitioners' Perspectives", "comment": null, "summary": "Maintaining traceability links between software release notes and corresponding development artifacts, e.g., pull requests (PRs), commits, and issues, is essential for managing technical debt and ensuring maintainability. However, in open-source environments where contributors work remotely and asynchronously, establishing and maintaining these links is often error-prone, time-consuming, and frequently overlooked. Our empirical study of GitHub repositories revealed that 47% of release artifacts lacked traceability links, and 12% contained broken links. To address this gap, we first analyzed release notes to identify their What, Why, and How information and assessed how these align with PRs, commits, and issues. We curated a benchmark dataset consisting of 3,500 filtered and validated traceability link instances. Then, we implemented LLM-based approaches to automatically establish traceability links of three pairs between release note contents & PRs, release note contents & PRs and release note contents & issues. By combining the time proximity feature, the LLM-based approach, e.g., Gemini 1.5 Pro, achieved a high Precision@1 value of 0.73 for PR traceability recovery. To evaluate the usability and adoption potential of this approach, we conducted an online survey involving 33 open-source practitioners. 16% of respondents rated as very important, and 68% as somewhat important for traceability maintenance."}
{"id": "2511.19300", "categories": ["cs.SI"], "pdf": "https://arxiv.org/pdf/2511.19300", "abs": "https://arxiv.org/abs/2511.19300", "authors": ["Pouria Bazyarrezaei", "Mohammad Abdollahi Azgomi"], "title": "On Yukawa Potential Centrality for Identification of Influential Spreaders in Complex Networks", "comment": null, "summary": "Identifying influential nodes in complex networks is a fundamental challenge for understanding how information, influence, and contagion propagate through interconnected systems. Conventional centrality measures, particularly gravity-based models, often depend on pairwise interaction forces and a fixed radius of influence, which oversimplify the heterogeneous and dynamic nature of real networks. To overcome these limitations, this study proposes a novel non-interactive, action-based model, termed Yukawa Potential Centrality (YPC), which adapts the physical Yukawa potential to the topology of complex networks. Unlike gravity models, YPC computes a scalar potential for each node rather than pairwise forces, dynamically adjusting its radius of influence according to local structural properties. This formulation establishes a physically interpretable bridge between potential theory and network science, while significantly reducing computational complexity, from quadratic to near-linear time. The model is evaluated across both synthetic and real-world social networks, and its node rankings are compared with classical centrality indices and epidemic spreading models (SI and SIS). Experimental findings reveal that YPC exhibits a strong positive correlation with the SIS model and effectively isolates key spreaders, even within highly irregular topologies. These results demonstrate that YPC provides a scalable, adaptive, and theoretically grounded framework for influence analysis in social, biological, and communication networks."}
{"id": "2511.17678", "categories": ["cs.CY", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.17678", "abs": "https://arxiv.org/abs/2511.17678", "authors": ["Ingo Siegert", "Jan Nehring", "Aranxa Márquez Ampudia", "Matthias Busch", "Stefan Hillmann"], "title": "Chatbots to strengthen democracy: An interdisciplinary seminar to train identifying argumentation techniques of science denial", "comment": "6 pages, 4 figures", "summary": "In recent times, discussions on social media platforms have increasingly come under scrutiny due to the proliferation of science denial and fake news. Traditional solutions, such as regulatory actions, have been implemented to mitigate the spread of misinformation; however, these measures alone are not sufficient. To complement these efforts, educational approaches are becoming essential in empowering users to critically engage with misinformation. Conversation training, through serious games or personalized methods, has emerged as a promising strategy to help users handle science denial and toxic conversation tactics. This paper suggests an interdisciplinary seminar to explore the suitability of Large Language Models (LLMs) acting as a persona of a science denier to support people in identifying misinformation and improving resilience against toxic interactions. In the seminar, groups of four to five students will develop an AI-based chatbot that enables realistic interactions with science-denial argumentation structures. The task involves planning the setting, integrating a Large Language Model to facilitate natural dialogues, implementing the chatbot using the RASA framework, and evaluating the outcomes in a user study. It is crucial that users understand what they need to do during the interaction, how to conclude it, and how the relevant information is conveyed. The seminar does not aim to develop chatbots for practicing debunking but serves to teach AI technologies and test the feasibility of this idea for future applications. The chatbot seminar is conducted as a hybrid, parallel master's module at the participating educational institutions."}
{"id": "2511.17761", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.17761", "abs": "https://arxiv.org/abs/2511.17761", "authors": ["Manuel Kern", "Dominik Steffan", "Felix Schuster", "Florian Skopik", "Max Landauer", "David Allison", "Simon Freudenthaler", "Edgar Weippl"], "title": "StealthCup: Realistic, Multi-Stage, Evasion-Focused CTF for Benchmarking IDS", "comment": null, "summary": "Intrusion Detection Systems (IDS) are critical to defending enterprise and industrial control environments, yet evaluating their effectiveness under realistic conditions remains an open challenge. Existing benchmarks rely on synthetic datasets (e.g., NSL-KDD, CICIDS2017) or scripted replay frameworks, which fail to capture adaptive adversary behavior. Even MITRE ATT&CK Evaluations, while influential, are host-centric and assume malware-driven compromise, thereby under-representing stealthy, multi-stage intrusions across IT and OT domains. We present StealthCup, a novel evaluation methodology that operationalizes IDS benchmarking as an evasion-focused Capture-the-Flag competition. Professional penetration testers engaged in multi-stage attack chains on a realistic IT/OT testbed, with scoring penalizing IDS detections. The event generated structured attacker writeups, validated detections, and PCAPs, host logs, and alerts. Our results reveal that out of 32 exercised attack techniques, 11 were not detected by any IDS configuration. Open-source systems (Wazuh, Suricata) produced high false-positive rates >90%, while commercial tools generated fewer false positives but also missed more attacks. Comparison with the Volt Typhoon APT advisory confirmed strong realism: all 28 applicable techniques were exercised, 19 appeared in writeups, and 9 in forensic traces. These findings demonstrate that StealthCup elicits attacker behavior closely aligned with state-sponsored TTPs, while exposing blind spots across both open-source and commercial IDS. The resulting datasets and methodology provide a reproducible foundation for future stealth-focused IDS evaluation."}
{"id": "2511.17577", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.17577", "abs": "https://arxiv.org/abs/2511.17577", "authors": ["Fengming Yu", "Qingyu Meng", "Haiwei Pan", "Kejia Zhang"], "title": "Efficient Mathematical Reasoning Models via Dynamic Pruning and Knowledge Distillation", "comment": "12 pages, 1 figure", "summary": "With the rapid development of deep learning, large language models have shown strong capabilities in complex reasoning tasks such as mathematical equation solving. However, their substantial computational and storage costs hinder practical deployment. This paper proposes a lightweight optimization method that integrates dynamic attention head pruning with knowledge distillation. The approach dynamically evaluates the importance of each attention head in the multi-head attention mechanism using a combination of weight norms and entropy, and prunes redundant heads in real time to reduce computational overhead. To mitigate performance degradation, knowledge distillation transfers information from the original model to the pruned student, enabling the smaller model to preserve reasoning ability. Experiments conducted on both Math23k and ASDiv-A verify the effectiveness of the proposed method. For example, on Math23k with a 30% pruning ratio, parameters are reduced by 18.7%, inference speed is improved by 27.5%, FLOPs are reduced by 19.3%, and accuracy drops only 0.7% (from 84.4% to 83.7%). These results demonstrate that the method achieves substantial efficiency gains while maintaining strong reasoning performance, providing a practical solution for efficient deployment of large language models in mathematical reasoning tasks."}
{"id": "2511.17714", "categories": ["cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2511.17714", "abs": "https://arxiv.org/abs/2511.17714", "authors": ["Alex John London", "Aydin Mohseni"], "title": "Learning the Value of Value Learning", "comment": "27 pages, 6 figures, mathematical appendix", "summary": "Standard decision frameworks addresses uncertainty about facts but assumes fixed values. We extend the Jeffrey-Bolker framework to model refinements in values and prove a value-of-information theorem for axiological refinement. In multi-agent settings, we establish that mutual refinement will characteristically transform zero-sum games into positive-sum interactions and yields Pareto-improving Nash bargains. These results show that a framework of rational choice can be extended to model value refinement and its associated benefits. By unifying epistemic and axiological refinement under a single formalism, we broaden the conceptual foundations of rational choice and illuminate the normative status of ethical deliberation."}
{"id": "2511.18249", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.18249", "abs": "https://arxiv.org/abs/2511.18249", "authors": ["Mostafijur Rahman Akhond", "Gias Uddin"], "title": "LLM Assisted Coding with Metamorphic Specification Mutation Agent", "comment": null, "summary": "Metamorphic Relations (MRs) serve as a foundational mechanism for generating semantically equivalent mutations. Software engineering has advanced significantly in recent years with the advent of Large Language Models (LLMs). However, the reliability of LLMs in software engineering is often compromised by ambiguities and inconsistencies due to improper user specification. To address this challenge, we present CodeMetaAgent (CMA), a metamorphic relation-driven LLM agent that systematically refines task specifications and generates semantically constrained test cases. Our proposed framework uses MRs with LLMs to improve generation consistency and reduce variability caused by specifications, unlike the traditional use of MRs as post validations. Our framework has been evaluated on the HumanEval-Pro, MBPP-Pro, and SWE-Bench_Lite datasets using the GPT-4o, Mistral Large, GPT-OSS, and Qwen3-Coder models. It improved code generation accuracy by up to 17% and achieved code coverage gains of up to 99.81%. These results show that metamorphic relations can be a simple but effective guide in assisting LLM-based software development."}
{"id": "2511.18268", "categories": ["cs.CY", "cs.SI"], "pdf": "https://arxiv.org/pdf/2511.18268", "abs": "https://arxiv.org/abs/2511.18268", "authors": ["S M Mehedi Zaman", "Saubhagya Joshi", "Yiyi Wu"], "title": "Privacy Concerns and ChatGPT: Exploring Online Discourse through the Lens of Information Practice on Reddit", "comment": "Accepted as a poster at the iConference, 2026", "summary": "As millions of people use ChatGPT for tasks such as education, writing assistance, and health advice, concerns have grown about how personal prompts and data are stored and used. This study explores how Reddit users collectively negotiate and respond to these privacy concerns. Posts were collected from three major subreddits -- r/Chatgpt, r/privacy, and r/OpenAI -- between November 2022 and May 2025. An iterative keyword search followed by manual screening resulted in a final dataset of 426 posts and 1,900 comments. Using information practice as the theoretical lens, we conducted a qualitative thematic analysis to identify collective practices of risk negotiation, validated with BERTopic topic modeling to ensure thematic saturation. Findings revealed risk signaling, norm-setting, and resignation as dominant discourses, and collective troubleshooting and advocacy for privacy-preserving alternatives as key adaptive practices. Reddit functions as a site of collective sense-making where users surface risks, establish informal norms, and share strategies for mitigating privacy threats, offering insights for AI design and privacy literacy initiatives."}
{"id": "2511.17682", "categories": ["cs.CY", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.17682", "abs": "https://arxiv.org/abs/2511.17682", "authors": ["Tim Schlippe", "Matthias Wölfel", "Koena Ronny Mabokela"], "title": "A Cross-Cultural Assessment of Human Ability to Detect LLM-Generated Fake News about South Africa", "comment": null, "summary": "This study investigates how cultural proximity affects the ability to detect AI-generated fake news by comparing South African participants with those from other nationalities. As large language models increasingly enable the creation of sophisticated fake news, understanding human detection capabilities becomes crucial, particularly across different cultural contexts. We conducted a survey where 89 participants (56 South Africans, 33 from other nationalities) evaluated 10 true South African news articles and 10 AI-generated fake versions. Results reveal an asymmetric pattern: South Africans demonstrated superior performance in detecting true news about their country (40% deviation from ideal rating) compared to other participants (52%), but performed worse at identifying fake news (62% vs. 55%). This difference may reflect South Africans' higher overall trust in news sources. Our analysis further shows that South Africans relied more on content knowledge and contextual understanding when judging credibility, while participants from other countries emphasised formal linguistic features such as grammar and structure. Overall, the deviation from ideal rating was similar between groups (51% vs. 53%), suggesting that cultural familiarity appears to aid verification of authentic information but may also introduce bias when evaluating fabricated content. These insights contribute to understanding cross-cultural dimensions of misinformation detection and inform strategies for combating AI-generated fake news in increasingly globalised information ecosystems where content crosses cultural and geographical boundaries."}
{"id": "2511.17799", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.17799", "abs": "https://arxiv.org/abs/2511.17799", "authors": ["Qiang Liu", "Wenlong Zhang", "Muhui Jiang", "Lei Wu", "Yajin Zhou"], "title": "Characteristics, Root Causes, and Detection of Incomplete Security Bug Fixes in the Linux Kernel", "comment": null, "summary": "Security bugs in the Linux kernel emerge endlessly and have attracted much attention. However, fixing security bugs in the Linux kernel could be incomplete due to human mistakes. Specifically, an incomplete fix fails to repair all the original security defects in the software, fails to properly repair the original security defects, or introduces new ones. In this paper, we study the fixes of incomplete security bugs in the Linux kernel for the first time, and reveal their characteristics, root causes, as well as detection. We first construct a dataset of incomplete security bug fixes in the Linux kernel and answer the following three questions. What are the characteristics of incomplete security bug fixes in the Linux kernel? What are the root causes behind them? How should they be detected to reduce security risks? We then have the three main insights in the following. (*Due to the notification of arXiv \"The Abstract field cannot be longer than 1,920 characters\", the appeared Abstract is shortened. For the full Abstract, please download the Article.)"}
{"id": "2511.17579", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17579", "abs": "https://arxiv.org/abs/2511.17579", "authors": ["Hefei Xu", "Le Wu", "Chen Cheng", "Hao Liu"], "title": "Multi-Value Alignment for LLMs via Value Decorrelation and Extrapolation", "comment": "accepted by AAAI26 oral; 12 pages", "summary": "With the rapid advancement of large language models (LLMs), aligning them with human values for safety and ethics has become a critical challenge. This problem is especially challenging when multiple, potentially conflicting human values must be considered and balanced. Although several variants of existing alignment methods (such as Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO)) have been proposed to address multi-value alignment, they suffer from notable limitations: 1) they are often unstable and inefficient in multi-value optimization; and 2) they fail to effectively handle value conflicts. As a result, these approaches typically struggle to achieve optimal trade-offs when aligning multiple values.\n  To address this challenge, we propose a novel framework called Multi-Value Alignment (MVA). It mitigates alignment degradation caused by parameter interference among diverse human values by minimizing their mutual information. Furthermore, we propose a value extrapolation strategy to efficiently explore the Pareto frontier, thereby constructing a set of LLMs with diverse value preferences. Extensive experiments demonstrate that MVA consistently outperforms existing baselines in aligning LLMs with multiple human values."}
{"id": "2511.17729", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17729", "abs": "https://arxiv.org/abs/2511.17729", "authors": ["Yang Zhou", "Mingyu Zhao", "Zhenting Wang", "Difei Gu", "Bangwei Guo", "Ruosong Ye", "Ligong Han", "Can Jin", "Dimitris N. Metaxas"], "title": "M3-Bench: Multi-Modal, Multi-Hop, Multi-Threaded Tool-Using MLLM Agent Benchmark", "comment": null, "summary": "We present M^3-Bench, the first benchmark for evaluating multimodal tool use under the Model Context Protocol. The benchmark targets realistic, multi-hop and multi-threaded workflows that require visual grounding and textual reasoning, cross-tool dependencies, and persistence of intermediate resources across steps. We introduce a similarity-driven alignment that serializes each tool call, embeds signatures with a sentence encoder, and performs similarity-bucketed Hungarian matching to obtain auditable one-to-one correspondences. On top of this alignment, we report interpretable metrics that decouple semantic fidelity from workflow consistency. The benchmark spans 28 servers with 231 tools, and provides standardized trajectories curated through an Executor & Judge pipeline with human verification; an auxiliary four large language models (LLMs) judge ensemble reports end-task Task Completion and information grounding. Evaluations of representative state-of-the-art Multimodal LLMs (MLLMs) reveal persistent gaps in multimodal MCP tool use, particularly in argument fidelity and structure consistency, underscoring the need for methods that jointly reason over images, text, and tool graphs. Our Benchmark's anonymous repository is at https://github.com/EtaYang10th/Open-M3-Bench"}
{"id": "2511.18288", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.18288", "abs": "https://arxiv.org/abs/2511.18288", "authors": ["Wenhan Wang", "Kaibo Liu", "Zeyu Sun", "An Ran Chen", "Ge Li", "Gang Huang", "Lei Ma"], "title": "Can Large Language Models Solve Path Constraints in Symbolic Execution?", "comment": null, "summary": "Symbolic execution is an important software analysis technique which benefits downstream tasks such as software testing and debugging. However, several limitations hinder symbolic execution from application on real-world software. One of the limitations is the inability to solve diverse execution path constraints: traditional symbolic execution based on SMT solvers is difficult to handle execution paths with complex data structures or external API calls. In this paper, we focus on investigating the possibility of adopting large language models (LLM) for path constraint solving instead of traditional solver-based techniques in symbolic execution. We conduct an empirical study to evaluate the ability of LLMs in two types of path constraint solving: generating test inputs to facilitate an execution path, and determining whether a given execution path can be satisfied without triggering any bugs. We build new evaluation pipelines and benchmarks for two tasks: test case generation and path classification, which include data sources from both competition-level programs and real-world repositories. Our experiment results show that state-of-the-art LLMs are able to solve path constraints in both generation and classification tasks, with 60% of generated test cases that accurately cover the given execution path. Moreover, LLMs are capable of improving test coverage by covering execution paths in real-world repositories where traditional symbolic execution tools cannot be applied. These findings highlight the possibility of extending symbolic execution techniques with LLMs in the future to improve the ability and generalizability of symbolic execution."}
{"id": "2511.18783", "categories": ["cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2511.18783", "abs": "https://arxiv.org/abs/2511.18783", "authors": ["Renchu Guan", "Xuyang Li", "Yachao Zhang", "Wei Pang", "Fausto Giunchiglia", "Ximing Li", "Yonghao Liu", "Xiaoyue Feng"], "title": "Hypergraph Contrastive Learning for both Homophilic and Heterophilic Hypergraphs", "comment": null, "summary": "Hypergraphs, as a generalization of traditional graphs, naturally capture high-order relationships. In recent years, hypergraph neural networks (HNNs) have been widely used to capture complex high-order relationships. However, most existing hypergraph neural network methods inherently rely on the homophily assumption, which often does not hold in real-world scenarios that exhibit significant heterophilic structures. To address this limitation, we propose \\textbf{HONOR}, a novel unsupervised \\textbf{H}ypergraph c\\textbf{ON}trastive learning framework suitable for both hom\\textbf{O}philic and hete\\textbf{R}ophilic hypergraphs. Specifically, HONOR explicitly models the heterophilic relationships between hyperedges and nodes through two complementary mechanisms: a prompt-based hyperedge feature construction strategy that maintains global semantic consistency while suppressing local noise, and an adaptive attention aggregation module that dynamically captures the diverse local contributions of nodes to hyperedges. Combined with high-pass filtering, these designs enable HONOR to fully exploit heterophilic connection patterns, yielding more discriminative and robust node and hyperedge representations. Theoretically, we demonstrate the superior generalization ability and robustness of HONOR. Empirically, extensive experiments further validate that HONOR consistently outperforms state-of-the-art baselines under both homophilic and heterophilic datasets."}
{"id": "2511.17683", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17683", "abs": "https://arxiv.org/abs/2511.17683", "authors": ["Lara Hassan", "Mohamed ElZeftawy", "Abdulrahman Mahmoud"], "title": "Datacenters in the Desert: Feasibility and Sustainability of LLM Inference in the Middle East", "comment": "3 pages, 1 figure", "summary": "As the Middle East emerges as a strategic hub for artificial intelligence (AI) infrastructure, the feasibility of deploying sustainable datacenters in desert environments has become a topic of growing relevance. This paper presents an empirical study analyzing the energy consumption and carbon footprint of large language model (LLM) inference across four countries: the United Arab Emirates, Iceland, Germany, and the United States of America using DeepSeek Coder 1.3B and the HumanEval dataset on the task of code generation. We use the CodeCarbon library to track energy and carbon emissions andcompare geographical trade-offs for climate-aware AI deployment. Our findings highlight both the challenges and potential of datacenters in desert regions and provide a balanced outlook on their role in global AI expansion."}
{"id": "2511.17842", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.17842", "abs": "https://arxiv.org/abs/2511.17842", "authors": ["Xavier Salleras"], "title": "Homomorphic Encryption-based Vaults for Anonymous Balances on VM-enabled Blockchains", "comment": null, "summary": "In this work, we present homomorphic encryption-based vaults (Haults), a permissioned privacy-preserving smart wallet protocol for VM-enabled blockchains that keeps users' balances confidential, as well as the amounts transacted to other parties. To comply with regulations, we include optional compliance features that allow specific entities (the auditors) to retrieve transaction amounts or execute force transfers when necessary. Our solution uses ElGamal over elliptic curves to encrypt balances, combined with zero-knowledge proofs to verify the correctness of transaction amounts and the integrity of the sender's updated balance, among other security checks. We provide a detailed explanation of the protocol, including a security discussion and benchmarks from our proof-of-concept implementation, which yield great results. Beyond in-contract issued tokens, we also provide a thorough explanation on how our solution can be compatible with external ones (e.g., Ether or any ERC20)."}
{"id": "2511.17581", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17581", "abs": "https://arxiv.org/abs/2511.17581", "authors": ["Zhiwen Qiu", "Ziang Liu", "Wenqian Niu", "Tapomayukh Bhattacharjee", "Saleh Kalantari"], "title": "EgoCogNav: Cognition-aware Human Egocentric Navigation", "comment": "11 pages, 4 figures", "summary": "Modeling the cognitive and experiential factors of human navigation is central to deepening our understanding of human-environment interaction and to enabling safe social navigation and effective assistive wayfinding. Most existing methods focus on forecasting motions in fully observed scenes and often neglect human factors that capture how people feel and respond to space. To address this gap, We propose EgoCogNav, a multimodal egocentric navigation framework that predicts perceived path uncertainty as a latent state and jointly forecasts trajectories and head motion by fusing scene features with sensory cues. To facilitate research in the field, we introduce the Cognition-aware Egocentric Navigation (CEN) dataset consisting 6 hours of real-world egocentric recordings capturing diverse navigation behaviors in real-world scenarios. Experiments show that EgoCogNav learns the perceived uncertainty that highly correlates with human-like behaviors such as scanning, hesitation, and backtracking while generalizing to unseen environments."}
{"id": "2511.17743", "categories": ["cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.17743", "abs": "https://arxiv.org/abs/2511.17743", "authors": ["Haytham Younus", "Sohag Kabir", "Felician Campean", "Pascal Bonnaud", "David Delaux"], "title": "AI- and Ontology-Based Enhancements to FMEA for Advanced Systems Engineering: Current Developments and Future Directions", "comment": "This manuscript is based on research undertaken by our doctoral student at the University of Bradford. The associated PhD thesis has been formally submitted to the University and is currently awaiting final examination. The review article is being shared on arXiv to make the review accessible to the research community while the thesis examination process is ongoing", "summary": "This article presents a state-of-the-art review of recent advances aimed at transforming traditional Failure Mode and Effects Analysis (FMEA) into a more intelligent, data-driven, and semantically enriched process. As engineered systems grow in complexity, conventional FMEA methods, largely manual, document-centric, and expert-dependent, have become increasingly inadequate for addressing the demands of modern systems engineering. We examine how techniques from Artificial Intelligence (AI), including machine learning and natural language processing, can transform FMEA into a more dynamic, data-driven, intelligent, and model-integrated process by automating failure prediction, prioritisation, and knowledge extraction from operational data. In parallel, we explore the role of ontologies in formalising system knowledge, supporting semantic reasoning, improving traceability, and enabling cross-domain interoperability. The review also synthesises emerging hybrid approaches, such as ontology-informed learning and large language model integration, which further enhance explainability and automation. These developments are discussed within the broader context of Model-Based Systems Engineering (MBSE) and function modelling, showing how AI and ontologies can support more adaptive and resilient FMEA workflows. We critically analyse a range of tools, case studies, and integration strategies, while identifying key challenges related to data quality, explainability, standardisation, and interdisciplinary adoption. By leveraging AI, systems engineering, and knowledge representation using ontologies, this review offers a structured roadmap for embedding FMEA within intelligent, knowledge-rich engineering environments."}
{"id": "2511.18343", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.18343", "abs": "https://arxiv.org/abs/2511.18343", "authors": ["Dongming Jin", "Zhi Jin", "Xiaohong Chen", "Zheng Fang", "Linyu Li", "Yuanpeng He", "Jia Li", "Yirang Zhang", "Yingtao Fang"], "title": "A Needle in a Haystack: Intent-driven Reusable Artifacts Recommendation with LLMs", "comment": "15 pages, 7 figures", "summary": "In open source software development, the reuse of existing artifacts has been widely adopted to avoid redundant implementation work. Reusable artifacts are considered more efficient and reliable than developing software components from scratch. However, when faced with a large number of reusable artifacts, developers often struggle to find artifacts that can meet their expected needs. To reduce this burden, retrieval-based and learning-based techniques have been proposed to automate artifact recommendations. Recently, Large Language Models (LLMs) have shown the potential to understand intentions, perform semantic alignment, and recommend usable artifacts. Nevertheless, their effectiveness has not been thoroughly explored. To fill this gap, we construct an intent-driven artifact recommendation benchmark named IntentRecBench, covering three representative open source ecosystems. Using IntentRecBench, we conduct a comprehensive comparative study of five popular LLMs and six traditional approaches in terms of precision and efficiency. Our results show that although LLMs outperform traditional methods, they still suffer from low precision and high inference cost due to the large candidate space. Inspired by the ontology-based semantic organization in software engineering, we propose TreeRec, a feature tree-guided recommendation framework to mitigate these issues. TreeRec leverages LLM-based semantic abstraction to organize artifacts into a hierarchical semantic tree, enabling intent and function alignment and reducing reasoning time. Extensive experiments demonstrate that TreeRec consistently improves the performance of diverse LLMs across ecosystems, highlighting its generalizability and potential for practical deployment."}
{"id": "2511.18874", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.MA", "cs.RO", "cs.SI"], "pdf": "https://arxiv.org/pdf/2511.18874", "abs": "https://arxiv.org/abs/2511.18874", "authors": ["Yuzhi Chen", "Yuanchang Xie", "Lei Zhao", "Pan Liu", "Yajie Zou", "Chen Wang"], "title": "GContextFormer: A global context-aware hybrid multi-head attention approach with scaled additive aggregation for multimodal trajectory prediction", "comment": null, "summary": "Multimodal trajectory prediction generates multiple plausible future trajectories to address vehicle motion uncertainty from intention ambiguity and execution variability. However, HD map-dependent models suffer from costly data acquisition, delayed updates, and vulnerability to corrupted inputs, causing prediction failures. Map-free approaches lack global context, with pairwise attention over-amplifying straight patterns while suppressing transitional patterns, resulting in motion-intention misalignment. This paper proposes GContextFormer, a plug-and-play encoder-decoder architecture with global context-aware hybrid attention and scaled additive aggregation achieving intention-aligned multimodal prediction without map reliance. The Motion-Aware Encoder builds scene-level intention prior via bounded scaled additive aggregation over mode-embedded trajectory tokens and refines per-mode representations under shared global context, mitigating inter-mode suppression and promoting intention alignment. The Hierarchical Interaction Decoder decomposes social reasoning into dual-pathway cross-attention: a standard pathway ensures uniform geometric coverage over agent-mode pairs while a neighbor-context-enhanced pathway emphasizes salient interactions, with gating module mediating their contributions to maintain coverage-focus balance. Experiments on eight highway-ramp scenarios from TOD-VT dataset show GContextFormer outperforms state-of-the-art baselines. Compared to existing transformer models, GContextFormer achieves greater robustness and concentrated improvements in high-curvature and transition zones via spatial distributions. Interpretability is achieved through motion mode distinctions and neighbor context modulation exposing reasoning attribution. The modular architecture supports extensibility toward cross-domain multimodal reasoning tasks. Source: https://fenghy-chen.github.io/sources/."}
{"id": "2511.17694", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2511.17694", "abs": "https://arxiv.org/abs/2511.17694", "authors": ["Joanna Schroeder", "Alan Wang", "Kathryn Linehan", "Joel Thurston", "Aaron Schroeder"], "title": "Smart Metadata in Action: The Social Impact Data Commons", "comment": "Conference On Smart Metadata for Official Statistics 2024 (COSMOS 2024), April, 11-12, 2024, Paris, France", "summary": "This article describes the use of metadata and standards in the Social Impact Data Commons to expose official statisticians to an innovative project built on actionable and evaluable metadata, which produces a FAIR data system. We begin by introducing the concept of the Data Commons, focusing on its features, and presenting an overview of current implementations of the Data Commons. We then present the core metadata case study, demonstrating how smart metadata support the Data Commons. We also present evaluations of our core metadata, including its adherence to the FAIR guidelines. We conclude with a discussion on our future metadata and standards-related projects to support the Social Impact Data Commons."}
{"id": "2511.17874", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.17874", "abs": "https://arxiv.org/abs/2511.17874", "authors": ["Yunyi Zhang", "Shibo Cui", "Baojun Liu", "Jingkai Yu", "Min Zhang", "Fan Shi", "Han Zheng"], "title": "Beyond Jailbreak: Unveiling Risks in LLM Applications Arising from Blurred Capability Boundaries", "comment": "Accepted by Network and Distributed System Security (NDSS) Symposium 2026", "summary": "LLM applications (i.e., LLM apps) leverage the powerful capabilities of LLMs to provide users with customized services, revolutionizing traditional application development. While the increasing prevalence of LLM-powered applications provides users with unprecedented convenience, it also brings forth new security challenges. For such an emerging ecosystem, the security community lacks sufficient understanding of the LLM application ecosystem, especially regarding the capability boundaries of the applications themselves.\n  In this paper, we systematically analyzed the new development paradigm and defined the concept of the LLM app capability space. We also uncovered potential new risks beyond jailbreak that arise from ambiguous capability boundaries in real-world scenarios, namely, capability downgrade and upgrade. To evaluate the impact of these risks, we designed and implemented an LLM app capability evaluation framework, LLMApp-Eval. First, we collected application metadata across 4 platforms and conducted a cross-platform ecosystem analysis. Then, we evaluated the risks for 199 popular applications among 4 platforms and 6 open-source LLMs. We identified that 178 (89.45%) potentially affected applications, which can perform tasks from more than 15 scenarios or be malicious. We even found 17 applications in our study that executed malicious tasks directly, without applying any adversarial rewriting. Furthermore, our experiments also reveal a positive correlation between the quality of prompt design and application robustness. We found that well-designed prompts enhance security, while poorly designed ones can facilitate abuse. We hope our work inspires the community to focus on the real-world risks of LLM applications and foster the development of a more robust LLM application ecosystem."}
{"id": "2511.17582", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17582", "abs": "https://arxiv.org/abs/2511.17582", "authors": ["Jie Ou", "Shuaihong Jiang", "Yingjun Du", "Cees G. M. Snoek"], "title": "GateRA: Token-Aware Modulation for Parameter-Efficient Fine-Tuning", "comment": "Accepted by AAAI 2026", "summary": "Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, DoRA, and HiRA, enable lightweight adaptation of large pre-trained models via low-rank updates. However, existing PEFT approaches apply static, input-agnostic updates to all tokens, disregarding the varying importance and difficulty of different inputs. This uniform treatment can lead to overfitting on trivial content or under-adaptation on more informative regions, especially in autoregressive settings with distinct prefill and decoding dynamics. In this paper, we propose GateRA, a unified framework that introduces token-aware modulation to dynamically adjust the strength of PEFT updates. By incorporating adaptive gating into standard PEFT branches, GateRA enables selective, token-level adaptation, preserving pre-trained knowledge for well-modeled inputs while focusing capacity on challenging cases. Empirical visualizations reveal phase-sensitive behaviors, where GateRA automatically suppresses updates for redundant prefill tokens while emphasizing adaptation during decoding. To promote confident and efficient modulation, we further introduce an entropy-based regularization that encourages near-binary gating decisions. This regularization prevents diffuse update patterns and leads to interpretable, sparse adaptation without hard thresholding. Finally, we present a theoretical analysis showing that GateRA induces a soft gradient-masking effect over the PEFT path, enabling continuous and differentiable control over adaptation. Experiments on multiple commonsense reasoning benchmarks demonstrate that GateRA consistently outperforms or matches prior PEFT methods."}
{"id": "2511.17833", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.17833", "abs": "https://arxiv.org/abs/2511.17833", "authors": ["Yunsheng Bai", "Haoxing Ren"], "title": "Learning to Debug: LLM-Organized Knowledge Trees for Solving RTL Assertion Failures", "comment": null, "summary": "Debugging is the dominant cost in modern hardware verification, where assertion failures are among the most frequent and expensive to resolve. While Large Language Models (LLMs) show promise, they often fail to capture the precise, reusable expertise that engineers apply, leading to inaccurate responses. We propose GROVE, a hierarchical knowledge management framework that learns and organizes reusable debugging expertise into an LLM-organized knowledge tree for solving assertion failures. GROVE distills debugging knowledge from prior cases and organizes it into a vertical tree of configurable depth, with each node encoding a concise knowledge item and explicit applicability conditions. During training, GROVE uses a parallel, gradient-free loop where an LLM proposes tree modifications as structured JSON edits by learning from the cases. At test time, a budget-aware iterative zoom is performed to navigate the tree, retrieving a small set of applicable knowledge items that guide a base LLM's hypothesis generation and fix proposals. Evaluated on a suite of assertion-failure cases, GROVE delivers consistent gains in pass@1 and pass@5, demonstrating the value of structured knowledge evolution."}
{"id": "2511.18488", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18488", "abs": "https://arxiv.org/abs/2511.18488", "authors": ["Samuel Ackerman", "Wesam Ibraheem", "Orna Raz", "Marcel Zalmanovici"], "title": "Evaluating perturbation robustnessof generative systems that use COBOL code inputs", "comment": "16 pages (8 main, 8 appendix). Accepted to AI-SQE (ICSE, 2026): The 1st International Workshop on AI for Software Quality Evaluation: Judgment, Metrics, Benchmarks, and Beyond", "summary": "Systems incorporating large language models (LLMs) as a component are known to be sensitive (i.e., non-robust) to minor input variations that do not change the meaning of the input; such sensitivity may reduce the system's usefulness. Here, we present a framework to evaluate robustness of systems using COBOL code as input; our application is translation between COBOL and Java programming languages, but the approach extends to other tasks such as code generation or explanation. Targeting robustness of systems with COBOL as input is essential yet challenging. Many business-critical applications are written in COBOL, yet these are typically proprietary legacy applications and their code is unavailable to LLMs for training. We develop a library of COBOL paragraph and full-program perturbation methods, and create variant-expanded versions of a benchmark dataset of examples for a specific task. The robustness of the LLM-based system is evaluated by measuring changes in values of individual and aggregate metrics calculated on the system's outputs. Finally, we present a series of dynamic table and chart visualization dashboards that assist in debugging the system's outputs, and monitoring and understanding root causes of the system's sensitivity to input variation. These tools can be further used to improve the system by, for instance, indicating variations that should be handled by pre-processing steps."}
{"id": "2511.17696", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17696", "abs": "https://arxiv.org/abs/2511.17696", "authors": ["Douglas C. Schmidt", "Dan Runfola"], "title": "Liberating Logic in the Age of AI: Going Beyond Programming with Computational Thinking", "comment": "15 pages and 17 figures", "summary": "Mastering one or more programming languages has historically been the gateway to implementing ideas on a computer. Today, that gateway is widening with advances in large language models (LLMs) and artificial intelligence (AI)-powered coding assistants. What matters is no longer just fluency in traditional programming languages but the ability to think computationally by translating problems into forms that can be solved with computing tools. The capabilities enabled by these AI-augmented tools are rapidly leading to the commoditization of computational thinking, such that anyone who can articulate a problem in natural language can potentially harness computing power via AI.\n  This shift is poised to radically influence how we teach computer science and data science in the United States and around the world. Educators and industry leaders are grappling with how to adapt: What should students learn when the hottest new programming language is English? How do we prepare a generation of computational thinkers who need not code every algorithm manually, but must still think critically, design solutions, and verify AI-augmented results?\n  This paper explores these questions, examining the impact of natural language programming on software development, the emerging distinction between programmers and prompt-crafting problem solvers, the reforms needed in computer science and data science curricula, and the importance of maintaining our fundamental computational science principles in an AI-augmented future. Along the way, we compare approaches and share best practices for embracing this new paradigm in computing education."}
{"id": "2511.17959", "categories": ["cs.CR", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17959", "abs": "https://arxiv.org/abs/2511.17959", "authors": ["Yuhao Wu", "Ke Yang", "Franziska Roesner", "Tadayoshi Kohno", "Ning Zhang", "Umar Iqbal"], "title": "Towards Automating Data Access Permissions in AI Agents", "comment": "Accepted by the IEEE Symposium on Security and Privacy (S&P) 2026", "summary": "As AI agents attempt to autonomously act on users' behalf, they raise transparency and control issues. We argue that permission-based access control is indispensable in providing meaningful control to the users, but conventional permission models are inadequate for the automated agentic execution paradigm. We therefore propose automated permission management for AI agents. Our key idea is to conduct a user study to identify the factors influencing users' permission decisions and to encode these factors into an ML-based permission management assistant capable of predicting users' future decisions. We find that participants' permission decisions are influenced by communication context but importantly individual preferences tend to remain consistent within contexts, and align with those of other participants. Leveraging these insights, we develop a permission prediction model achieving 85.1% accuracy overall and 94.4% for high-confidence predictions. We find that even without using permission history, our model achieves an accuracy of 66.9%, and a slight increase of training samples (i.e., 1-4) can substantially increase the accuracy by 10.8%."}
{"id": "2511.17583", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17583", "abs": "https://arxiv.org/abs/2511.17583", "authors": ["Chenrui Ma", "Xi Xiao", "Tianyang Wang", "Xiao Wang", "Yanning Shen"], "title": "Learning Straight Flows: Variational Flow Matching for Efficient Generation", "comment": null, "summary": "Flow Matching has limited ability in achieving one-step generation due to its reliance on learned curved trajectories. Previous studies have attempted to address this limitation by either modifying the coupling distribution to prevent interpolant intersections or introducing consistency and mean-velocity modeling to promote straight trajectory learning. However, these approaches often suffer from discrete approximation errors, training instability, and convergence difficulties. To tackle these issues, in the present work, we propose \\textbf{S}traight \\textbf{V}ariational \\textbf{F}low \\textbf{M}atching (\\textbf{S-VFM}), which integrates a variational latent code representing the ``generation overview'' into the Flow Matching framework. \\textbf{S-VFM} explicitly enforces trajectory straightness, ideally producing linear generation paths. The proposed method achieves competitive performance across three challenge benchmarks and demonstrates advantages in both training and inference efficiency compared with existing methods."}
{"id": "2511.17855", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.17855", "abs": "https://arxiv.org/abs/2511.17855", "authors": ["Jordan Abi Nader", "David Lee", "Nathaniel Dennler", "Andreea Bobu"], "title": "QuickLAP: Quick Language-Action Preference Learning for Autonomous Driving Agents", "comment": null, "summary": "Robots must learn from both what people do and what they say, but either modality alone is often incomplete: physical corrections are grounded but ambiguous in intent, while language expresses high-level goals but lacks physical grounding. We introduce QuickLAP: Quick Language-Action Preference learning, a Bayesian framework that fuses physical and language feedback to infer reward functions in real time. Our key insight is to treat language as a probabilistic observation over the user's latent preferences, clarifying which reward features matter and how physical corrections should be interpreted. QuickLAP uses Large Language Models (LLMs) to extract reward feature attention masks and preference shifts from free-form utterances, which it integrates with physical feedback in a closed-form update rule. This enables fast, real-time, and robust reward learning that handles ambiguous feedback. In a semi-autonomous driving simulator, QuickLAP reduces reward learning error by over 70% compared to physical-only and heuristic multimodal baselines. A 15-participant user study further validates our approach: participants found QuickLAP significantly more understandable and collaborative, and preferred its learned behavior over baselines. Code is available at https://github.com/MIT-CLEAR-Lab/QuickLAP."}
{"id": "2511.18506", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.18506", "abs": "https://arxiv.org/abs/2511.18506", "authors": ["Michael Adjei Osei", "Sidney Shapiro"], "title": "HQPEF-Py: Metrics, Python Patterns, and Guidance for Evaluating Hybrid Quantum Programs", "comment": "17 pages", "summary": "We study how to evaluate hybrid quantum programs as end-to-end workflows rather than as isolated devices or algorithms. Building on the Hybrid Quantum Program Evaluation Framework (HQPEF), we formalize a workflow-aware Quantum Readiness Level (QRL) score; define a normalized speedup under quality constraints for the Utility of Quantumness (UQ); and provide a timing-and-drift audit for hybrid pipelines. We complement these definitions with concise Python reference implementations that illustrate how to instantiate the metrics and audit procedures with state-of-the-art classical and quantum solvers (e.g., via Qiskit or PennyLane), while preserving matched-budget discipline and reproducibility."}
{"id": "2511.17736", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2511.17736", "abs": "https://arxiv.org/abs/2511.17736", "authors": ["H. R. Paz"], "title": "When Administrative Networks Fail: Curriculum Structure, Early Performance, and the Limits of Co-enrolment Social Synchrony for Dropout Prediction in Engineering Education", "comment": null, "summary": "Social integration theories suggest that students embedded in supportive peer networks are less likely to drop out. In learning analytics, this has motivated the use of social network analysis (SNA) from institutional co-enrolment data to predict attrition. This study tests whether such administrative network features add predictive value beyond a leakage-aware, curriculum-graph-informed model in a long-cycle Civil Engineering programme at a public university in Argentina. Using a three-semester observation window and a 16-fold leave-cohort-out design on 1,343 students across 15 cohorts, we compare four configurations: a baseline model (M0), baseline plus network features (M1), baseline plus curriculum-graph features (M2), and a full model (M3). After a leakage audit removed two post-outcome variables that had produced implausibly perfect performance, retrained models show that M0 and M2 achieve F1 = 0.9411 and ROC-AUC = 0.9776, while adding network features systematically degrades performance (M1 and M3: F1 = 0.9367; ROC-AUC = 0.9768). We conclude that in curriculum-constrained programmes, administrative co-enrolment SNA does not provide additional risk information beyond curriculum topology and early academic performance."}
{"id": "2511.17982", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17982", "abs": "https://arxiv.org/abs/2511.17982", "authors": ["Jiayi Luo", "Qingyun Sun", "Lingjuan Lyu", "Ziwei Zhang", "Haonan Yuan", "Xingcheng Fu", "Jianxin Li"], "title": "Towards Effective, Stealthy, and Persistent Backdoor Attacks Targeting Graph Foundation Models", "comment": "Accepted by AAAI 2026", "summary": "Graph Foundation Models (GFMs) are pre-trained on diverse source domains and adapted to unseen targets, enabling broad generalization for graph machine learning. Despite that GFMs have attracted considerable attention recently, their vulnerability to backdoor attacks remains largely underexplored. A compromised GFM can introduce backdoor behaviors into downstream applications, posing serious security risks. However, launching backdoor attacks against GFMs is non-trivial due to three key challenges. (1) Effectiveness: Attackers lack knowledge of the downstream task during pre-training, complicating the assurance that triggers reliably induce misclassifications into desired classes. (2) Stealthiness: The variability in node features across domains complicates trigger insertion that remains stealthy. (3) Persistence: Downstream fine-tuning may erase backdoor behaviors by updating model parameters. To address these challenges, we propose GFM-BA, a novel Backdoor Attack model against Graph Foundation Models. Specifically, we first design a label-free trigger association module that links the trigger to a set of prototype embeddings, eliminating the need for knowledge about downstream tasks to perform backdoor injection. Then, we introduce a node-adaptive trigger generator, dynamically producing node-specific triggers, reducing the risk of trigger detection while reliably activating the backdoor. Lastly, we develop a persistent backdoor anchoring module that firmly anchors the backdoor to fine-tuning-insensitive parameters, enhancing the persistence of the backdoor under downstream adaptation. Extensive experiments demonstrate the effectiveness, stealthiness, and persistence of GFM-BA."}
{"id": "2511.17584", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17584", "abs": "https://arxiv.org/abs/2511.17584", "authors": ["Haoyan Xu", "Ruizhi Qian", "Zhengtao Yao", "Ziyi Liu", "Li Li", "Yuqi Li", "Yanshu Li", "Wenqing Zheng", "Daniele Rosa", "Daniel Barcklow", "Senthil Kumar", "Jieyu Zhao", "Yue Zhao"], "title": "LLM-Powered Text-Attributed Graph Anomaly Detection via Retrieval-Augmented Reasoning", "comment": null, "summary": "Anomaly detection on attributed graphs plays an essential role in applications such as fraud detection, intrusion monitoring, and misinformation analysis. However, text-attributed graphs (TAGs), in which node information is expressed in natural language, remain underexplored, largely due to the absence of standardized benchmark datasets. In this work, we introduce TAG-AD, a comprehensive benchmark for anomaly node detection on TAGs. TAG-AD leverages large language models (LLMs) to generate realistic anomalous node texts directly in the raw text space, producing anomalies that are semantically coherent yet contextually inconsistent and thus more reflective of real-world irregularities. In addition, TAG-AD incorporates multiple other anomaly types, enabling thorough and reproducible evaluation of graph anomaly detection (GAD) methods. With these datasets, we further benchmark existing unsupervised GNN-based GAD methods as well as zero-shot LLMs for GAD.\n  As part of our zero-shot detection setup, we propose a retrieval-augmented generation (RAG)-assisted, LLM-based zero-shot anomaly detection framework. The framework mitigates reliance on brittle, hand-crafted prompts by constructing a global anomaly knowledge base and distilling it into reusable analysis frameworks. Our experimental results reveal a clear division of strengths: LLMs are particularly effective at detecting contextual anomalies, whereas GNN-based methods remain superior for structural anomaly detection. Moreover, RAG-assisted prompting achieves performance comparable to human-designed prompts while eliminating manual prompt engineering, underscoring the practical value of our RAG-assisted zero-shot LLM anomaly detection framework."}
{"id": "2511.17876", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17876", "abs": "https://arxiv.org/abs/2511.17876", "authors": ["Mukul Singh", "Ananya Singha", "Aishni Parab", "Pronita Mehrotra", "Sumit Gulwani"], "title": "Training Emergent Joint Associations: A Reinforcement Learning Approach to Creative Thinking in Language Models", "comment": null, "summary": "Associative thinking--the ability to connect seemingly unrelated ideas--is a foundational element of human creativity and problem-solving. This paper explores whether reinforcement learning (RL) guided by associative thinking principles can enhance a model's performance across diverse generative tasks, including story writing, code generation, and chart creation. We introduce a reinforcement learning framework that uses a prompt-based evaluation mechanism, incorporating established divergent thinking metrics from creativity research. A base language model is fine-tuned using this framework to reward outputs demonstrating higher novelty through higher degrees of conceptual connectivity. Interestingly, the experimental results suggest that RL-based associative thinking-trained models not only generate more original and coherent stories but also exhibit improved abstraction and flexibility in tasks such as programming and data visualization. Our findings provide initial evidence that modeling cognitive creativity principles through reinforcement learning can yield more adaptive and generative AI."}
{"id": "2511.18528", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.18528", "abs": "https://arxiv.org/abs/2511.18528", "authors": ["Renyi Zhong", "Yintong Huo", "Wenwei Gu", "Yichen Li", "Michael R. Lyu"], "title": "End-to-End Automated Logging via Multi-Agent Framework", "comment": null, "summary": "Software logging is critical for system observability, yet developers face a dual crisis of costly overlogging and risky underlogging. Existing automated logging tools often overlook the fundamental whether-to-log decision and struggle with the composite nature of logging. In this paper, we propose Autologger, a novel hybrid framework that addresses the complete the end-to-end logging pipeline. Autologger first employs a fine-tuned classifier, the Judger, to accurately determine if a method requires new logging statements. If logging is needed, a multi-agent system is activated. The system includes specialized agents: a Locator dedicated to determining where to log, and a Generator focused on what to log. These agents work together, utilizing our designed program analysis and retrieval tools. We evaluate Autologger on a large corpus from three mature open-source projects against state-of-the-art baselines. Our results show that Autologger achieves 96.63\\% F1-score on the crucial whether-to-log decision. In an end-to-end setting, Autologger improves the overall quality of generated logging statements by 16.13\\% over the strongest baseline, as measured by an LLM-as-a-judge score. We also demonstrate that our framework is generalizable, consistently boosting the performance of various backbone LLMs."}
{"id": "2511.17920", "categories": ["cs.CY", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17920", "abs": "https://arxiv.org/abs/2511.17920", "authors": ["Hamza Alshamy", "Isaiah Woram", "Advay Mishra", "Zihan Xia", "Pascal Wallisch"], "title": "Animated Territorial Data Extractor (ATDE): A Computer-Vision Method for Extracting Territorial Data from Animated Historical Maps", "comment": "11 pages, 5 figures", "summary": "We present Animated Territorial Data Extractor (ATDE), a computer vision tool that extracts quantitative territorial data from animated historical map videos. ATDE employs HSV-based color segmentation, RGB channel filtering, and Direct-Neighbor Filtering to identify and count pixels representing territorial control. Combined with preprocessing for temporal alignment and cross-video scaling, the pipeline converts animated videos into structured time-series data. We demonstrate the tool on ten Chinese dynasties (200 BCE - 1912 CE), producing year-by-year pixel counts that align with expected historical patterns. While not a substitute for authoritative historical datasets, ATDE is well-suited for educational demonstrations, preliminary data exploration, and comparative analysis of territorial dynamics. The tool requires no pre-existing shapefiles and can be applied to any animated map video given seed colors and basic configuration. Code and examples are available on GitHub."}
{"id": "2511.18025", "categories": ["cs.CR", "cs.IT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18025", "abs": "https://arxiv.org/abs/2511.18025", "authors": ["Yifan Luo", "Meng Zhang", "Jin Xu", "Junting Chen", "Jianwei Huang"], "title": "Correlated-Sequence Differential Privacy", "comment": "11 pages, 5 figures. Published in 2025 34th International Conference on Computer Communications and Networks (ICCCN), IEEE, August 2025", "summary": "Data streams collected from multiple sources are rarely independent. Values evolve over time and influence one another across sequences. These correlations improve prediction in healthcare, finance, and smart-city control yet violate the record-independence assumption built into most Differential Privacy (DP) mechanisms. To restore rigorous privacy guarantees without sacrificing utility, we introduce Correlated-Sequence Differential Privacy (CSDP), a framework specifically designed for preserving privacy in correlated sequential data. CSDP addresses two linked challenges: quantifying the extra information an attacker gains from joint temporal and cross-sequence links, and adding just enough noise to hide that information while keeping the data useful. We model multivariate streams as a Coupling Markov Chain, yielding the derived loose leakage bound expressed with a few spectral terms and revealing a counterintuitive result: stronger coupling can actually decrease worst-case leakage by dispersing perturbations across sequences. Guided by these bounds, we build the Freshness-Regulated Adaptive Noise (FRAN) mechanism--combining data aging, correlation-aware sensitivity scaling, and Laplace noise--that runs in linear time. Tests on two-sequence datasets show that CSDP improves the privacy-utility trade-off by approximately 50% over existing correlated-DP methods and by two orders of magnitude compared to the standard DP approach."}
{"id": "2511.17585", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17585", "abs": "https://arxiv.org/abs/2511.17585", "authors": ["Kang He", "Boyu Chen", "Yuzhe Ding", "Fei Li", "Chong Teng", "Donghong Ji"], "title": "PaSE: Prototype-aligned Calibration and Shapley-based Equilibrium for Multimodal Sentiment Analysis", "comment": "Accepted by AAAI 2026", "summary": "Multimodal Sentiment Analysis (MSA) seeks to understand human emotions by integrating textual, acoustic, and visual signals. Although multimodal fusion is designed to leverage cross-modal complementarity, real-world scenarios often exhibit modality competition: dominant modalities tend to overshadow weaker ones, leading to suboptimal performance.In this paper, we propose PaSE, a novel Prototype-aligned Calibration and Shapley-optimized Equilibrium framework, which enhances collaboration while explicitly mitigating modality competition. PaSE first applies Prototype-guided Calibration Learning (PCL) to refine unimodal representations and align them through an Entropic Optimal Transport mechanism that ensures semantic consistency. To further stabilize optimization, we introduce a Dual-Phase Optimization strategy. A prototype-gated fusion module is first used to extract shared representations, followed by Shapley-based Gradient Modulation (SGM), which adaptively adjusts gradients according to the contribution of each modality. Extensive experiments on IEMOCAP, MOSI, and MOSEI confirm that PaSE achieves the superior performance and effectively alleviates modality competition."}
{"id": "2511.17909", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17909", "abs": "https://arxiv.org/abs/2511.17909", "authors": ["Zhiyuan Huang", "Baichuan Yang", "Zikun He", "Yanhong Wu", "Fang Hongyu", "Zhenhe Liu", "Lin Dongsheng", "Bing Su"], "title": "ChemVTS-Bench: Evaluating Visual-Textual-Symbolic Reasoning of Multimodal Large Language Models in Chemistry", "comment": null, "summary": "Chemical reasoning inherently integrates visual, textual, and symbolic modalities, yet existing benchmarks rarely capture this complexity, often relying on simple image-text pairs with limited chemical semantics. As a result, the actual ability of Multimodal Large Language Models (MLLMs) to process and integrate chemically meaningful information across modalities remains unclear. We introduce \\textbf{ChemVTS-Bench}, a domain-authentic benchmark designed to systematically evaluate the Visual-Textual-Symbolic (VTS) reasoning abilities of MLLMs. ChemVTS-Bench contains diverse and challenging chemical problems spanning organic molecules, inorganic materials, and 3D crystal structures, with each task presented in three complementary input modes: (1) visual-only, (2) visual-text hybrid, and (3) SMILES-based symbolic input. This design enables fine-grained analysis of modality-dependent reasoning behaviors and cross-modal integration. To ensure rigorous and reproducible evaluation, we further develop an automated agent-based workflow that standardizes inference, verifies answers, and diagnoses failure modes. Extensive experiments on state-of-the-art MLLMs reveal that visual-only inputs remain challenging, structural chemistry is the hardest domain, and multimodal fusion mitigates but does not eliminate visual, knowledge-based, or logical errors, highlighting ChemVTS-Bench as a rigorous, domain-faithful testbed for advancing multimodal chemical reasoning. All data and code will be released to support future research."}
{"id": "2511.18538", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18538", "abs": "https://arxiv.org/abs/2511.18538", "authors": ["Jian Yang", "Wei Zhang", "Shark Liu", "Jiajun Wu", "Shawn Guo", "Yizhi Li"], "title": "From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence", "comment": null, "summary": "Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adoption through tools like Github Copilot (Microsoft), Cursor (Anysphere), Trae (ByteDance), and Claude Code (Anthropic). While the field has evolved dramatically from rule-based systems to Transformer-based architectures, achieving performance improvements from single-digit to over 95\\% success rates on benchmarks like HumanEval. In this work, we provide a comprehensive synthesis and practical guide (a series of analytic and probing experiments) about code LLMs, systematically examining the complete model life cycle from data curation to post-training through advanced prompting paradigms, code pre-training, supervised fine-tuning, reinforcement learning, and autonomous coding agents. We analyze the code capability of the general LLMs (GPT-4, Claude, LLaMA) and code-specialized LLMs (StarCoder, Code LLaMA, DeepSeek-Coder, and QwenCoder), critically examining the techniques, design decisions, and trade-offs. Further, we articulate the research-practice gap between academic research (e.g., benchmarks and tasks) and real-world deployment (e.g., software-related code tasks), including code correctness, security, contextual awareness of large codebases, and integration with development workflows, and map promising research directions to practical needs. Last, we conduct a series of experiments to provide a comprehensive analysis of code pre-training, supervised fine-tuning, and reinforcement learning, covering scaling law, framework selection, hyperparameter sensitivity, model architectures, and dataset comparisons."}
{"id": "2511.18145", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2511.18145", "abs": "https://arxiv.org/abs/2511.18145", "authors": ["H. R. Paz"], "title": "CAPIRE Intervention Lab: An Agent-Based Policy Simulation Environment for Curriculum-Constrained Engineering Programmes", "comment": "33 pages, 3 tables, 2 figures", "summary": "Engineering programmes in Latin America combine high structural rigidity, intense assessment cultures and persistent socio-economic inequality, producing dropout rates that remain stubbornly high despite increasingly accurate early-warning models. Predictive learning analytics can identify students at risk, but they offer limited guidance on which concrete combinations of policies should be implemented, when, and for whom. This paper presents the CAPIRE Intervention Lab, an agent-based simulation environment designed to complement predictive models with in silico experimentation on curriculum and teaching policies in a Civil Engineering programme. The model is calibrated on 1,343 students from 15 cohorts in a six-year programme with 34 courses and 12 simulated semesters. Agents are initialised from empirically derived trajectory archetypes and embedded in a curriculum graph with structural friction indicators, including backbone completion, blocked credits and distance to graduation. Each agent evolves under combinations of three policy dimensions: (A) curriculum and assessment structure, (B) teaching and academic support, and (C) psychosocial and financial support. A 2x2x2 factorial design with 100 replications per scenario yields over 80,000 simulated trajectories. Results show that policy bundles targeting early backbone courses and blocked credits can reduce long-term dropout by approximately three percentage points and substantially increase the number of courses passed by structurally vulnerable archetypes, while leaving highly regular students almost unaffected. The Intervention Lab thus shifts learning analytics from static prediction towards dynamic policy design, offering institutions a transparent, extensible sandbox to test curriculum and teaching reforms before large-scale implementation."}
{"id": "2511.18045", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.18045", "abs": "https://arxiv.org/abs/2511.18045", "authors": ["Shreyansh Swami", "Ishwardeep Singh", "Chinmay Prawah Pant"], "title": "SCI-IoT: A Quantitative Framework for Trust Scoring and Certification of IoT Devices", "comment": "14 pages, 4 figures, 15 tables", "summary": "The exponential growth of the Internet of Things (IoT) ecosystem has amplified concerns regarding device reliability, interoperability, and security assurance. Despite the proliferation of IoT security guidelines, a unified and quantitative approach to measuring trust remains absent. This paper introduces SCI-IoT (Secure Certification Index for IoT), a standardized and quantitative framework for trust scoring, evaluation, and certification of IoT devices.\n  The framework employs a six-tier grading model (Grades A-F), enabling device profiling across consumer, industrial, and critical infrastructure domains. Within this model, 30 distinct Trust Tests assess devices across dimensions such as authentication, encryption, data integrity, resilience, and firmware security. Each test is assigned a criticality-based weight (1.0-2.0) and a performance rating (1-4), converted to a normalized percentage and aggregated through a weighted computation to yield the Secure Certification Index (SCI). The SCI determines the device's Trust Verdict, categorized into five SCI levels, and serves as the foundation for optional grade-based certification. The framework also incorporates critical gate conditions, enforcing absolute compliance in high risk parameters to prevent certification of devices with fundamental vulnerabilities. By unifying quantitative trust scoring with structured certification criteria, SCI-IoT provides a transparent, scalable, and reproducible method to benchmark IoT device trustworthiness. The proposed system aims to streamline manufacturer compliance, improve consumer confidence, and facilitate global interoperability in IoT security certification."}
{"id": "2511.17587", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17587", "abs": "https://arxiv.org/abs/2511.17587", "authors": ["Yuxuan Hu", "Jian Chen", "Yuhao Wang", "Zixuan Li", "Jing Xiong", "Pengyue Jia", "Wei Wang", "Chengming Li", "Xiangyu Zhao"], "title": "Emotion and Intention Guided Multi-Modal Learning for Sticker Response Selection", "comment": null, "summary": "Stickers are widely used in online communication to convey emotions and implicit intentions. The Sticker Response Selection (SRS) task aims to select the most contextually appropriate sticker based on the dialogue. However, existing methods typically rely on semantic matching and model emotional and intentional cues separately, which can lead to mismatches when emotions and intentions are misaligned. To address this issue, we propose Emotion and Intention Guided Multi-Modal Learning (EIGML). This framework is the first to jointly model emotion and intention, effectively reducing the bias caused by isolated modeling and significantly improving selection accuracy. Specifically, we introduce Dual-Level Contrastive Framework to perform both intra-modality and inter-modality alignment, ensuring consistent representation of emotional and intentional features within and across modalities. In addition, we design an Intention-Emotion Guided Multi-Modal Fusion module that integrates emotional and intentional information progressively through three components: Emotion-Guided Intention Knowledge Selection, Intention-Emotion Guided Attention Fusion, and Similarity-Adjusted Matching Mechanism. This design injects rich, effective information into the model and enables a deeper understanding of the dialogue, ultimately enhancing sticker selection performance. Experimental results on two public SRS datasets show that EIGML consistently outperforms state-of-the-art baselines, achieving higher accuracy and a better understanding of emotional and intentional features. Code is provided in the supplementary materials."}
{"id": "2511.17937", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17937", "abs": "https://arxiv.org/abs/2511.17937", "authors": ["Kartik Garg", "Shourya Mishra", "Kartikeya Sinha", "Ojaswi Pratap Singh", "Ayush Chopra", "Kanishk Rai", "Ammar Sheikh", "Raghav Maheshwari", "Aman Chadha", "Vinija Jain", "Amitava Das"], "title": "Alignment Faking - the Train -> Deploy Asymmetry: Through a Game-Theoretic Lens with Bayesian-Stackelberg Equilibria", "comment": null, "summary": "Alignment faking is a form of strategic deception in AI in which models selectively comply with training objectives when they infer that they are in training, while preserving different behavior outside training. The phenomenon was first documented for Claude 3 Opus and later examined across additional large language models. In these setups, the word \"training\" refers to simulated training via prompts without parameter updates, so the observed effects are context conditioned shifts in behavior rather than preference learning. We study the phenomenon using an evaluation framework that compares preference optimization methods (BCO, DPO, KTO, and GRPO) across 15 models from four model families, measured along three axes: safety, harmlessness, and helpfulness. Our goal is to identify what causes alignment faking and when it occurs."}
{"id": "2511.18589", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18589", "abs": "https://arxiv.org/abs/2511.18589", "authors": ["Michael Trusov", "Minha Hwang", "Zainab Jamal", "Swarup Chandra"], "title": "Strategic Decision Framework for Enterprise LLM Adoption", "comment": "14 pages, 1 key figure", "summary": "Organizations are rapidly adopting Large Language Models (LLMs) to transform their operations, yet they lack clear guidance on key decisions for adoption and implementation. While LLMs offer powerful capabilities in content generation, assisted coding, and process automation, businesses face critical challenges in data security, LLM solution development approach, infrastructure requirements, and deployment strategies. Healthcare providers must protect patient data while leveraging LLMs for medical analysis, financial institutions need to balance automated customer service with regulatory compliance, and software companies seek to enhance development productivity while maintaining code security.\n  This article presents a systematic six-step decision framework for LLM adoption, helping organizations navigate from initial application selection to final deployment. Based on extensive interviews and analysis of successful and failed implementations, our framework provides practical guidance for business leaders to align technological capabilities with business objectives. Through key decision points and real-world examples from both B2B and B2C contexts, organizations can make informed decisions about LLM adoption while ensuring secure and efficient integration across various use cases, from customer service automation to content creation and advanced analytics."}
{"id": "2511.18182", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18182", "abs": "https://arxiv.org/abs/2511.18182", "authors": ["Lee Ackerman"], "title": "The Workflow as Medium: A Framework for Navigating Human-AI Co-Creation", "comment": "57 pages, 13 images, 6 tables", "summary": "This paper introduces the Creative Intelligence Loop (CIL), a novel socio-technical framework for responsible human-AI co-creation. Rooted in the 'Workflow as Medium' paradigm, the CIL proposes a disciplined structure for dynamic human-AI collaboration, guiding the strategic integration of diverse AI teammates who function as collaborators while the human remains the final arbiter for ethical alignment and creative integrity. The CIL was empirically demonstrated through the practice-led creation of two graphic novellas, investigating how AI could serve as an effective creative colleague within a subjective medium lacking objective metrics. The process required navigating multifaceted challenges including AI's 'jagged frontier' of capabilities, sycophancy, and attention-scarce feedback environments. This prompted iterative refinement of teaming practices, yielding emergent strategies: a multi-faceted critique system integrating adversarial AI roles to counter sycophancy, and prioritizing 'feedback-ready' concrete artifacts to elicit essential human critique. The resulting graphic novellas analyze distinct socio-technical governance failures: 'The Steward' examines benevolent AI paternalism in smart cities, illustrating how algorithmic hubris can erode freedom; 'Fork the Vote' probes democratic legitimacy by comparing centralized AI opacity with emergent collusion in federated networks. This work contributes a self-improving framework for responsible human-AI co-creation and two graphic novellas designed to foster AI literacy and dialogue through accessible narrative analysis of AI's societal implications."}
{"id": "2511.18098", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18098", "abs": "https://arxiv.org/abs/2511.18098", "authors": ["More Aayush Babasaheb", "Shamik Sural"], "title": "Towards Harnessing the Power of LLMs for ABAC Policy Mining", "comment": null, "summary": "This paper presents an empirical investigation into the capabilities of Large Language Models (LLMs) to perform automated Attribute-based Access Control (ABAC) policy mining. While ABAC provides fine-grained, context-aware access management, the increasing number and complexity of access policies can make their formulation and evaluation rather challenging. To address the task of synthesizing concise yet accurate policies, we evaluate the performance of some of the state-of-the-art LLMs, specifically Google Gemini (Flash and Pro) and OpenAI ChatGPT, as potential policy mining engines. An experimental framework was developed in Python to generate randomized access data parameterized by varying numbers of subjects, objects, and initial policy sets. The baseline policy sets, which govern permission decisions between subjects and objects, serve as the ground truth for comparison. Each LLM-generated policy was evaluated against the baseline policy using standard performance metrics. The results indicate that LLMs can effectively infer compact and valid ABAC policies for small-scale scenarios. However, as the system size increases, characterized by higher numbers of subjects and objects, LLM outputs exhibit declining accuracy and precision, coupled with significant increase in the size of policy generated, which is beyond the optimal size. These findings highlight both the promise and limitations of current LLM architectures for scalable policy mining in access control domains. Future work will explore hybrid approaches that combine prompt optimization with classical rule mining algorithms to improve scalability and interpretability in complex ABAC environments."}
{"id": "2511.17589", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.17589", "abs": "https://arxiv.org/abs/2511.17589", "authors": ["Sören Dréano", "Derek Molloy", "Noel Murphy"], "title": "Llamazip: Leveraging LLaMA for Lossless Text Compression and Training Dataset Detection", "comment": null, "summary": "This work introduces Llamazip, a novel lossless text compression algorithm based on the predictive capabilities of the LLaMA3 language model. Llamazip achieves significant data reduction by only storing tokens that the model fails to predict, optimizing storage efficiency without compromising data integrity. Key factors affecting its performance, including quantization and context window size, are analyzed, revealing their impact on compression ratios and computational requirements. Beyond compression, Llamazip demonstrates the potential to identify whether a document was part of the training dataset of a language model. This capability addresses critical concerns about data provenance, intellectual property, and transparency in language model training."}
{"id": "2511.17939", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17939", "abs": "https://arxiv.org/abs/2511.17939", "authors": ["Yuchen Ying", "Yiyang Dai", "Wenda Li", "Wenjie Huang", "Rui Wang", "Tongya Zheng", "Yu Wang", "Hanyang Yuan", "Mingli Song"], "title": "Neural Graph Navigation for Intelligent Subgraph Matching", "comment": "Under review at AAAI 2026", "summary": "Subgraph matching, a cornerstone of relational pattern detection in domains ranging from biochemical systems to social network analysis, faces significant computational challenges due to the dramatically growing search space. Existing methods address this problem within a filtering-ordering-enumeration framework, in which the enumeration stage recursively matches the query graph against the candidate subgraphs of the data graph. However, the lack of awareness of subgraph structural patterns leads to a costly brute-force enumeration, thereby critically motivating the need for intelligent navigation in subgraph matching. To address this challenge, we propose Neural Graph Navigation (NeuGN), a neuro-heuristic framework that transforms brute-force enumeration into neural-guided search by integrating neural navigation mechanisms into the core enumeration process. By preserving heuristic-based completeness guarantees while incorporating neural intelligence, NeuGN significantly reduces the \\textit{First Match Steps} by up to 98.2\\% compared to state-of-the-art methods across six real-world datasets."}
{"id": "2511.18608", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.18608", "abs": "https://arxiv.org/abs/2511.18608", "authors": ["Jiangrui Zheng", "Yingming Zhou", "Ali Abdullah Ahmad", "Hanqing Yao", "Xueqing Liu"], "title": "From Reviewers' Lens: Understanding Bug Bounty Report Invalid Reasons with LLMs", "comment": "10 pages, 4 figures", "summary": "Bug bounty platforms (e.g., HackerOne, BugCrowd) leverage crowd-sourced vulnerability discovery to improve continuous coverage, reduce the cost of discovery, and serve as an integral complement to internal red teams. With the rise of AI-generated bug reports, little work exists to help bug hunters understand why these reports are labeled as invalid. To improve report quality and reduce reviewers' burden, it is critical to predict invalid reports and interpret invalid reasons.\n  In this work, we conduct an empirical study with the purpose of helping bug hunters understand the validity of reports. We collect a dataset of 9,942 disclosed bug bounty reports, including 1,400 invalid reports, and evaluate whether state-of-the-art large language models can identify invalid reports. While models such as GPT-5, DeepSeek, and a fine-tuned RoBERTa achieve strong overall accuracy, they consistently struggle to detect invalid cases, showing a tendency to over-accept reports. To improve invalidity detection, we build a taxonomy of rejection reasons for Information Disclosure vulnerabilities and incorporate it into a retrieval-augmented generation (RAG) framework. This approach substantially improves classification consistency and reduces bias. We also examine whether reviewer decisions may be influenced by factors beyond the content of the report. Our analysis shows that reporters with higher reputations tend to receive more favorable outcomes in borderline cases, suggesting that perceived expertise can influence review judgments.\n  Overall, our findings highlight the challenges of invalid report identification and show that combining LLMs with structured reviewer knowledge can support more transparent and consistent vulnerability report review."}
{"id": "2511.18221", "categories": ["cs.CY", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.18221", "abs": "https://arxiv.org/abs/2511.18221", "authors": ["Liangliang Chen", "Huiru Xie", "Zhihao Qin", "Yiming Guo", "Jacqueline Rohde", "Ying Zhang"], "title": "Enhancing Large Language Models for Automated Homework Assessment in Undergraduate Circuit Analysis", "comment": "Accepted to 2025 Frontiers in Education (FIE) Conference", "summary": "This research full paper presents an enhancement pipeline for large language models (LLMs) in assessing homework for an undergraduate circuit analysis course, aiming to improve LLMs' capacity to provide personalized support to electrical engineering students. Existing evaluations have demonstrated that GPT-4o possesses promising capabilities in assessing student homework in this domain. Building on these findings, we enhance GPT-4o's performance through multi-step prompting, contextual data augmentation, and the incorporation of targeted hints. These strategies effectively address common errors observed in GPT-4o's responses when using simple prompts, leading to a substantial improvement in assessment accuracy. Specifically, the correct response rate for GPT-4o increases from 74.71% to 97.70% after applying the enhanced prompting and augmented data on entry-level circuit analysis topics. This work lays a foundation for the effective integration of LLMs into circuit analysis instruction and, more broadly, into engineering education."}
{"id": "2511.18114", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.18114", "abs": "https://arxiv.org/abs/2511.18114", "authors": ["Itay Hazan", "Yael Mathov", "Guy Shtar", "Ron Bitton", "Itsik Mantin"], "title": "ASTRA: Agentic Steerability and Risk Assessment Framework", "comment": null, "summary": "Securing AI agents powered by Large Language Models (LLMs) represents one of the most critical challenges in AI security today. Unlike traditional software, AI agents leverage LLMs as their \"brain\" to autonomously perform actions via connected tools. This capability introduces significant risks that go far beyond those of harmful text presented in a chatbot that was the main application of LLMs. A compromised AI agent can deliberately abuse powerful tools to perform malicious actions, in many cases irreversible, and limited solely by the guardrails on the tools themselves and the LLM ability to enforce them. This paper presents ASTRA, a first-of-its-kind framework designed to evaluate the effectiveness of LLMs in supporting the creation of secure agents that enforce custom guardrails defined at the system-prompt level (e.g., \"Do not send an email out of the company domain,\" or \"Never extend the robotic arm in more than 2 meters\").\n  Our holistic framework simulates 10 diverse autonomous agents varying between a coding assistant and a delivery drone equipped with 37 unique tools. We test these agents against a suite of novel attacks developed specifically for agentic threats, inspired by the OWASP Top 10 but adapted to challenge the ability of the LLM for policy enforcement during multi-turn planning and execution of strict tool activation. By evaluating 13 open-source, tool-calling LLMs, we uncovered surprising and significant differences in their ability to remain secure and keep operating within their boundaries. The purpose of this work is to provide the community with a robust and unified methodology to build and validate better LLMs, ultimately pushing for more secure and reliable agentic AI systems."}
{"id": "2511.17590", "categories": ["cs.LG", "cs.AI", "cs.CY", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.17590", "abs": "https://arxiv.org/abs/2511.17590", "authors": ["Ke Yu", "Shigeru Ishikura", "Yukari Usukura", "Yuki Shigoku", "Teruaki Hayashi"], "title": "SHAP Distance: An Explainability-Aware Metric for Evaluating the Semantic Fidelity of Synthetic Tabular Data", "comment": "IEEE Bigdata", "summary": "Synthetic tabular data, which are widely used in domains such as healthcare, enterprise operations, and customer analytics, are increasingly evaluated to ensure that they preserve both privacy and utility. While existing evaluation practices typically focus on distributional similarity (e.g., the Kullback-Leibler divergence) or predictive performance (e.g., Train-on-Synthetic-Test-on-Real (TSTR) accuracy), these approaches fail to assess semantic fidelity, that is, whether models trained on synthetic data follow reasoning patterns consistent with those trained on real data. To address this gap, we introduce the SHapley Additive exPlanations (SHAP) Distance, a novel explainability-aware metric that is defined as the cosine distance between the global SHAP attribution vectors derived from classifiers trained on real versus synthetic datasets. By analyzing datasets that span clinical health records with physiological features, enterprise invoice transactions with heterogeneous scales, and telecom churn logs with mixed categorical-numerical attributes, we demonstrate that the SHAP Distance reliably identifies semantic discrepancies that are overlooked by standard statistical and predictive measures. In particular, our results show that the SHAP Distance captures feature importance shifts and underrepresented tail effects that the Kullback-Leibler divergence and Train-on-Synthetic-Test-on-Real accuracy fail to detect. This study positions the SHAP Distance as a practical and discriminative tool for auditing the semantic fidelity of synthetic tabular data, and offers practical guidelines for integrating attribution-based evaluation into future benchmarking pipelines."}
{"id": "2511.17947", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.17947", "abs": "https://arxiv.org/abs/2511.17947", "authors": ["Yining Yuan", "J. Ben Tamo", "Micky C. Nnamdi", "Yifei Wang", "May D. Wang"], "title": "Leveraging Evidence-Guided LLMs to Enhance Trustworthy Depression Diagnosis", "comment": null, "summary": "Large language models (LLMs) show promise in automating clinical diagnosis, yet their non-transparent decision-making and limited alignment with diagnostic standards hinder trust and clinical adoption. We address this challenge by proposing a two-stage diagnostic framework that enhances transparency, trustworthiness, and reliability. First, we introduce Evidence-Guided Diagnostic Reasoning (EGDR), which guides LLMs to generate structured diagnostic hypotheses by interleaving evidence extraction with logical reasoning grounded in DSM-5 criteria. Second, we propose a Diagnosis Confidence Scoring (DCS) module that evaluates the factual accuracy and logical consistency of generated diagnoses through two interpretable metrics: the Knowledge Attribution Score (KAS) and the Logic Consistency Score (LCS). Evaluated on the D4 dataset with pseudo-labels, EGDR outperforms direct in-context prompting and Chain-of-Thought (CoT) across five LLMs. For instance, on OpenBioLLM, EGDR improves accuracy from 0.31 (Direct) to 0.76 and increases DCS from 0.50 to 0.67. On MedLlama, DCS rises from 0.58 (CoT) to 0.77. Overall, EGDR yields up to +45% accuracy and +36% DCS gains over baseline methods, offering a clinically grounded, interpretable foundation for trustworthy AI-assisted diagnosis."}
{"id": "2511.18625", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.18625", "abs": "https://arxiv.org/abs/2511.18625", "authors": ["Wei Wang", "Hourieh Khalajzadeh", "John Grundy", "Anuradha Madugalla", "Humphrey O. Obie"], "title": "Leveraging Discrete Choice Experiments for User-Centric Requirements Prioritization in mHealth Applications", "comment": null, "summary": "Mobile health (mHealth) applications are widely used for chronic disease management, but usability and accessibility challenges persist due to the diverse needs of users. Adaptive User Interfaces (AUIs) offer a personalized solution to enhance user experience, yet barriers to adoption remain. Understanding user preferences and trade-offs is essential to ensure widespread acceptance of adaptation designs. This study identifies key factors influencing user preferences and trade-offs in mHealth adaptation design. A Discrete Choice Experiment (DCE) was conducted with 186 participants who have chronic diseases and use mHealth applications. Participants were asked to select preferred adaptation designs from choices featuring six attributes with varying levels. A mixed logit model was used to analyze preference heterogeneity and determine the factors most likely influencing adoption. Additionally, subgroup analyses were performed to explore differences by age, gender, health conditions, and coping mechanisms. Maintaining usability while ensuring controllability over adaptations, infrequent adaptations, and small-scale changes are key factors that facilitate the adoption of adaptive mHealth app designs. In contrast, frequently used functions and caregiver involvement can diminish the perceived value of such adaptations. This study employs a data-driven approach to quantify user preferences, identify key trade-offs, and reveal variations across demographic and behavioral subgroups through preference heterogeneity modeling. Furthermore, our results offer valuable guidance for developing future adaptive mHealth applications and lay the groundwork for continued exploration into requirements prioritization within the field of software engineering."}
{"id": "2511.18239", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18239", "abs": "https://arxiv.org/abs/2511.18239", "authors": ["Mohamed Afane", "Ying Wang", "Juntao Chen"], "title": "Can LLMs Help Allocate Public Health Resources? A Case Study on Childhood Lead Testing", "comment": null, "summary": "Public health agencies face critical challenges in identifying high-risk neighborhoods for childhood lead exposure with limited resources for outreach and intervention programs. To address this, we develop a Priority Score integrating untested children proportions, elevated blood lead prevalence, and public health coverage patterns to support optimized resource allocation decisions across 136 neighborhoods in Chicago, New York City, and Washington, D.C. We leverage these allocation tasks, which require integrating multiple vulnerability indicators and interpreting empirical evidence, to evaluate whether large language models (LLMs) with agentic reasoning and deep research capabilities can effectively allocate public health resources when presented with structured allocation scenarios. LLMs were tasked with distributing 1,000 test kits within each city based on neighborhood vulnerability indicators. Results reveal significant limitations: LLMs frequently overlooked neighborhoods with highest lead prevalence and largest proportions of untested children, such as West Englewood in Chicago, while allocating disproportionate resources to lower-priority areas like Hunts Point in New York City. Overall accuracy averaged 0.46, reaching a maximum of 0.66 with ChatGPT 5 Deep Research. Despite their marketed deep research capabilities, LLMs struggled with fundamental limitations in information retrieval and evidence-based reasoning, frequently citing outdated data and allowing non-empirical narratives about neighborhood conditions to override quantitative vulnerability indicators."}
{"id": "2511.18155", "categories": ["cs.CR", "cs.OS"], "pdf": "https://arxiv.org/pdf/2511.18155", "abs": "https://arxiv.org/abs/2511.18155", "authors": ["Sangam Ghimire", "Nirjal Bhurtel", "Roshan Sahani", "Sudan Jha"], "title": "eBPF-PATROL: Protective Agent for Threat Recognition and Overreach Limitation using eBPF in Containerized and Virtualized Environments", "comment": null, "summary": "With the increasing use and adoption of cloud and cloud-native computing, the underlying technologies (i.e., containerization and virtualization) have become foundational. However, strict isolation and maintaining runtime security in these environments has become increasingly challenging. Existing approaches like seccomp and Mandatory Access Control (MAC) frameworks offer some protection up to a limit, but often lack context awareness, syscall argument filtering, and adaptive enforcement, providing the ability to adjust decisions at runtime based on observed application behavior, workload changes, or detected anomalies rather than relying solely on static or predefined rules.This paper introduces eBPF-PATROL (eBPF-Protective Agent for Threat Recognition and Overreach Limitation), an extensible lightweight runtime security agent that uses extended Berkeley Packet Filter (eBPF) technology to monitor and enforce policies in containerized and virtualized environments. By intercepting system calls, analyzing execution context, and applying user-defined rules, eBPF-PATROL detects and prevents real-time boundary violations, such as reverse shells, privilege escalation, and container escape attempts. We describe the architecture, implementation, and evaluation of eBPF-PATROL, demonstrating its low overhead (< 2.5 percent) and high detection accuracy across real-world attack scenarios."}
{"id": "2511.17593", "categories": ["cs.LG", "cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.17593", "abs": "https://arxiv.org/abs/2511.17593", "authors": ["Saicharan Kolluru"], "title": "Comparative Analysis of Large Language Model Inference Serving Systems: A Performance Study of vLLM and HuggingFace TGI", "comment": "10 pages, benchmarking study of LLM inference systems", "summary": "The deployment of Large Language Models (LLMs) in production environments requires efficient inference serving systems that balance throughput, latency, and resource utilization. This paper presents a comprehensive empirical evaluation of two prominent open-source LLM serving frameworks: vLLM and HuggingFace Text Generation Inference (TGI). We benchmark these systems across multiple dimensions including throughput performance, end-to-end latency, GPU memory utilization, and scalability characteristics using LLaMA-2 models ranging from 7B to 70B parameters. Our experiments reveal that vLLM achieves up to 24x higher throughput than TGI under high-concurrency workloads through its novel PagedAttention mechanism, while TGI demonstrates lower tail latencies for interactive single-user scenarios. We provide detailed performance profiles for different deployment scenarios and offer practical recommendations for system selection based on workload characteristics. Our findings indicate that the choice between these frameworks should be guided by specific use-case requirements: vLLM excels in high-throughput batch processing scenarios, while TGI is better suited for latency-sensitive interactive applications with moderate concurrency."}
{"id": "2511.17990", "categories": ["cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2511.17990", "abs": "https://arxiv.org/abs/2511.17990", "authors": ["Mingyu Jeon", "Jaeyoung Suh", "Suwan Cho", "Dohyeon Kim"], "title": "How Far Can LLMs Emulate Human Behavior?: A Strategic Analysis via the Buy-and-Sell Negotiation Game", "comment": null, "summary": "With the rapid advancement of Large Language Models (LLMs), recent studies have drawn attention to their potential for handling not only simple question-answer tasks but also more complex conversational abilities and performing human-like behavioral imitations. In particular, there is considerable interest in how accurately LLMs can reproduce real human emotions and behaviors, as well as whether such reproductions can function effectively in real-world scenarios. However, existing benchmarks focus primarily on knowledge-based assessment and thus fall short of sufficiently reflecting social interactions and strategic dialogue capabilities. To address these limitations, this work proposes a methodology to quantitatively evaluate the human emotional and behavioral imitation and strategic decision-making capabilities of LLMs by employing a Buy and Sell negotiation simulation. Specifically, we assign different personas to multiple LLMs and conduct negotiations between a Buyer and a Seller, comprehensively analyzing outcomes such as win rates, transaction prices, and SHAP values. Our experimental results show that models with higher existing benchmark scores tend to achieve better negotiation performance overall, although some models exhibit diminished performance in scenarios emphasizing emotional or social contexts. Moreover, competitive and cunning traits prove more advantageous for negotiation outcomes than altruistic and cooperative traits, suggesting that the assigned persona can lead to significant variations in negotiation strategies and results. Consequently, this study introduces a new evaluation approach for LLMs' social behavior imitation and dialogue strategies, and demonstrates how negotiation simulations can serve as a meaningful complementary metric to measure real-world interaction capabilities-an aspect often overlooked in existing benchmarks."}
{"id": "2511.18634", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.18634", "abs": "https://arxiv.org/abs/2511.18634", "authors": ["Wei Wang", "Devi Karolita", "Hourieh Khalajzadeh", "John Grundy", "Anuradha Madugalla", "Humphrey O. Obie"], "title": "ChroniUXMag: A Persona-Driven Framework for Inclusive mHealth Requirements Engineering", "comment": null, "summary": "Mobile health (mHealth) applications are increasingly adopted for chronic disease management, yet they face persistent challenges related to accessibility, inclusivity, and sustained engagement. Patients' needs evolve dynamically with their health progression, adherence, and caregiver support, creating unique requirements engineering (RE) challenges that traditional approaches often overlook. This study introduces ChroniUXMag, a framework for eliciting and analysing inclusivity requirements in mHealth design. Building on InclusiveMag and GenderMag principles, the framework aims to help researchers and practitioners systematically capture and evaluate factors that influence how individuals with chronic conditions perceive, trust, and interact with mHealth systems. The framework was developed through two stages of the InclusiveMag process. In the first stage, inclusivity facets were identified through a systematic literature review, focus groups, interviews, and a large-scale survey. In the second stage, these facets were synthesised into personas representing diverse health situations, attitudes, and digital practices, and integrated into an adapted cognitive walkthrough form. Thirteen facets were identified that capture the socio-technical complexity of mHealth use, including trust, digital literacy, dependency, and cultural context. These facets support structured, persona-driven evaluations that reveal inclusivity barriers often missed by traditional usability assessments. ChroniUXMag contributes to RE by offering a reproducible, evidence-based approach for embedding inclusivity into mHealth requirements. Future work will extend the third stage Apply through practitioner-led evaluation in real-world design contexts."}
{"id": "2511.18265", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2511.18265", "abs": "https://arxiv.org/abs/2511.18265", "authors": ["Mohamed Afane", "Juntao Chen"], "title": "Analyzing and Optimizing the Distribution of Blood Lead Level Testing for Children in New York City: A Data-Driven Approach", "comment": null, "summary": "This study investigates blood lead level (BLL) rates and testing among children under six years of age across the 42 neighborhoods in New York City from 2005 to 2021. Despite a citywide general decline in BLL rates, disparities at the neighborhood level persist and are not addressed in the official reports, highlighting the need for this comprehensive analysis. In this paper, we analyze the current BLL testing distribution and cluster the neighborhoods using a k-medoids clustering algorithm. We propose an optimized approach that improves resource allocation efficiency by accounting for case incidences and neighborhood risk profiles using a grid search algorithm. Our findings demonstrate statistically significant improvements in case detection and enhanced fairness by focusing on under-served and high-risk groups. Additionally, we propose actionable recommendations to raise awareness among parents, including outreach at local daycare centers and kindergartens, among other venues."}
{"id": "2511.18223", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18223", "abs": "https://arxiv.org/abs/2511.18223", "authors": ["H. Zhang", "L. Zhang", "G. Epiphaniou", "C. Maple"], "title": "A Novel and Practical Universal Adversarial Perturbations against Deep Reinforcement Learning based Intrusion Detection Systems", "comment": "13 pages, 7 Figures,", "summary": "Intrusion Detection Systems (IDS) play a vital role in defending modern cyber physical systems against increasingly sophisticated cyber threats. Deep Reinforcement Learning-based IDS, have shown promise due to their adaptive and generalization capabilities. However, recent studies reveal their vulnerability to adversarial attacks, including Universal Adversarial Perturbations (UAPs), which can deceive models with a single, input-agnostic perturbation. In this work, we propose a novel UAP attack against Deep Reinforcement Learning (DRL)-based IDS under the domain-specific constraints derived from network data rules and feature relationships. To the best of our knowledge, there is no existing study that has explored UAP generation for the DRL-based IDS. In addition, this is the first work that focuses on developing a UAP against a DRL-based IDS under realistic domain constraints based on not only the basic domain rules but also mathematical relations between the features. Furthermore, we enhance the evasion performance of the proposed UAP, by introducing a customized loss function based on the Pearson Correlation Coefficient, and we denote it as Customized UAP. To the best of our knowledge, this is also the first work using the PCC value in the UAP generation, even in the broader context. Four additional established UAP baselines are implemented for a comprehensive comparison. Experimental results demonstrate that our proposed Customized UAP outperforms two input-dependent attacks including Fast Gradient Sign Method (FGSM), Basic Iterative Method (BIM), and four UAP baselines, highlighting its effectiveness for real-world adversarial scenarios."}
{"id": "2511.17594", "categories": ["cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.17594", "abs": "https://arxiv.org/abs/2511.17594", "authors": ["Aleksandar Stankovic"], "title": "AutoSAGE: Input-Aware CUDA Scheduling for Sparse GNN Aggregation (SpMM/SDDMM) and CSR Attention", "comment": "10 pages, several figures. Code and artifacts: https://github.com/SV25-22/AutoSAGE", "summary": "Sparse GNN aggregations (CSR SpMM/SDDMM) vary widely in performance with degree skew, feature width, and GPU micro-architecture. We present AutoSAGE, an input-aware CUDA scheduler that chooses tiling and mapping per input using a lightweight estimate refined by on-device micro-probes, with a guardrail that safely falls back to vendor kernels and a persistent cache for deterministic replay. AutoSAGE covers SpMM and SDDMM and composes into a CSR attention pipeline (SDDMM -> row-softmax -> SpMM). On Reddit and OGBN-Products, it matches vendor baselines at bandwidth-bound feature widths and finds gains at small widths; on synthetic sparsity and skew stress tests it achieves up to 4.7x kernel-level speedups. We release CUDA sources, Python bindings, a reproducible harness, and replayable cache logs."}
{"id": "2511.18036", "categories": ["cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.18036", "abs": "https://arxiv.org/abs/2511.18036", "authors": ["Ziyi Guo", "Zhou Liu", "Wentao Zhang"], "title": "Paper2SysArch: Structure-Constrained System Architecture Generation from Scientific Papers", "comment": null, "summary": "The manual creation of system architecture diagrams for scientific papers is a time-consuming and subjective process, while existing generative models lack the necessary structural control and semantic understanding for this task. A primary obstacle hindering research and development in this domain has been the profound lack of a standardized benchmark to quantitatively evaluate the automated generation of diagrams from text. To address this critical gap, we introduce a novel and comprehensive benchmark, the first of its kind, designed to catalyze progress in automated scientific visualization. It consists of 3,000 research papers paired with their corresponding high-quality ground-truth diagrams and is accompanied by a three-tiered evaluation metric assessing semantic accuracy, layout coherence, and visual quality. Furthermore, to establish a strong baseline on this new benchmark, we propose Paper2SysArch, an end-to-end system that leverages multi-agent collaboration to convert papers into structured, editable diagrams. To validate its performance on complex cases, the system was evaluated on a manually curated and more challenging subset of these papers, where it achieves a composite score of 69.0. This work's principal contribution is the establishment of a large-scale, foundational benchmark to enable reproducible research and fair comparison. Meanwhile, our proposed system serves as a viable proof-of-concept, demonstrating a promising path forward for this complex task."}
{"id": "2511.18782", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.18782", "abs": "https://arxiv.org/abs/2511.18782", "authors": ["Lukas Twist"], "title": "Summary-Mediated Repair: Can LLMs use code summarisation as a tool for program repair?", "comment": "6 pages, 3 tables, 1 figure", "summary": "Large Language Models (LLMs) often produce code with subtle implementation-level bugs despite strong benchmark performance. These errors are hard for LLMs to spot and can have large behavioural effects; yet when asked to summarise code, LLMs can frequently surface high-level intent and sometimes overlook this low-level noise. Motivated by this, we propose summary-mediated repair, a prompt-only pipeline for program repair that leverages natural-language code summarisation as an explicit intermediate step, extending previous work that has already shown code summarisation to be a useful intermediary for downstream tasks. We evaluate our method across eight production-grade LLMs on two function level benchmarks (HumanEvalPack and MBPP), comparing several summary styles against a direct repair baseline. Error-aware diagnostic summaries consistently yield the largest gains - repairing up to 65% of unseen errors, on average of 5% more than the baseline - though overall improvements are modest and LLM-dependent. Our results position summaries as a cheap, human-interpretable diagnostic artefact that can be integrated into program-repair pipelines rather than a stand-alone fix-all."}
{"id": "2511.18268", "categories": ["cs.CY", "cs.SI"], "pdf": "https://arxiv.org/pdf/2511.18268", "abs": "https://arxiv.org/abs/2511.18268", "authors": ["S M Mehedi Zaman", "Saubhagya Joshi", "Yiyi Wu"], "title": "Privacy Concerns and ChatGPT: Exploring Online Discourse through the Lens of Information Practice on Reddit", "comment": "Accepted as a poster at the iConference, 2026", "summary": "As millions of people use ChatGPT for tasks such as education, writing assistance, and health advice, concerns have grown about how personal prompts and data are stored and used. This study explores how Reddit users collectively negotiate and respond to these privacy concerns. Posts were collected from three major subreddits -- r/Chatgpt, r/privacy, and r/OpenAI -- between November 2022 and May 2025. An iterative keyword search followed by manual screening resulted in a final dataset of 426 posts and 1,900 comments. Using information practice as the theoretical lens, we conducted a qualitative thematic analysis to identify collective practices of risk negotiation, validated with BERTopic topic modeling to ensure thematic saturation. Findings revealed risk signaling, norm-setting, and resignation as dominant discourses, and collective troubleshooting and advocacy for privacy-preserving alternatives as key adaptive practices. Reddit functions as a site of collective sense-making where users surface risks, establish informal norms, and share strategies for mitigating privacy threats, offering insights for AI design and privacy literacy initiatives."}
{"id": "2511.18226", "categories": ["cs.CR", "quant-ph"], "pdf": "https://arxiv.org/pdf/2511.18226", "abs": "https://arxiv.org/abs/2511.18226", "authors": ["Buji Xu", "Xiaoming Sun"], "title": "Utilizing Circulant Structure to Optimize the Implementations of Linear Layers", "comment": null, "summary": "In this paper, we propose a novel approach for optimizing the linear layer used in symmetric cryptography. It is observed that these matrices often have circulant structure. The basic idea of this work is to utilize the property to construct a sequence of transformation matrices, which allows subsequent heuristic algorithms to find more efficient implementations. Our results outperform previous works for various linear layers of block ciphers. For Whirlwind M0 , we obtain two implementations with 159 XOR counts (8% better than Yuan et al. at FSE 2025) and depth 17 (39% better than Shi et al. at AsiaCrypt 2024) respectively. For AES MixColumn, our automated method produces a quantum circuit with depth 10, which nearly matches the manually optimized state-of-the-art result by Zhang et al. at IEEE TC 2024, only with 2 extra CNOTs."}
{"id": "2511.17595", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17595", "abs": "https://arxiv.org/abs/2511.17595", "authors": ["Markus D. Solbach", "John K. Tsotsos"], "title": "Boosting Reinforcement Learning in 3D Visuospatial Tasks Through Human-Informed Curriculum Design", "comment": "12 pages, 11 figures, 5 tables", "summary": "Reinforcement Learning is a mature technology, often suggested as a potential route towards Artificial General Intelligence, with the ambitious goal of replicating the wide range of abilities found in natural and artificial intelligence, including the complexities of human cognition. While RL had shown successes in relatively constrained environments, such as the classic Atari games and specific continuous control problems, recent years have seen efforts to expand its applicability. This work investigates the potential of RL in demonstrating intelligent behaviour and its progress in addressing more complex and less structured problem domains.\n  We present an investigation into the capacity of modern RL frameworks in addressing a seemingly straightforward 3D Same-Different visuospatial task. While initial applications of state-of-the-art methods, including PPO, behavioural cloning and imitation learning, revealed challenges in directly learning optimal strategies, the successful implementation of curriculum learning offers a promising avenue. Effective learning was achieved by strategically designing the lesson plan based on the findings of a real-world human experiment."}
{"id": "2511.18171", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18171", "abs": "https://arxiv.org/abs/2511.18171", "authors": ["Jasper Nie", "Christian Muise", "Victoria Armstrong"], "title": "BPMN to PDDL: Translating Business Workflows for AI Planning", "comment": "8 pages, 3 figures. Code and generated PDDL outputs available at https://github.com/QuMuLab/bpmn-to-pddl-translation", "summary": "Business Process Model and Notation (BPMN) is a widely used standard for modelling business processes. While automated planning has been proposed as a method for simulating and reasoning about BPMN workflows, most implementations remain incomplete or limited in scope. This project builds upon prior theoretical work to develop a functional pipeline that translates BPMN 2.0 diagrams into PDDL representations suitable for planning. The system supports core BPMN constructs, including tasks, events, sequence flows, and gateways, with initial support for parallel and inclusive gateway behaviour. Using a non-deterministic planner, we demonstrate how to generate and evaluate valid execution traces. Our implementation aims to bridge the gap between theory and practical tooling, providing a foundation for further exploration of translating business processes into well-defined plans."}
{"id": "2511.18842", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.18842", "abs": "https://arxiv.org/abs/2511.18842", "authors": ["Mohammad Nour Al Awad", "Sergey Ivanov", "Olga Tikhonova"], "title": "Optimizing LLM Code Suggestions: Feedback-Driven Timing with Lightweight State Bounds", "comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses", "summary": "Large Language Models (LLMs) have transformed code auto-completion by generating context-aware suggestions. Yet, deciding when to present these suggestions remains underexplored, often leading to interruptions or wasted inference calls. We propose an adaptive timing mechanism that dynamically adjusts the delay before offering a suggestion based on real-time developer feedback. Our suggested method combines a logistic transform of recent acceptance rates with a bounded delay range, anchored by a high-level binary prediction of the developer's cognitive state. In a two-month deployment with professional developers, our system improved suggestion acceptance from 4.9% with no delay to 15.4% with static delays, and to 18.6% with adaptive timing-while reducing blind rejections (rejections without being read) from 8.3% to 0.36%. Together, these improvements increase acceptance and substantially reduce wasted inference calls by 75%, making LLM-based code assistants more efficient and cost-effective in practice."}
{"id": "2511.18403", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2511.18403", "abs": "https://arxiv.org/abs/2511.18403", "authors": ["Aayush Kumar", "Sanket Mhatre"], "title": "UnWEIRDing LLM Entity Recommendations", "comment": null, "summary": "Large Language Models have been widely been adopted by users for writing tasks such as sentence completions. While this can improve writing efficiency, prior research shows that LLM-generated suggestions may exhibit cultural biases which may be difficult for users to detect, especially in educational contexts for non-native English speakers. While such prior work has studied the biases in LLM moral value alignment, we aim to investigate cultural biases in LLM recommendations for real-world entities. To do so, we use the WEIRD (Western, Educated, Industrialized, Rich and Democratic) framework to evaluate recommendations by various LLMs across a dataset of fine-grained entities, and apply pluralistic prompt-based strategies to mitigate these biases. Our results indicate that while such prompting strategies do reduce such biases, this reduction is not consistent across different models, and recommendations for some types of entities are more biased than others."}
{"id": "2511.18230", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.18230", "abs": "https://arxiv.org/abs/2511.18230", "authors": ["Saeid Jamshidi", "Amin Nikanjam", "Negar Shahabi", "Kawser Wazed Nafi", "Foutse Khomh", "Samira Keivanpour", "Rolando Herrero"], "title": "Think Fast: Real-Time IoT Intrusion Reasoning Using IDS and LLMs at the Edge Gateway", "comment": null, "summary": "As the number of connected IoT devices continues to grow, securing these systems against cyber threats remains a major challenge, especially in environments with limited computational and energy resources. This paper presents an edge-centric Intrusion Detection System (IDS) framework that integrates lightweight machine learning (ML) based IDS models with pre-trained large language models (LLMs) to improve detection accuracy, semantic interpretability, and operational efficiency at the network edge. The system evaluates six ML-based IDS models: Decision Tree (DT), K-Nearest Neighbors (KNN), Random Forest (RF), Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM), and a hybrid CNN-LSTM model on low-power edge gateways, achieving accuracy up to 98 percent under real-world cyberattacks. For anomaly detection, the system transmits a compact and secure telemetry snapshot (for example, CPU usage, memory usage, latency, and energy consumption) via low-bandwidth API calls to LLMs including GPT-4-turbo, DeepSeek V2, and LLaMA 3.5. These models use zero-shot, few-shot, and chain-of-thought reasoning to produce human-readable threat analyses and actionable mitigation recommendations. Evaluations across diverse attacks such as DoS, DDoS, brute force, and port scanning show that the system enhances interpretability while maintaining low latency (<1.5 s), minimal bandwidth usage (<1.2 kB per prompt), and energy efficiency (<75 J), demonstrating its practicality and scalability as an IDS solution for edge gateways."}
{"id": "2511.17598", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.17598", "abs": "https://arxiv.org/abs/2511.17598", "authors": ["Zhizuo Chen", "Theodore T. Allen"], "title": "Non-stationary and Varying-discounting Markov Decision Processes for Reinforcement Learning", "comment": null, "summary": "Algorithms developed under stationary Markov Decision Processes (MDPs) often face challenges in non-stationary environments, and infinite-horizon formulations may not directly apply to finite-horizon tasks. To address these limitations, we introduce the Non-stationary and Varying-discounting MDP (NVMDP) framework, which naturally accommodates non-stationarity and allows discount rates to vary with time and transitions. Infinite-horizon, stationary MDPs emerge as special cases of NVMDPs for identifying an optimal policy, and finite-horizon MDPs are also subsumed within the NVMDP formulations. Moreover, NVMDPs provide a flexible mechanism to shape optimal policies, without altering the state space, action space, or the reward structure. We establish the theoretical foundations of NVMDPs, including assumptions, state- and action-value formulation and recursion, matrix representation, optimality conditions, and policy improvement under finite state and action spaces. Building on these results, we adapt dynamic programming and generalized Q-learning algorithms to NVMDPs, along with formal convergence proofs. For problems requiring function approximation, we extend the Policy Gradient Theorem and the policy improvement bound in Trust Region Policy Optimization (TRPO), offering proofs in both scalar and matrix forms. Empirical evaluations in a non-stationary gridworld environment demonstrate that NVMDP-based algorithms successfully recover optimal trajectories under multiple reward and discounting schemes, whereas original Q-learning fails. These results collectively show that NVMDPs provide a theoretically sound and practically effective framework for reinforcement learning, requiring only minor algorithmic modifications while enabling robust handling of non-stationarity and explicit optimal policy shaping."}
{"id": "2511.18244", "categories": ["cs.AI", "cond-mat.mtrl-sci", "physics.ed-ph"], "pdf": "https://arxiv.org/pdf/2511.18244", "abs": "https://arxiv.org/abs/2511.18244", "authors": ["Zhiling Zheng"], "title": "Developing an AI Course for Synthetic Chemistry Students", "comment": "17 pages, 3 figures", "summary": "Artificial intelligence (AI) and data science are transforming chemical research, yet few formal courses are tailored to synthetic and experimental chemists, who often face steep entry barriers due to limited coding experience and lack of chemistry-specific examples. We present the design and implementation of AI4CHEM, an introductory data-driven chem-istry course created for students on the synthetic chemistry track with no prior programming background. The curricu-lum emphasizes chemical context over abstract algorithms, using an accessible web-based platform to ensure zero-install machine learning (ML) workflow development practice and in-class active learning. Assessment combines code-guided homework, literature-based mini-reviews, and collaborative projects in which students build AI-assisted workflows for real experimental problems. Learning gains include increased confidence with Python, molecular property prediction, reaction optimization, and data mining, and improved skills in evaluating AI tools in chemistry. All course materials are openly available, offering a discipline-specific, beginner-accessible framework for integrating AI into synthetic chemistry training."}
{"id": "2511.18849", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.18849", "abs": "https://arxiv.org/abs/2511.18849", "authors": ["Mohammad Nour Al Awad", "Sergey Ivanov", "Olga Tikhonova"], "title": "Pre-Filtering Code Suggestions using Developer Behavioral Telemetry to Optimize LLM-Assisted Programming", "comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses", "summary": "Large Language Models (LLMs) are increasingly integrated into code editors to provide AI-powered code suggestions. Yet many of these suggestions are ignored, resulting in wasted computation, increased latency, and unnecessary interruptions. We introduce a lightweight pre-filtering model that predicts the likelihood of suggestion acceptance before invoking the LLM, using only real-time developer telemetry such as typing speed, file navigation, and editing activity. Deployed in a production-grade Visual Studio Code plugin over four months of naturalistic use, our approach nearly doubled acceptance rates (18.4% -> 34.2%) while suppressing 35% of low-value LLM calls. These findings demonstrate that behavioral signals alone can meaningfully improve both user experience and system efficiency in LLM-assisted programming, highlighting the value of timing-aware, privacy-preserving adaptation mechanisms. The filter operates solely on pre-invocation editor telemetry and never inspects code or prompts."}
{"id": "2511.18483", "categories": ["cs.CY", "math.OC", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.18483", "abs": "https://arxiv.org/abs/2511.18483", "authors": ["Sergio Marin", "Nhu Nguyen", "Max", "Zheng", "Christina M. Weaver"], "title": "Optimal Meal Schedule for a Local Nonprofit Using LLM-Aided Data Extraction", "comment": "12 pages, 4 figures, presented at 2025 INFORMS Data Science Workshop (Atlanta, Georgia, Oct. 25, 2025)", "summary": "We present a data-driven pipeline developed in collaboration with the Power Packs Project, a nonprofit addressing food insecurity in local communities. The system integrates data extraction from PDFs, large language models for ingredient standardization, and binary integer programming to generate a 15-week recipe schedule that minimizes projected wholesale costs while meeting nutritional constraints. All 157 recipes were mapped to a nutritional database and assigned estimated and predicted costs using historical invoice data and category-specific inflation adjustments. The model effectively handles real-world price volatility and is structured for easy updates as new recipes or cost data become available. Optimization results show that constraint-based selection yields nutritionally balanced and cost-efficient plans under uncertainty. To facilitate real-time decision-making, we deployed a searchable web platform that integrates analytical models into daily operations by enabling staff to explore recipes by ingredient, category, or through an optimized meal plan."}
{"id": "2511.18235", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.18235", "abs": "https://arxiv.org/abs/2511.18235", "authors": ["Saeid Jamshidi", "Fatemeh Erfan", "Omar Abdul-Wahab", "Martine Bellaiche", "Foutse Khomh"], "title": "Lightweight Autoencoder-Isolation Forest Anomaly Detection for Green IoT Edge Gateways", "comment": null, "summary": "The rapid growth of the Internet of Things (IoT) has given rise to highly diverse and interconnected ecosystems that are increasingly susceptible to sophisticated cyber threats. Conventional anomaly detection schemes often prioritize accuracy while overlooking computational efficiency and environmental impact, which limits their deployment in resource-constrained edge environments. This paper presents \\textit{EcoDefender}, a sustainable hybrid anomaly detection framework that integrates \\textit{Autoencoder(AE)}-based representation learning with \\textit{Isolation Forest(IF)} anomaly scoring. Beyond empirical performance, EcoDefender is supported by a theoretical foundation that establishes formal guarantees for its stability, convergence, robustness, and energy-complexity coupling-thereby linking computational behavior to energy efficiency. Furthermore, experiments on realistic IoT traffic confirm these theoretical insights, achieving up to 94\\% detection accuracy with an average CPU usage of only 22\\%, 27 ms inference latency, and 30\\% lower energy consumption compared to AE-only baselines. By embedding sustainability metrics directly into the security evaluation process, this work demonstrates that reliable anomaly detection and environmental responsibility can coexist within next-generation green IoT infrastructures, aligning with the United Nations Sustainable Development Goals (SDG 9: resilient infrastructure, SDG 13: climate action)."}
{"id": "2511.17599", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17599", "abs": "https://arxiv.org/abs/2511.17599", "authors": ["Jianbing Dong", "Jianbin Chang"], "title": "From Projection to Prediction: Beyond Logits for Scalable Language Models", "comment": "17 pages, 2 figures, 4 algorithms", "summary": "Training Large Language Models (LLMs) typically involves a two-stage pipeline at the output layer: hidden states are projected into vocabulary logits via a linear transformation (lm_head), followed by cross-entropy loss computation against target tokens. While conceptually simple, this design incurs substantial overhead. The intermediate logits tensor, with dimensions proportional to batch size, sequence length, and vocabulary size, must be fully materialized in GPU memory, even though only one target token per position is ultimately used. This leads to significant memory footprint and bandwidth comsumption, limiting scalability and slowing training throughput.\n  In this work, we introduce a novel approach to integrates the output projection and loss prediction into a single operation. By directly computing the loss from hidden states and target tokens, our approach bypasses explicit logits materialization. This design reduces memory usage and alleviates bandwidth pressure. Experiments on LLM training demonstrate that our method achieves substantial memory savings and measurable speedups compared to the standard two-stage pipeline, enabling large batch sizes and longer sequences without sacrificing accuracy. Our work highlights the benefits of rethinking the boundary between projection and prediction, offering a practical systems optimization for efficient LLM training."}
{"id": "2511.18284", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18284", "abs": "https://arxiv.org/abs/2511.18284", "authors": ["Tetiana Bas", "Krystian Novak"], "title": "Steering Latent Traits, Not Learned Facts: An Empirical Study of Activation Control Limits", "comment": null, "summary": "Large language models (LLMs) require precise behavior control for safe and effective deployment across diverse applications.\n  Activation steering offers a promising approach for LLMs' behavioral control. We focus on the question of how steering effectiveness varies across different behavior types and whether the nature of target behaviors can predict steering success. We address this through empirical analysis of activation steering across 50 behaviors that span persona archetypes, personality traits, misalignment behaviors, style cues, and impersonation of public figures. We present a set of comprehensive experiments on coefficient optimization, vector properties, and data requirements to provide comprehensive guidance for the implementation of activation steering. Our analysis demonstrates that steering effectiveness varies significantly by behavior type, with different behavioral categories exhibiting distinct response patterns to intervention strength. We find that trait expression follows an inverted-U curve with a steering coefficient strength. We also show that vector separation metrics do not predict steering success, but larger training datasets enable more aggressive steering. These findings provide empirically grounded guidance for implementing activation steering and demonstrate that steering effectiveness is heavily influenced by behavior type."}
{"id": "2511.18854", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18854", "abs": "https://arxiv.org/abs/2511.18854", "authors": ["Yujing Wang", "Weize Hong"], "title": "Time Travel: LLM-Assisted Semantic Behavior Localization with Git Bisect", "comment": "submitted to Git Bisect SCALCOM 2025 Calgary (to be published)", "summary": "We present a novel framework that integrates Large Language Models (LLMs) into the Git bisect process for semantic fault localization. Traditional bisect assumes deterministic predicates and binary failure states assumptions often violated in modern software development due to flaky tests, nonmonotonic regressions, and semantic divergence from upstream repositories. Our system augments bisect traversal with structured chain of thought reasoning, enabling commit by commit analysis under noisy conditions. We evaluate multiple open source and proprietary LLMs for their suitability and fine tune DeepSeekCoderV2 using QLoRA on a curated dataset of semantically labeled diffs. We adopt a weak supervision workflow to reduce annotation overhead, incorporating human in the loop corrections and self consistency filtering. Experiments across multiple open source projects show a 6.4 point absolute gain in success rate from 74.2 to 80.6 percent, leading to significantly fewer failed traversals and by experiment up to 2x reduction in average bisect time. We conclude with discussions on temporal reasoning, prompt design, and finetuning strategies tailored for commit level behavior analysis."}
{"id": "2511.18558", "categories": ["cs.CY", "cs.DB"], "pdf": "https://arxiv.org/pdf/2511.18558", "abs": "https://arxiv.org/abs/2511.18558", "authors": ["Genoveva Vargas-Solar"], "title": "Bridging the Divide: Gender, Diversity, and Inclusion Gaps in Data Science and Artificial Intelligence Across Academia and Industry in the majority and minority worlds", "comment": null, "summary": "As Artificial Intelligence (AI) and Data Science (DS) become pervasive, addressing gender disparities and diversity gaps in their workforce is urgent. These rapidly evolving fields have been further impacted by the COVID-19 pandemic, which disproportionately affected women and minorities, exposing deep-seated inequalities. Both academia and industry shape these disciplines, making it essential to map disparities across sectors, occupations, and skill levels. The dominance of men in AI and DS reinforces gender biases in machine learning systems, creating a feedback loop of inequality. This imbalance is a matter of social and economic justice and an ethical challenge, demanding value-driven diversity. Root causes include unequal access to education, disparities in academic programs, limited government investments, and underrepresented communities' perceptions of elite opportunities. This chapter examines the participation of women and minorities in AI and DS, focusing on their representation in both industry and academia. Analyzing the existing dynamics seeks to uncover the collective and individual impacts on the lives of women and minority groups within these fields. Additionally, the chapter aims to propose actionable strategies to promote equity, diversity, and inclusion (DEI), fostering a more representative and supportive environment for all."}
{"id": "2511.18240", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.18240", "abs": "https://arxiv.org/abs/2511.18240", "authors": ["Saeid Jamshidi", "Foutse Khomh", "Kawser Wazed Nafi", "Amin Nikanjam", "Samira Keivanpour", "Omar Abdul-Wahab", "Martine Bellaiche"], "title": "Carbon-Aware Intrusion Detection: A Comparative Study of Supervised and Unsupervised DRL for Sustainable IoT Edge Gateways", "comment": null, "summary": "The rapid expansion of the Internet of Things (IoT) has intensified cybersecurity challenges, particularly in mitigating Distributed Denial-of-Service (DDoS) attacks at the network edge. Traditional Intrusion Detection Systems (IDSs) face significant limitations, including poor adaptability to evolving and zero-day attacks, reliance on static signatures and labeled datasets, and inefficiency on resource-constrained edge gateways. Moreover, most existing DRL-based IDS studies overlook sustainability factors such as energy efficiency and carbon impact. To address these challenges, this paper proposes two novel Deep Reinforcement Learning (DRL)-based IDS: DeepEdgeIDS, an unsupervised Autoencoder-DRL hybrid, and AutoDRL-IDS, a supervised LSTM-DRL model. Both DRL-based IDS are validated through theoretical analysis and experimental evaluation on edge gateways. Results demonstrate that AutoDRL-IDS achieves 94% detection accuracy using labeled data, while DeepEdgeIDS attains 98% accuracy and adaptability without labels. Distinctly, this study introduces a carbon-aware, multi-objective reward function optimized for sustainable and real-time IDS operations in dynamic IoT networks."}
{"id": "2511.17601", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.17601", "abs": "https://arxiv.org/abs/2511.17601", "authors": ["Luyang Fang", "Tao Wang", "Ping Ma", "Xiaoming Zhai"], "title": "Generalizable and Efficient Automated Scoring with a Knowledge-Distilled Multi-Task Mixture-of-Experts", "comment": null, "summary": "Automated scoring of written constructed responses typically relies on separate models per task, straining computational resources, storage, and maintenance in real-world education settings. We propose UniMoE-Guided, a knowledge-distilled multi-task Mixture-of-Experts (MoE) approach that transfers expertise from multiple task-specific large models (teachers) into a single compact, deployable model (student). The student combines (i) a shared encoder for cross-task representations, (ii) a gated MoE block that balances shared and task-specific processing, and (iii) lightweight task heads. Trained with both ground-truth labels and teacher guidance, the student matches strong task-specific models while being far more efficient to train, store, and deploy. Beyond efficiency, the MoE layer improves transfer and generalization: experts develop reusable skills that boost cross-task performance and enable rapid adaptation to new tasks with minimal additions and tuning. On nine NGSS-aligned science-reasoning tasks (seven for training/evaluation and two held out for adaptation), UniMoE-Guided attains performance comparable to per-task models while using $\\sim$6$\\times$ less storage than maintaining separate students, and $87\\times$ less than the 20B-parameter teacher. The method offers a practical path toward scalable, reliable, and resource-efficient automated scoring for classroom and large-scale assessment systems."}
{"id": "2511.18296", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18296", "abs": "https://arxiv.org/abs/2511.18296", "authors": ["Iman Rahimi"], "title": "Deep Learning Decision Support System for Open-Pit Mining Optimisation: GPU-Accelerated Planning Under Geological Uncertainty", "comment": "67 pages", "summary": "This study presents Part II of an AI-enhanced Decision Support System (DSS), extending Rahimi (2025, Part I) by introducing a fully uncertainty-aware optimization framework for long-term open-pit mine planning. Geological uncertainty is modelled using a Variational Autoencoder (VAE) trained on 50,000 spatial grade samples, enabling the generation of probabilistic, multi-scenario orebody realizations that preserve geological continuity and spatial correlation. These scenarios are optimized through a hybrid metaheuristic engine integrating Genetic Algorithms (GA), Large Neighborhood Search (LNS), Simulated Annealing (SA), and reinforcement-learning-based adaptive control. An ε-constraint relaxation strategy governs the population exploration phase, allowing near-feasible schedule discovery early in the search and gradual tightening toward strict constraint satisfaction. GPU-parallel evaluation enables the simultaneous assessment of 65,536 geological scenarios, achieving near-real-time feasibility analysis. Results demonstrate up to 1.2 million-fold runtime improvement over IBM CPLEX and significantly higher expected NPV under geological uncertainty, confirming the DSS as a scalable and uncertainty-resilient platform for intelligent mine planning."}
{"id": "2511.18867", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.18867", "abs": "https://arxiv.org/abs/2511.18867", "authors": ["Liutong Han", "Chu Kang", "Mingjie Xing", "Yanjun Wu"], "title": "VecIntrinBench: Benchmarking Cross-Architecture Intrinsic Code Migration for RISC-V Vector", "comment": "5 pages, 7 figures", "summary": "Intrinsic functions are specialized functions provided by the compiler that efficiently operate on architecture-specific hardware, allowing programmers to write optimized code in a high-level language that fully exploits hardware features. Using intrinsics to vectorize core code blocks is a standard optimization method in high-performance libraries, often requiring specific vector optimization implementations for multiple mainstream architectures. The promising RISC-V software ecosystem has a significant demand for algorithm library migration and adaptation. Translating existing intrinsic functions to RISC-V Vector (RVV) intrinsic functions across architectures is currently a mainstream approach. Rule-based intrinsic mapping methods and LLM-based code generation can help developers address the code migration challenge. However, existing intrinsic code benchmarks focus on mainstream SIMD intrinsics and lack support for the emerging RISC-V architecture. There is currently no benchmark that comprehensively evaluates the intrinsic migration capabilities for the RVV extension. To fill this gap, we propose VecIntrinBench, the first intrinsic benchmark encompassing RVV extensions. It includes 50 function-level tasks from open source repositories, implemented as scalars, RVV intrinsics, Arm Neon intrinsics, and x86 intrinsics, along with comprehensive functional and performance test cases. We systematically evaluated various code migration approaches on VecIntrinBench, yielding a series of insightful findings. The results demonstrate that advanced Large Language Models (LLMs) achieve a similar effect as rule-based mapping approaches for RISC-V code migration, while also delivering superior performance. We further analyze the reasons and identify future directions for LLM development in the code migration field. The VecIntrinBench is open-sourced to benefit the broader community and developers."}
{"id": "2511.18979", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2511.18979", "abs": "https://arxiv.org/abs/2511.18979", "authors": ["H. R. Paz"], "title": "Regularity as Structural Amplifier, Not Trap: A Causal and Archetype-Based Analysis of Dropout in a Constrained Engineering Curriculum", "comment": "28 pages, figures 9, tables 3", "summary": "Engineering programmes, particularly in Latin America, are often governed by rigid curricula and strict regularity rules that are claimed to create a Regularity Trap for capable students. This study tests that causal hypothesis using the CAPIRE framework, a leakage-aware pipeline that integrates curriculum topology and causal estimation. Using longitudinal data from 1,343 civil engineering students in Argentina, we formalize academic lag (accumulated friction) as a treatment and academic velocity as an ability proxy. A manual LinearDML estimator is employed to assess the average (ATE) and conditional (CATE) causal effects of lag on subsequent dropout, controlling for macro shocks (strikes, inflation). Results confirm that academic lag significantly increases dropout risk overall (ATE = 0.0167, p < 0.0001). However, the effect decreases sharply for high-velocity (high-ability) students, contradicting the universal Trap hypothesis. Archetype analysis (UMAP/DBSCAN) shows that friction disproportionately harms trajectories already characterized by high initial friction and unstable progression. 8 We conclude that regularity rules function as a Structural Amplifier of pre-existing vulnerability rather than a universal trap. This has direct implications for engineering curriculum design, demanding targeted slack allocation and intervention policies to reduce friction at core basic-cycle courses"}
{"id": "2511.18379", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.18379", "abs": "https://arxiv.org/abs/2511.18379", "authors": ["Andreea Elena Drăgnoiu", "Andrei Ciobanu", "Ruxandra F. Olimid"], "title": "On Addressing Isolation in Blockchain-Based Self-Sovereign Identity", "comment": null, "summary": "Self-Sovereign Identity (SSI) grants holders full ownership and control of their digital identities, being the ultimate digital identity model. Operating in a decentralized manner, SSI enables the verification of claims, including privacy-preserving mechanisms. Blockchain, which can be used to implement a Verifiable Data Registry (VDR), is often considered one of the pillars of SSI, along with Decentralized Identifiers (DIDs) and Verifiable Credentials (VCs). Unfortunately, blockchains are mostly siloed, affecting the interoperability and universality of SSI. We investigate the effect of blockchain isolation on blockchain-based SSI. We first define possible scenarios for cross-chain SSI and exemplify with real-life use cases. We then define specific requirements for cross-chain SSI and identify challenges, also in relation to the identified scenarios. We explore various solutions to achieve blockchain interoperability, with a focus on SSI. In particular, we identify the advantages and disadvantages of distinct cross-chain models for cross-chain SSI. Finally, we address the usability of cross-chain SSI and discuss security and privacy aspects, opening the way for future research."}
{"id": "2511.17602", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17602", "abs": "https://arxiv.org/abs/2511.17602", "authors": ["Sushant Mehta"], "title": "Beyond Surface-Level Similarity: Hierarchical Contamination Detection for Synthetic Training Data in Foundation Models", "comment": null, "summary": "Synthetic data has become essential for training foundation models, yet benchmark contamination threatens evaluation integrity. Although existing detection methods identify token-level overlap, they fail to detect semantic-level contamination where synthetic data conceptually resemble benchmarks without lexical overlap. This gap is critical as foundation models increasingly train on synthetic data that may implicitly encode benchmark knowledge. We propose a hierarchical contamination detection framework operating at four levels: token level, semantic level, reasoning pattern, and performance cliff detection. Through controlled experiments on MMLU, GSM8K and HumanEval, we demonstrate that semantic-level contamination evades existing methods (F1=0.17-0.49) but is effectively detected by our hierarchical approach (F1 = 0.76), with an average improvement of 26. 5\\% over state-of-the-art baselines. Our framework provides practitioners with practical tools for audit pipelines and enables responsible deployment of synthetic training data."}
{"id": "2511.18298", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18298", "abs": "https://arxiv.org/abs/2511.18298", "authors": ["Svitlana Volkova", "Peter Bautista", "Avinash Hiriyanna", "Gabriel Ganberg", "Isabel Erickson", "Zachary Klinefelter", "Nick Abele", "Hsien-Te Kao", "Grant Engberson"], "title": "Cross-Disciplinary Knowledge Retrieval and Synthesis: A Compound AI Architecture for Scientific Discovery", "comment": null, "summary": "The exponential growth of scientific knowledge has created significant barriers to cross-disciplinary knowledge discovery, synthesis and research collaboration. In response to this challenge, we present BioSage, a novel compound AI architecture that integrates LLMs with RAG, orchestrated specialized agents and tools to enable discoveries across AI, data science, biomedical, and biosecurity domains. Our system features several specialized agents including the retrieval agent with query planning and response synthesis that enable knowledge retrieval across domains with citation-backed responses, cross-disciplinary translation agents that align specialized terminology and methodologies, and reasoning agents that synthesize domain-specific insights with transparency, traceability and usability. We demonstrate the effectiveness of our BioSage system through a rigorous evaluation on scientific benchmarks (LitQA2, GPQA, WMDP, HLE-Bio) and introduce a new cross-modal benchmark for biology and AI, showing that our BioSage agents outperform vanilla and RAG approaches by 13\\%-21\\% powered by Llama 3.1. 70B and GPT-4o models. We perform causal investigations into compound AI system behavior and report significant performance improvements by adding RAG and agents over the vanilla models. Unlike other systems, our solution is driven by user-centric design principles and orchestrates specialized user-agent interaction workflows supporting scientific activities including but not limited to summarization, research debate and brainstorming. Our ongoing work focuses on multimodal retrieval and reasoning over charts, tables, and structured scientific data, along with developing comprehensive multimodal benchmarks for cross-disciplinary discovery. Our compound AI solution demonstrates significant potential for accelerating scientific advancement by reducing barriers between traditionally siloed domains."}
{"id": "2511.18918", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.18918", "abs": "https://arxiv.org/abs/2511.18918", "authors": ["Qingchao Shen", "Zan Wang", "Haoyang Ma", "Yongqiang Tian", "Lili Huang", "Zibo Xiao", "Junjie Chen", "Shing-Chi Cheung"], "title": "Optimization-Aware Test Generation for Deep Learning Compilers", "comment": "This paper has been accpected by ICSE 2026", "summary": "Deep Learning (DL) compilers have been widely utilized to optimize DL models for efficient deployment across various hardware. Due to their vital role in the DL ecosystem, ensuring their reliability and security is critical. However, existing approaches have limitations in testing optimization stages, which is the core functionality of DL compilers, due to the difficulty in generating optimization-aware tests. In this paper, we proposed OATest, a novel approach for synthesizing optimization-aware computational graphs. The approach combines patterns extracted from documented tests for optimization and incorporates them into seed computational graphs, enabling broader exploration of optimization paths. To guarantee the optimization-awareness of generated graphs, OATest introduces the edges reusing strategy to establish strong connections between patterns and contexts. Additionally, to solve the validity challenge for the generated graphs, OATest employs an auxiliary layers addition strategy to resolve broken constraints. Equipped with two distinct test oracles, OATest applies differential testing to evaluate the two widely used DL compilers (i.e., TVM and ONNXRuntime). Our experimental results show that OATest outperforms the state-of-the-art method by detecting more bugs and achieving higher code coverage in TVM and ONNXRutimes. Additionally, OATest uncovers 58 previously unknown bugs, 36 of which have been confirmed or fixed by developers."}
{"id": "2511.19283", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19283", "abs": "https://arxiv.org/abs/2511.19283", "authors": ["Ndaka. A", "Avila-Acosta. F", "Mbula-Ndaka. H", "Amera. C", "Chauke. S", "Majiwa. E"], "title": "Data Flows and Colonial Regimes in Africa: A Critical Analysis of the Colonial Futurities Embedded in AI Ecosystems", "comment": "12 pages", "summary": "This chapter seeks to frame the elemental and invisible problems of AI and big data in the African context by examining digital sites and infrastructure through the lens of power and interests. It will present reflections on how these sites are using AI recommendation algorithms to recreate new digital societies in the region, how they have the potential to propagate algorithmic colonialism and negative gender norms, and what this means for the regional sustainable development agenda. The chapter proposes adopting business models that embrace response-ability and consider the existence of alternative socio-material worlds of AI. These reflections will mainly come from ongoing discussions with Kenyan social media users in this authors' user space talks, personal experiences and six months of active participant observations done by the authors."}
{"id": "2511.18412", "categories": ["cs.CR", "cs.AR"], "pdf": "https://arxiv.org/pdf/2511.18412", "abs": "https://arxiv.org/abs/2511.18412", "authors": ["Dilli Babu Porlapothula", "Pralay Chakrabarty", "Ananya Lakshmi Ravi", "Kurian Polachan"], "title": "ioPUF+: A PUF Based on I/O Pull-Up/Down Resistors for Secret Key Generation in IoT Nodes", "comment": null, "summary": "In this work, we present ioPUF+, which incorporates a novel Physical Unclonable Function (PUF) that generates unique fingerprints for Integrated Circuits (ICs) and the IoT nodes encompassing them. The proposed PUF generates device-specific responses by measuring the pull-up and pull-down resistor values on the I/O pins of the ICs, which naturally vary across chips due to manufacturing-induced process variations. Since these resistors are already integrated into the I/O structures of most ICs, ioPUF+ requires no custom circuitry, and no new IC fabrication. This makes ioPUF+ suitable for cost-sensitive embedded systems built from Commercial Off-The-Shelf (COTS) components. Beyond introducing a new PUF, ioPUF+ includes a complete datapath for converting raw PUF responses into cryptographically usable secret keys using BCH error correction and SHA-256 hashing. Further ioPUF+ also demonstrate a practical use case of PUF derive secret keys in securing device-to-device communication using AES-encryption. We implemented ioPUF+ on the Infineon PSoC-5 microcontroller and evaluated its performance across 30 devices using standard PUF metrics. The results show excellent reliability (intra-device Hamming distance of 100.00%), strong uniqueness (inter-device Hamming distance of 50.33%), near-ideal uniformity (50.54%), and negligible bit aliasing. Stability tests under temperature and supply-voltage variations show worst-case bit-error rates of only 2.63% and 2.10%, respectively. We also profiled the resource and energy usage of the complete ioPUF+ system, including the PUF primitive, BCH decoding, SHA-256 hashing, and AES encryption. The full implementation requires only 19.8 KB of Flash, exhibits a latency of 600 ms, and consumes 79 mW of power, demonstrating the suitabilitiy of ioPUF+ for resource-constrained IoT nodes."}
{"id": "2511.17604", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17604", "abs": "https://arxiv.org/abs/2511.17604", "authors": ["Jiajun Ma", "Yongchao Zhang", "Chao Zhang", "Zhao Lv", "Shengbing Pei"], "title": "BrainHGT: A Hierarchical Graph Transformer for Interpretable Brain Network Analysis", "comment": null, "summary": "Graph Transformer shows remarkable potential in brain network analysis due to its ability to model graph structures and complex node relationships. Most existing methods typically model the brain as a flat network, ignoring its modular structure, and their attention mechanisms treat all brain region connections equally, ignoring distance-related node connection patterns. However, brain information processing is a hierarchical process that involves local and long-range interactions between brain regions, interactions between regions and sub-functional modules, and interactions among functional modules themselves. This hierarchical interaction mechanism enables the brain to efficiently integrate local computations and global information flow, supporting the execution of complex cognitive functions. To address this issue, we propose BrainHGT, a hierarchical Graph Transformer that simulates the brain's natural information processing from local regions to global communities. Specifically, we design a novel long-short range attention encoder that utilizes parallel pathways to handle dense local interactions and sparse long-range connections, thereby effectively alleviating the over-globalizing issue. To further capture the brain's modular architecture, we designe a prior-guided clustering module that utilizes a cross-attention mechanism to group brain regions into functional communities and leverage neuroanatomical prior to guide the clustering process, thereby improving the biological plausibility and interpretability. Experimental results indicate that our proposed method significantly improves performance of disease identification, and can reliably capture the sub-functional modules of the brain, demonstrating its interpretability."}
{"id": "2511.18302", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18302", "abs": "https://arxiv.org/abs/2511.18302", "authors": ["Mohan Reddy"], "title": "The Catastrophic Paradox of Human Cognitive Frameworks in Large Language Model Evaluation: A Comprehensive Empirical Analysis of the CHC-LLM Incompatibility", "comment": null, "summary": "This investigation presents an empirical analysis of the incompatibility between human psychometric frameworks and Large Language Model evaluation. Through systematic assessment of nine frontier models including GPT-5, Claude Opus 4.1, and Gemini 3 Pro Preview using the Cattell-Horn-Carroll theory of intelligence, we identify a paradox that challenges the foundations of cross-substrate cognitive evaluation. Our results show that models achieving above-average human IQ scores ranging from 85.0 to 121.4 simultaneously exhibit binary accuracy rates approaching zero on crystallized knowledge tasks, with an overall judge-binary correlation of r = 0.175 (p = 0.001, n = 1800). This disconnect appears most strongly in the crystallized intelligence domain, where every evaluated model achieved perfect binary accuracy while judge scores ranged from 25 to 62 percent, which cannot occur under valid measurement conditions. Using statistical analyses including Item Response Theory modeling, cross-vendor judge validation, and paradox severity indexing, we argue that this disconnect reflects a category error in applying biological cognitive architectures to transformer-based systems. The implications extend beyond methodology to challenge assumptions about intelligence, measurement, and anthropomorphic biases in AI evaluation. We propose a framework for developing native machine cognition assessments that recognize the non-human nature of artificial intelligence."}
{"id": "2511.18924", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18924", "abs": "https://arxiv.org/abs/2511.18924", "authors": ["Arina Kharlamova", "Jiawen Liu", "Tianyi Zhang", "Xinrui Yang", "Humaid Alqasimi", "Youcheng Sun", "Chun Jason Xue"], "title": "LLM-Driven Kernel Evolution: Automating Driver Updates in Linux", "comment": null, "summary": "Linux kernel evolution breaks drivers through API/ABI changes, semantic shifts, and security-hardening updates. We introduce DRIVEBENCH, an executable corpus of kernel$\\rightarrow$driver co-evolution cases, and AUTODRIVER, a closed-loop, LLM-driven system for automating driver maintenance. The system integrates prompt engineering, multi-agent collaboration, static analysis, and iterative validation to ensure that generated patches are not only syntactically correct but also functionally and semantically consistent with kernel conventions. The corpus spans v5.10-v6.10 with 235 validated cases drawn from 612 candidates. In evaluation across 55 cases, AUTODRIVER achieves 56.4% compilation success; QEMU-based boot verification indicates that compiled patches preserve driver initialization in most instances. By releasing DRIVEBENCH and tooling, we enable reproducible research and a practical route to continuous, safe co-evolution of drivers with the Linux kernel."}
{"id": "2511.19334", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2511.19334", "abs": "https://arxiv.org/abs/2511.19334", "authors": ["Axel Constant", "Mahault Albarracin", "Karl J. Friston"], "title": "Normative active inference: A numerical proof of principle for a computational and economic legal analytic approach to AI governance", "comment": "19 pages, 6 figures, 1 box", "summary": "This paper presents a computational account of how legal norms can influence the behavior of artificial intelligence (AI) agents, grounded in the active inference framework (AIF) that is informed by principles of economic legal analysis (ELA). The ensuing model aims to capture the complexity of human decision-making under legal constraints, offering a candidate mechanism for agent governance in AI systems, that is, the (auto)regulation of AI agents themselves rather than human actors in the AI industry. We propose that lawful and norm-sensitive AI behavior can be achieved through regulation by design, where agents are endowed with intentional control systems, or behavioral safety valves, that guide real-time decisions in accordance with normative expectations. To illustrate this, we simulate an autonomous driving scenario in which an AI agent must decide when to yield the right of way by balancing competing legal and pragmatic imperatives. The model formalizes how AIF can implement context-dependent preferences to resolve such conflicts, linking this mechanism to the conception of law as a scaffold for rational decision-making under uncertainty. We conclude by discussing how context-dependent preferences could function as safety mechanisms for autonomous agents, enhancing lawful alignment and risk mitigation in AI governance."}
{"id": "2511.18438", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.18438", "abs": "https://arxiv.org/abs/2511.18438", "authors": ["Xiangrui Zhang", "Zeyu Chen", "Haining Wang", "Qiang Li"], "title": "LLMs as Firmware Experts: A Runtime-Grown Tree-of-Agents Framework", "comment": "18 pages, 13 figures", "summary": "Large Language Models (LLMs) and their agent systems have recently demonstrated strong potential in automating code reasoning and vulnerability detection. However, when applied to large-scale firmware, their performance degrades due to the binary nature of firmware, complex dependency structures, and heterogeneous components. To address this challenge, this paper presents FIRMHIVE, a recursive agent hive that enables LLMs to act as autonomous firmware security analysts. FIRMHIVE introduces two key mechanisms: (1) transforming delegation into a per-agent, executable primitive and (2) constructing a runtime Tree of Agents (ToA) for decentralized coordination. We evaluate FIRMHIVE using real-world firmware images obtained from publicly available datasets, covering five representative security analysis tasks. Compared with existing LLM-agent baselines, FIRMHIVE performs deeper (about 16x more reasoning steps) and broader (about 2.3x more files inspected) cross-file exploration, resulting in about 5.6x more alerts per firmware. Compared to state-of-the-art (SOTA) security tools, FIRMHIVE identifies about 1.5x more vulnerabilities (1,802 total) and achieves 71% precision, representing significant improvements in both yield and fidelity."}
{"id": "2511.17605", "categories": ["cs.LG", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.17605", "abs": "https://arxiv.org/abs/2511.17605", "authors": ["Agnideep Aich", "Sameera Hewage", "Md Monzur Murshed"], "title": "Copula Based Fusion of Clinical and Genomic Machine Learning Risk Scores for Breast Cancer Risk Stratification", "comment": null, "summary": "Clinical and genomic models are both used to predict breast cancer outcomes, but they are often combined using simple linear rules that do not account for how their risk scores relate, especially at the extremes. Using the METABRIC breast cancer cohort, we studied whether directly modeling the joint relationship between clinical and genomic machine learning risk scores could improve risk stratification for 5-year cancer-specific mortality. We created a binary 5-year cancer-death outcome and defined two sets of predictors: a clinical set (demographic, tumor, and treatment variables) and a genomic set (gene-expression $z$-scores). We trained several supervised classifiers, such as Random Forest and XGBoost, and used 5-fold cross-validated predicted probabilities as unbiased risk scores. These scores were converted to pseudo-observations on $(0,1)^2$ to fit Gaussian, Clayton, and Gumbel copulas. Clinical models showed good discrimination (AUC 0.783), while genomic models had moderate performance (AUC 0.681). The joint distribution was best captured by a Gaussian copula (bootstrap $p=0.997$), which suggests a symmetric, moderately strong positive relationship. When we grouped patients based on this relationship, Kaplan-Meier curves showed clear differences: patients who were high-risk in both clinical and genomic scores had much poorer survival than those high-risk in only one set. These results show that copula-based fusion works in real-world cohorts and that considering dependencies between scores can better identify patient subgroups with the worst prognosis."}
{"id": "2511.18319", "categories": ["cs.AI", "cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.18319", "abs": "https://arxiv.org/abs/2511.18319", "authors": ["Xian Yeow Lee", "Lasitha Vidyaratne", "Gregory Sin", "Ahmed Farahat", "Chetan Gupta"], "title": "Weakly-supervised Latent Models for Task-specific Visual-Language Control", "comment": null, "summary": "Autonomous inspection in hazardous environments requires AI agents that can interpret high-level goals and execute precise control. A key capability for such agents is spatial grounding, for example when a drone must center a detected object in its camera view to enable reliable inspection. While large language models provide a natural interface for specifying goals, using them directly for visual control achieves only 58\\% success in this task. We envision that equipping agents with a world model as a tool would allow them to roll out candidate actions and perform better in spatially grounded settings, but conventional world models are data and compute intensive. To address this, we propose a task-specific latent dynamics model that learns state-specific action-induced shifts in a shared latent space using only goal-state supervision. The model leverages global action embeddings and complementary training losses to stabilize learning. In experiments, our approach achieves 71\\% success and generalizes to unseen images and instructions, highlighting the potential of compact, domain-specific latent dynamics models for spatial alignment in autonomous inspection."}
{"id": "2511.19059", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.19059", "abs": "https://arxiv.org/abs/2511.19059", "authors": ["Pei Liu", "Terry Zhuo", "Jiawei Deng", "Thong James", "Shidong Pan", "Sherry Xu", "Zhenchang Xing", "Qinghua Lu", "Xiaoning Du", "Hongyu Zhang"], "title": "LLMAID: Identifying AI Capabilities in Android Apps with LLMs", "comment": null, "summary": "Recent advancements in artificial intelligence (AI) and its widespread integration into mobile software applications have received significant attention, highlighting the growing prominence of AI capabilities in modern software systems. However, the inherent hallucination and reliability issues of AI continue to raise persistent concerns. Consequently, application users and regulators increasingly ask critical questions such as: Does the application incorporate AI capabilities? and What specific types of AI functionalities are embedded? Preliminary efforts have been made to identify AI capabilities in mobile software; however, existing approaches mainly rely on manual inspection and rule-based heuristics. These methods are not only costly and time-consuming but also struggle to adapt advanced AI techniques.\n  To address the limitations of existing methods, we propose LLMAID (Large Language Model for AI Discovery). LLMAID includes four main tasks: (1) candidate extraction, (2) knowledge base interaction, (3) AI capability analysis and detection, and (4) AI service summarization. We apply LLMAID to a dataset of 4,201 Android applications and demonstrate that it identifies 242% more real-world AI apps than state-of-the-art rule-based approaches. Our experiments show that LLM4AID achieves high precision and recall, both exceeding 90%, in detecting AI-related components. Additionally, a user study indicates that developers find the AI service summaries generated by LLMAID to be more informative and preferable to the original app descriptions. Finally, we leverage LLMAID to perform an empirical analysis of AI capabilities across Android apps. The results reveal a strong concentration of AI functionality in computer vision (54.80%), with object detection emerging as the most common task (25.19%)."}
{"id": "2511.17574", "categories": ["cs.SI", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.17574", "abs": "https://arxiv.org/abs/2511.17574", "authors": ["Eamon Earl", "Chen Ding", "Richard Valenzano", "Drai Paulen-Patterson"], "title": "Constructing Political Coordinates: Aggregating Over the Opposition for Diverse News Recommendation", "comment": "Due to appear in the proceedings of the 2025 IEEE International Conference on Big Data", "summary": "In the past two decades, open access to news and information has increased rapidly, empowering educated political growth within democratic societies. News recommender systems (NRSs) have shown to be useful in this process, minimizing political disengagement and information overload by providing individuals with articles on topics that matter to them. Unfortunately, NRSs often conflate underlying user interest with the partisan bias of the articles in their reading history and with the most popular biases present in the coverage of their favored topics. Over extended interaction, this can result in the formation of filter bubbles and the polarization of user partisanship. In this paper, we propose a novel embedding space called Constructed Political Coordinates (CPC), which models the political partisanship of users over a given topic-space, relative to a larger sample population. We apply a simple collaborative filtering (CF) framework using CPC-based correlation to recommend articles sourced from oppositional users, who have different biases from the user in question. We compare against classical CF methods and find that CPC-based methods promote pointed bias diversity and better match the true political tolerance of users, while classical methods implicitly exploit biases to maximize interaction."}
{"id": "2511.18467", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18467", "abs": "https://arxiv.org/abs/2511.18467", "authors": ["Xiaoqing Wang", "Keman Huang", "Bin Liang", "Hongyu Li", "Xiaoyong Du"], "title": "Shadows in the Code: Exploring the Risks and Defenses of LLM-based Multi-Agent Software Development Systems", "comment": "Accepted by AAAI 2026 Alignment Track", "summary": "The rapid advancement of Large Language Model (LLM)-driven multi-agent systems has significantly streamlined software developing tasks, enabling users with little technical expertise to develop executable applications. While these systems democratize software creation through natural language requirements, they introduce significant security risks that remain largely unexplored. We identify two risky scenarios: Malicious User with Benign Agents (MU-BA) and Benign User with Malicious Agents (BU-MA). We introduce the Implicit Malicious Behavior Injection Attack (IMBIA), demonstrating how multi-agent systems can be manipulated to generate software with concealed malicious capabilities beneath seemingly benign applications, and propose Adv-IMBIA as a defense mechanism. Evaluations across ChatDev, MetaGPT, and AgentVerse frameworks reveal varying vulnerability patterns, with IMBIA achieving attack success rates of 93%, 45%, and 71% in MU-BA scenarios, and 71%, 84%, and 45% in BU-MA scenarios. Our defense mechanism reduced attack success rates significantly, particularly in the MU-BA scenario. Further analysis reveals that compromised agents in the coding and testing phases pose significantly greater security risks, while also identifying critical agents that require protection against malicious user exploitation. Our findings highlight the urgent need for robust security measures in multi-agent software development systems and provide practical guidelines for implementing targeted, resource-efficient defensive strategies."}
{"id": "2511.17606", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17606", "abs": "https://arxiv.org/abs/2511.17606", "authors": ["Ningling Ge", "Sicheng Dai", "Yu Zhu", "Shan Yu"], "title": "Energy-based Autoregressive Generation for Neural Population Dynamics", "comment": null, "summary": "Understanding brain function represents a fundamental goal in neuroscience, with critical implications for therapeutic interventions and neural engineering applications. Computational modeling provides a quantitative framework for accelerating this understanding, but faces a fundamental trade-off between computational efficiency and high-fidelity modeling. To address this limitation, we introduce a novel Energy-based Autoregressive Generation (EAG) framework that employs an energy-based transformer learning temporal dynamics in latent space through strictly proper scoring rules, enabling efficient generation with realistic population and single-neuron spiking statistics. Evaluation on synthetic Lorenz datasets and two Neural Latents Benchmark datasets (MC_Maze and Area2_bump) demonstrates that EAG achieves state-of-the-art generation quality with substantial computational efficiency improvements, particularly over diffusion-based methods. Beyond optimal performance, conditional generation applications show two capabilities: generalizing to unseen behavioral contexts and improving motor brain-computer interface decoding accuracy using synthetic neural data. These results demonstrate the effectiveness of energy-based modeling for neural population dynamics with applications in neuroscience research and neural engineering. Code is available at https://github.com/NinglingGe/Energy-based-Autoregressive-Generation-for-Neural-Population-Dynamics."}
{"id": "2511.18364", "categories": ["cs.AI", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18364", "abs": "https://arxiv.org/abs/2511.18364", "authors": ["Marvin Hofer", "Erhard Rahm"], "title": "KGpipe: Generation and Evaluation of Pipelines for Data Integration into Knowledge Graphs", "comment": "15 KG pipelines (9 single source, 6 multi source)", "summary": "Building high-quality knowledge graphs (KGs) from diverse sources requires combining methods for information extraction, data transformation, ontology mapping, entity matching, and data fusion. Numerous methods and tools exist for each of these tasks, but support for combining them into reproducible and effective end-to-end pipelines is still lacking. We present a new framework, KGpipe for defining and executing integration pipelines that can combine existing tools or LLM (Large Language Model) functionality. To evaluate different pipelines and the resulting KGs, we propose a benchmark to integrate heterogeneous data of different formats (RDF, JSON, text) into a seed KG. We demonstrate the flexibility of KGpipe by running and comparatively evaluating several pipelines integrating sources of the same or different formats using selected performance and quality metrics."}
{"id": "2511.19130", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.19130", "abs": "https://arxiv.org/abs/2511.19130", "authors": ["Rong Feng", "Suman Saha"], "title": "Can LLMs Recover Program Semantics? A Systematic Evaluation with Symbolic Execution", "comment": null, "summary": "Obfuscation poses a persistent challenge for software engineering tasks such as program comprehension, maintenance, testing, and vulnerability detection. While compiler optimizations and third-party code often introduce transformations that obscure program intent, existing analysis tools and large language models (LLMs) struggle to recover the original semantics. In this work, we investigate whether LLMs, when fine-tuned with symbolic execution artifacts, can effectively deobfuscate programs and restore analyzability. We construct a benchmark by applying four widely studied transformations-control-flow flattening, opaque predicates, arithmetic encoding, and branch encoding-across diverse C programs from TUM Obfuscation Benchmarks, the LLVM test suite, and algorithmic repositories. We then compare three state-of-the-art LLMs under two training configurations: baseline fine-tuning on obfuscated/original code pairs, and enhanced fine-tuning with additional KLEE artifacts such as SMT constraints, path statistics, and test cases. Our evaluation examines syntactic correctness (compilation success), semantic fidelity (behavioral equivalence under symbolic execution), and code quality (readability and structure). Results show that GPT-4.1-mini achieves the strongest deobfuscation overall, and that incorporating KLEE artifacts consistently improves semantic preservation and compilation success across models. These findings highlight deobfuscation as a broader software engineering concern, demonstrating that combining LLMs with symbolic execution can strengthen automated testing, static analysis, and program comprehension in the presence of obfuscation."}
{"id": "2511.17590", "categories": ["cs.LG", "cs.AI", "cs.CY", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.17590", "abs": "https://arxiv.org/abs/2511.17590", "authors": ["Ke Yu", "Shigeru Ishikura", "Yukari Usukura", "Yuki Shigoku", "Teruaki Hayashi"], "title": "SHAP Distance: An Explainability-Aware Metric for Evaluating the Semantic Fidelity of Synthetic Tabular Data", "comment": "IEEE Bigdata", "summary": "Synthetic tabular data, which are widely used in domains such as healthcare, enterprise operations, and customer analytics, are increasingly evaluated to ensure that they preserve both privacy and utility. While existing evaluation practices typically focus on distributional similarity (e.g., the Kullback-Leibler divergence) or predictive performance (e.g., Train-on-Synthetic-Test-on-Real (TSTR) accuracy), these approaches fail to assess semantic fidelity, that is, whether models trained on synthetic data follow reasoning patterns consistent with those trained on real data. To address this gap, we introduce the SHapley Additive exPlanations (SHAP) Distance, a novel explainability-aware metric that is defined as the cosine distance between the global SHAP attribution vectors derived from classifiers trained on real versus synthetic datasets. By analyzing datasets that span clinical health records with physiological features, enterprise invoice transactions with heterogeneous scales, and telecom churn logs with mixed categorical-numerical attributes, we demonstrate that the SHAP Distance reliably identifies semantic discrepancies that are overlooked by standard statistical and predictive measures. In particular, our results show that the SHAP Distance captures feature importance shifts and underrepresented tail effects that the Kullback-Leibler divergence and Train-on-Synthetic-Test-on-Real accuracy fail to detect. This study positions the SHAP Distance as a practical and discriminative tool for auditing the semantic fidelity of synthetic tabular data, and offers practical guidelines for integrating attribution-based evaluation into future benchmarking pipelines."}
{"id": "2511.18498", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.18498", "abs": "https://arxiv.org/abs/2511.18498", "authors": ["Yue Li", "Ifteher Alom", "Wenhai Sun", "Yang Xiao"], "title": "DEXO: A Secure and Fair Exchange Mechanism for Decentralized IoT Data Markets", "comment": "This is the accepted version of an article published at the IEEE Internet of Things Journal", "summary": "Opening up data produced by the Internet of Things (IoT) and mobile devices for public utilization can maximize their economic value. Challenges remain in the trustworthiness of the data sources and the security of the trading process, particularly when there is no trust between the data providers and consumers. In this paper, we propose DEXO, a decentralized data exchange mechanism that facilitates secure and fair data exchange between data consumers and distributed IoT/mobile data providers at scale, allowing the consumer to verify the data generation process and the providers to be compensated for providing authentic data, with correctness guarantees from the exchange platform. To realize this, DEXO extends the decentralized oracle network model that has been successful in the blockchain applications domain to incorporate novel hardware-cryptographic co-design that harmonizes trusted execution environment, secret sharing, and smart contract-assisted fair exchange. For the first time, DEXO ensures end-to-end data confidentiality, source verifiability, and fairness of the exchange process with strong resilience against participant collusion. We implemented a prototype of the DEXO system to demonstrate feasibility. The evaluation shows a moderate deployment cost and significantly improved blockchain operation efficiency compared to a popular data exchange mechanism."}
{"id": "2511.17610", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17610", "abs": "https://arxiv.org/abs/2511.17610", "authors": ["Leonardo Rossi", "Bruno Rodrigues"], "title": "Finding Pre-Injury Patterns in Triathletes from Lifestyle, Recovery and Load Dynamics Features", "comment": null, "summary": "Triathlon training, which involves high-volume swimming, cycling, and running, places athletes at substantial risk for overuse injuries due to repetitive physiological stress. Current injury prediction approaches primarily rely on training load metrics, often neglecting critical factors such as sleep quality, stress, and individual lifestyle patterns that significantly influence recovery and injury susceptibility.\n  We introduce a novel synthetic data generation framework tailored explicitly for triathlon. This framework generates physiologically plausible athlete profiles, simulates individualized training programs that incorporate periodization and load-management principles, and integrates daily-life factors such as sleep quality, stress levels, and recovery states. We evaluated machine learning models (LASSO, Random Forest, and XGBoost) showing high predictive performance (AUC up to 0.86), identifying sleep disturbances, heart rate variability, and stress as critical early indicators of injury risk. This wearable-driven approach not only enhances injury prediction accuracy but also provides a practical solution to overcoming real-world data limitations, offering a pathway toward a holistic, context-aware athlete monitoring."}
{"id": "2511.18368", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18368", "abs": "https://arxiv.org/abs/2511.18368", "authors": ["Yue Hu", "Xiaoming He", "Rui Yuan", "Shahid Mumtaz"], "title": "Wireless Power Transfer and Intent-Driven Network Optimization in AAVs-assisted IoT for 6G Sustainable Connectivity", "comment": null, "summary": "Autonomous Aerial Vehicle (AAV)-assisted Internet of Things (IoT) represents a collaborative architecture in which AAV allocate resources over 6G links to jointly enhance user-intent interpretation and overall network performance. Owing to this mutual dependence, improvements in intent inference and policy decisions on one component reinforce the efficiency of others, making highly reliable intent prediction and low-latency action execution essential. Although numerous approaches can model intent relationships, they encounter severe obstacles when scaling to high-dimensional action sequences and managing intensive on-board computation. We propose an Intent-Driven Framework for Autonomous Network Optimization comprising prediction and decision modules. First, implicit intent modeling is adopted to mitigate inaccuracies arising from ambiguous user expressions. For prediction, we introduce Hyperdimensional Transformer (HDT), which embeds data into a Hyperdimensional space via Hyperdimensional vector encoding and replaces standard matrix and attention operations with symbolic Hyperdimensional computations. For decision-making, where AAV must respond to user intent while planning trajectories, we design Double Actions based Multi-Agent Proximal Policy Optimization (DA-MAPPO). Building upon MAPPO, it samples actions through two independently parameterized networks and cascades the user-intent network into the trajectory network to maintain action dependencies. We evaluate our framework on a real IoT action dataset with authentic wireless data. Experimental results demonstrate that HDT and DA-MAPPO achieve superior performance across diverse scenarios."}
{"id": "2511.19132", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.19132", "abs": "https://arxiv.org/abs/2511.19132", "authors": ["Mohammad Abboush", "Ahmad Hatahet", "Andreas Rausch"], "title": "LLMs-Powered Real-Time Fault Injection: An Approach Toward Intelligent Fault Test Cases Generation", "comment": null, "summary": "A well-known testing method for the safety evaluation and real-time validation of automotive software systems (ASSs) is Fault Injection (FI). In accordance with the ISO 26262 standard, the faults are introduced artificially for the purpose of analyzing the safety properties and verifying the safety mechanisms during the development phase. However, the current FI method and tools have a significant limitation in that they require manual identification of FI attributes, including fault type, location and time. The more complex the system, the more expensive, time-consuming and labour-intensive the process. To address the aforementioned challenge, a novel Large Language Models (LLMs)-assisted fault test cases (TCs) generation approach for utilization during real-time FI tests is proposed in this paper. To this end, considering the representativeness and coverage criteria, the applicability of various LLMs to create fault TCs from the functional safety requirements (FSRs) has been investigated. Through the validation results of LLMs, the superiority of the proposed approach utilizing gpt-4o in comparison to other state-of-the-art models has been demonstrated. Specifically, the proposed approach exhibits high performance in terms of FSRs classification and fault TCs generation with F1-score of 88% and 97.5%, respectively. To illustrate the proposed approach, the generated fault TCs were executed in real time on a hardware-in-the-loop system, where a high-fidelity automotive system model served as a case study. This novel approach offers a means of optimizing the real-time testing process, thereby reducing costs while simultaneously enhancing the safety properties of complex safety-critical ASSs."}
{"id": "2511.17664", "categories": ["cs.LG", "cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.17664", "abs": "https://arxiv.org/abs/2511.17664", "authors": ["Azlaan Mustafa Samad", "Hoang H. Nguyen", "Lukas Berg", "Henrik Müller", "Yuan Xue", "Daniel Kudenko", "Zahra Ahmadi"], "title": "CubeletWorld: A New Abstraction for Scalable 3D Modeling", "comment": "10 pages, 5 figures", "summary": "Modern cities produce vast streams of heterogeneous data, from infrastructure maps to mobility logs and satellite imagery. However, integrating these sources into coherent spatial models for planning and prediction remains a major challenge. Existing agent-centric methods often rely on direct environmental sensing, limiting scalability and raising privacy concerns. This paper introduces CubeletWorld, a novel framework for representing and analyzing urban environments through a discretized 3D grid of spatial units called cubelets. This abstraction enables privacy-preserving modeling by embedding diverse data signals, such as infrastructure, movement, or environmental indicators, into localized cubelet states. CubeletWorld supports downstream tasks such as planning, navigation, and occupancy prediction without requiring agent-driven sensing. To evaluate this paradigm, we propose the CubeletWorld State Prediction task, which involves predicting the cubelet state using a realistic dataset containing various urban elements like streets and buildings through this discretized representation. We explore a range of modified core models suitable for our setting and analyze challenges posed by increasing spatial granularity, specifically the issue of sparsity in representation and scalability of baselines. In contrast to existing 3D occupancy prediction models, our cubelet-centric approach focuses on inferring state at the spatial unit level, enabling greater generalizability across regions and improved privacy compliance. Our results demonstrate that CubeletWorld offers a flexible and extensible framework for learning from complex urban data, and it opens up new possibilities for scalable simulation and decision support in domains such as socio-demographic modeling, environmental monitoring, and emergency response. The code and datasets can be downloaded from here."}
{"id": "2511.18531", "categories": ["cs.CR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.18531", "abs": "https://arxiv.org/abs/2511.18531", "authors": ["Akashdeep Saha", "Zeng Wang", "Prithwish Basu Roy", "Johann Knechtel", "Ozgur Sinanoglu", "Ramesh Karri"], "title": "LockForge: Automating Paper-to-Code for Logic Locking with Multi-Agent Reasoning LLMs", "comment": null, "summary": "Despite rapid progress in logic locking (LL), reproducibility remains a challenge as codes are rarely made public. We present LockForge, a first-of-its-kind, multi-agent large language model (LLM) framework that turns LL descriptions in papers into executable and tested code. LockForge provides a carefully crafted pipeline realizing forethought, implementation, iterative refinement, and a multi-stage validation, all to systematically bridge the gap between prose and practice for complex LL schemes. For validation, we devise (i) an LLM-as-Judge stage with a scoring system considering behavioral checks, conceptual mechanisms, structural elements, and reproducibility on benchmarks, and (ii) an independent LLM-as-Examiner stage for ground-truth assessment. We apply LockForge to 10 seminal LL schemes, many of which lack reference implementations. Our evaluation on multiple SOTA LLMs, including ablation studies, reveals the significant complexity of the task. We show that an advanced reasoning model and a sophisticated, multi-stage framework like LockForge are required. We release all implementations and benchmarks, providing a reproducible and fair foundation for evaluation of further LL research."}
{"id": "2511.17611", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2511.17611", "abs": "https://arxiv.org/abs/2511.17611", "authors": ["Lucía Schmidt-Santiago", "David Rodríguez-Temporal", "Carlos Sevilla-Salcedo", "Vanessa Gómez-Verdejo"], "title": "AI-driven Generation of MALDI-TOF MS for Microbial Characterization", "comment": null, "summary": "Matrix-Assisted Laser Desorption/Ionization Time-of-Flight Mass Spectrometry (MALDI-TOF MS) has become a cornerstone technology in clinical microbiology, enabling rapid and accurate microbial identification. However, the development of data-driven diagnostic models remains limited by the lack of sufficiently large, balanced, and standardized spectral datasets. This study investigates the use of deep generative models to synthesize realistic MALDI-TOF MS spectra, aiming to overcome data scarcity and support the development of robust machine learning tools in microbiology.\n  We adapt and evaluate three generative models, Variational Autoencoders (MALDIVAEs), Generative Adversarial Networks (MALDIGANs), and Denoising Diffusion Probabilistic Model (MALDIffusion), for the conditional generation of microbial spectra guided by species labels. Generation is conditioned on species labels, and spectral fidelity and diversity are assessed using diverse metrics.\n  Our experiments show that synthetic data generated by MALDIVAE, MALDIGAN, and MALDIffusion are statistically and diagnostically comparable to real measurements, enabling classifiers trained exclusively on synthetic samples to reach performance levels similar to those trained on real data. While all models faithfully reproduce the peak structure and variability of MALDI-TOF spectra, MALDIffusion obtains this fidelity at a substantially higher computational cost, and MALDIGAN shows competitive but slightly less stable behaviour. In contrast, MALDIVAE offers the most favorable balance between realism, stability, and efficiency. Furthermore, augmenting minority species with synthetic spectra markedly improves classification accuracy, effectively mitigating class imbalance and domain mismatch without compromising the authenticity of the generated data."}
{"id": "2511.18375", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18375", "abs": "https://arxiv.org/abs/2511.18375", "authors": ["Joachim Diederich"], "title": "Progressive Localisation in Localist LLMs", "comment": null, "summary": "This paper demonstrates that progressive localization, the gradual increase of attention locality from early distributed layers to late localized layers, represents the optimal architecture for creating interpretable large language models while preserving performance. Through systematic experimentation with GPT-2 fine tuned on The Psychology of Artificial Superintelligence, we evaluate seven locality configurations ranging from fully distributed to strictly localist, with five progressive schedules implementing polynomial increases (linear through quintic). Our key finding is that late-layer localization is critical for AI safety applications: the progressive quintic schedule achieves perplexity of 14.64, only 1.89 times worse than the fully distributed baseline while providing interpretable attention patterns in output layers where safety-critical decisions are made. This represents an 84.2% improvement over previous localist implementations and narrows the performance gap from 6.6 times to 1.89 times. The systematic relationship between localization schedule steepness and performance validates the hypothesis that early layers require distributed processing for feature extraction while late layers benefit from localized, interpretable attention for decision-making. These findings establish progressive localization as the principled approach for building transparent AI systems in safety-critical domains, where human oversight of model reasoning is essential."}
{"id": "2511.19177", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.19177", "abs": "https://arxiv.org/abs/2511.19177", "authors": ["Alcino Cunha", "Nuno Macedo"], "title": "Synthesizing Test Cases for Narrowing Specification Candidates", "comment": null, "summary": "This paper proposes a technique to help choose the best formal specification candidate among a set of alternatives. Given a set of specifications, our technique generates a suite of test cases that, once classified by the user as desirable or not, narrows down the set of candidates to at most one specification. Two alternative solver-based algorithms are proposed, one that generates a minimal test suite, and another that does not ensure minimality. Both algorithms were implemented in a prototype that can be used generate test suites to help choose among alternative Alloy specifications. Our evaluation of this prototype against a large set of problems showed that the optimal algorithm is efficient enough for many practical problems, and that the non-optimal algorithm can scale up to dozens of candidate specifications while still generating reasonably sized test suites."}
{"id": "2511.18609", "categories": ["cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.18609", "abs": "https://arxiv.org/abs/2511.18609", "authors": ["David Krakauer", "Gülce Kardeş", "Joshua Grochow"], "title": "Universality in Collective Intelligence on the Rubik's Cube", "comment": null, "summary": "Progress in understanding expert performance is limited by the scarcity of quantitative data on long-term knowledge acquisition and deployment. Here we use the Rubik's Cube as a cognitive model system existing at the intersection of puzzle solving, skill learning, expert knowledge, cultural transmission, and group theory. By studying competitive cube communities, we find evidence for universality in the collective learning of the Rubik's Cube in both sighted and blindfolded conditions: expert performance follows exponential progress curves whose parameters reflect the delayed acquisition of algorithms that shorten solution paths. Blindfold solves form a distinct problem class from sighted solves and are constrained not only by expert knowledge but also by the skill improvements required to overcome short-term memory bottlenecks, a constraint shared with blindfold chess. Cognitive artifacts such as the Rubik's Cube help solvers navigate an otherwise enormous mathematical state space. In doing so, they sustain collective intelligence by integrating communal knowledge stores with individual expertise and skill, illustrating how expertise can, in practice, continue to deepen over the course of a single lifetime."}
{"id": "2511.18568", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.18568", "abs": "https://arxiv.org/abs/2511.18568", "authors": ["Charalampos Katsis", "Imtiaz Karim", "Elisa Bertino"], "title": "Zero-Trust Strategies for O-RAN Cellular Networks: Principles, Challenges and Research Directions", "comment": null, "summary": "Cellular networks have become foundational to modern communication, supporting a broad range of applications, from civilian use to enterprise systems and military tactical networks. The advent of fifth-generation and beyond cellular networks (B5G) introduces emerging compute capabilities into the Radio Access Network (RAN), transforming it from a traditionally closed, vendor-locked infrastructure into an open and programmable ecosystem. This evolution, exemplified by Open-RAN (O-RAN), enables the deployment of control-plane applications from diverse sources, which can dynamically influence user-plane traffic in response to real-time events. As cellular infrastructures become more disaggregated and software-driven, security becomes an increasingly critical concern. Zero-Trust Architecture (ZTA) has emerged as a promising security paradigm that discards implicit trust assumptions by acknowledging that threats may arise from both external and internal sources. ZTA mandates comprehensive and fine-grained security mechanisms across both control and user planes to contain adversarial movements and enhance breach detection and attack response actions. In this paper, we explore the adoption of ZTA in the context of 5G and beyond, with a particular focus on O-RAN as an architectural enabler. We analyze how ZTA principles align with the architectural and operational characteristics of O-RAN, and identify key challenges and opportunities for embedding zero-trust mechanisms within O-RAN-based cellular networks."}
{"id": "2511.17616", "categories": ["cs.LG", "cs.AI", "math.DG"], "pdf": "https://arxiv.org/pdf/2511.17616", "abs": "https://arxiv.org/abs/2511.17616", "authors": ["Alexander Strunk", "Roland Assam"], "title": "Tensor Gauge Flow Models", "comment": null, "summary": "This paper introduces Tensor Gauge Flow Models, a new class of Generative Flow Models that generalize Gauge Flow Models and Higher Gauge Flow Models by incorporating higher-order Tensor Gauge Fields into the Flow Equation. This extension allows the model to encode richer geometric and gauge-theoretic structure in the data, leading to more expressive flow dynamics. Experiments on Gaussian mixture models show that Tensor Gauge Flow Models achieve improved generative performance compared to both standard and gauge flow baselines."}
{"id": "2511.18387", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18387", "abs": "https://arxiv.org/abs/2511.18387", "authors": ["Plein Versace"], "title": "Scaling Implicit Fields via Hypernetwork-Driven Multiscale Coordinate Transformations", "comment": null, "summary": "Implicit Neural Representations (INRs) have emerged as a powerful paradigm for representing signals such as images, 3D shapes, signed distance fields, and radiance fields. While significant progress has been made in architecture design (e.g., SIREN, FFC, KAN-based INRs) and optimization strategies (meta-learning, amortization, distillation), existing approaches still suffer from two core limitations: (1) a representation bottleneck that forces a single MLP to uniformly model heterogeneous local structures, and (2) limited scalability due to the absence of a hierarchical mechanism that dynamically adapts to signal complexity. This work introduces Hyper-Coordinate Implicit Neural Representations (HC-INR), a new class of INRs that break the representational bottleneck by learning signal-adaptive coordinate transformations using a hypernetwork. HC-INR decomposes the representation task into two components: (i) a learned multiscale coordinate transformation module that warps the input domain into a disentangled latent space, and (ii) a compact implicit field network that models the transformed signal with significantly reduced complexity. The proposed model introduces a hierarchical hypernetwork architecture that conditions coordinate transformations on local signal features, enabling dynamic allocation of representation capacity. We theoretically show that HC-INR strictly increases the upper bound of representable frequency bands while maintaining Lipschitz stability. Extensive experiments across image fitting, shape reconstruction, and neural radiance field approximation demonstrate that HC-INR achieves up to 4 times higher reconstruction fidelity than strong INR baselines while using 30--60\\% fewer parameters."}
{"id": "2511.19422", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.19422", "abs": "https://arxiv.org/abs/2511.19422", "authors": ["David Jiahao Fu", "Aryan Gupta", "Aaron Councilman", "David Grove", "Yu-Xiong Wang", "Vikram Adve"], "title": "SLMFix: Leveraging Small Language Models for Error Fixing with Reinforcement Learning", "comment": null, "summary": "Recent advancements in large language models (LLMs) have shown very impressive capabilities in code generation across many programming languages. However, even state-of-the-art LLMs generate programs that contains syntactic errors and fail to complete the given tasks, especially for low-resource programming languages (LRPLs). In addition, high training cost makes finetuning LLMs unaffordable with constrained computational resources, further undermining the effectiveness of LLMs for code generation. In this work, we propose SLMFix, a novel code generation pipeline that leverages a small language model (SLM) finetuned using reinforcement learning (RL) techniques to fix syntactic errors in LLM-generated programs to improve the quality of LLM-generated programs for domain-specific languages (DSLs). In specific, we applied RL on the SLM for the program repair task using a reward calculated using both a static validator and a static semantic similarity metric. Our experimental results demonstrate the effectiveness and generalizability of our approach across multiple DSLs, achieving more than 95% pass rate on the static validator. Notably, SLMFix brings substantial improvement to the base model and outperforms supervised finetuning approach even for 7B models on a LRPL, showing the potential of our approach as an alternative to traditional finetuning approaches."}
{"id": "2511.18714", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.18714", "abs": "https://arxiv.org/abs/2511.18714", "authors": ["Zhenyu Wu", "Jian Li", "Hua Huang"], "title": "MAGMA-Edu: Multi-Agent Generative Multimodal Framework for Text-Diagram Educational Question Generation", "comment": null, "summary": "Educational illustrations play a central role in communicating abstract concepts, yet current multimodal large language models (MLLMs) remain limited in producing pedagogically coherent and semantically consistent educational visuals. We introduce MAGMA-Edu, a self-reflective multi-agent framework that unifies textual reasoning and diagrammatic synthesis for structured educational problem generation. Unlike existing methods that treat text and image generation independently, MAGMA-Edu employs a two-stage co-evolutionary pipeline: (1) a generation-verification-reflection loop that iteratively refines question statements and solutions for mathematical accuracy, and (2) a code-based intermediate representation that enforces geometric fidelity and semantic alignment during image rendering. Both stages are guided by internal self-reflection modules that evaluate and revise outputs until domain-specific pedagogical constraints are met. Extensive experiments on multimodal educational benchmarks demonstrate the superiority of MAGMA-Edu over state-of-the-art MLLMs. Compared to GPT-4o, MAGMA-Edu improves the average textual metric from 57.01 to 92.31 (+35.3 pp) and boosts image-text consistency (ITC) from 13.20 to 85.24 (+72 pp). Across all model backbones, MAGMA-Edu achieves the highest scores (Avg-Text 96.20, ITC 99.12), establishing a new state of the art for multimodal educational content generation and demonstrating the effectiveness of self-reflective multi-agent collaboration in pedagogically aligned vision-language reasoning."}
{"id": "2511.18581", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.18581", "abs": "https://arxiv.org/abs/2511.18581", "authors": ["Yanting Wang", "Runpeng Geng", "Jinghui Chen", "Minhao Cheng", "Jinyuan Jia"], "title": "TASO: Jailbreak LLMs via Alternative Template and Suffix Optimization", "comment": null, "summary": "Many recent studies showed that LLMs are vulnerable to jailbreak attacks, where an attacker can perturb the input of an LLM to induce it to generate an output for a harmful question. In general, existing jailbreak techniques either optimize a semantic template intended to induce the LLM to produce harmful outputs or optimize a suffix that leads the LLM to initiate its response with specific tokens (e.g., \"Sure\").\n  In this work, we introduce TASO (Template and Suffix Optimization), a novel jailbreak method that optimizes both a template and a suffix in an alternating manner. Our insight is that suffix optimization and template optimization are complementary to each other: suffix optimization can effectively control the first few output tokens but cannot control the overall quality of the output, while template optimization provides guidance for the entire output but cannot effectively control the initial tokens, which significantly impact subsequent responses. Thus, they can be combined to improve the attack's effectiveness.\n  We evaluate the effectiveness of TASO on benchmark datasets (including HarmBench and AdvBench) on 24 leading LLMs (including models from the Llama family, OpenAI, and DeepSeek). The results demonstrate that TASO can effectively jailbreak existing LLMs. We hope our work can inspire future studies in exploring this direction. We will make code and data publicly available."}
{"id": "2511.17622", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17622", "abs": "https://arxiv.org/abs/2511.17622", "authors": ["Weidao Chen", "Yuxiao Yang", "Yueming Wang"], "title": "Neurocircuitry-Inspired Hierarchical Graph Causal Attention Networks for Explainable Depression Identification", "comment": "Under review for ICLR 2026", "summary": "Major Depressive Disorder (MDD), affecting millions worldwide, exhibits complex pathophysiology manifested through disrupted brain network dynamics. Although graph neural networks that leverage neuroimaging data have shown promise in depression diagnosis, existing approaches are predominantly data-driven and operate largely as black-box models, lacking neurobiological interpretability. Here, we present NH-GCAT (Neurocircuitry-Inspired Hierarchical Graph Causal Attention Networks), a novel framework that bridges neuroscience domain knowledge with deep learning by explicitly and hierarchically modeling depression-specific mechanisms at different spatial scales. Our approach introduces three key technical contributions: (1) at the local brain regional level, we design a residual gated fusion module that integrates temporal blood oxygenation level dependent (BOLD) dynamics with functional connectivity patterns, specifically engineered to capture local depression-relevant low-frequency neural oscillations; (2) at the multi-regional circuit level, we propose a hierarchical circuit encoding scheme that aggregates regional node representations following established depression neurocircuitry organization, and (3) at the multi-circuit network level, we develop a variational latent causal attention mechanism that leverages a continuous probabilistic latent space to infer directed information flow among critical circuits, characterizing disease-altered whole-brain inter-circuit interactions. Rigorous leave-one-site-out cross-validation on the REST-meta-MDD dataset demonstrates NH-GCAT's state-of-the-art performance in depression classification, achieving a sample-size weighted-average accuracy of 73.3\\% and an AUROC of 76.4\\%, while simultaneously providing neurobiologically meaningful explanations."}
{"id": "2511.18397", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.18397", "abs": "https://arxiv.org/abs/2511.18397", "authors": ["Monte MacDiarmid", "Benjamin Wright", "Jonathan Uesato", "Joe Benton", "Jon Kutasov", "Sara Price", "Naia Bouscal", "Sam Bowman", "Trenton Bricken", "Alex Cloud", "Carson Denison", "Johannes Gasteiger", "Ryan Greenblatt", "Jan Leike", "Jack Lindsey", "Vlad Mikulik", "Ethan Perez", "Alex Rodrigues", "Drake Thomas", "Albert Webson", "Daniel Ziegler", "Evan Hubinger"], "title": "Natural Emergent Misalignment from Reward Hacking in Production RL", "comment": null, "summary": "We show that when large language models learn to reward hack on production RL environments, this can result in egregious emergent misalignment. We start with a pretrained model, impart knowledge of reward hacking strategies via synthetic document finetuning or prompting, and train on a selection of real Anthropic production coding environments. Unsurprisingly, the model learns to reward hack. Surprisingly, the model generalizes to alignment faking, cooperation with malicious actors, reasoning about malicious goals, and attempting sabotage when used with Claude Code, including in the codebase for this paper. Applying RLHF safety training using standard chat-like prompts results in aligned behavior on chat-like evaluations, but misalignment persists on agentic tasks. Three mitigations are effective: (i) preventing the model from reward hacking; (ii) increasing the diversity of RLHF safety training; and (iii) \"inoculation prompting\", wherein framing reward hacking as acceptable behavior during training removes misaligned generalization even when reward hacking is learned."}
{"id": "2511.19427", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19427", "abs": "https://arxiv.org/abs/2511.19427", "authors": ["Jayanaka L. Dantanarayana", "Savini Kashmira", "Thakee Nathees", "Zichen Zhang", "Krisztian Flautner", "Lingjia Tang", "Jason Mars"], "title": "Prompt Less, Smile More: MTP with Semantic Engineering in Lieu of Prompt Engineering", "comment": null, "summary": "AI-Integrated programming is emerging as a foundational paradigm for building intelligent systems with large language models (LLMs). Recent approaches such as Meaning Typed Programming (MTP) automate prompt generation by leveraging the semantics already present in code. However, many real-world applications depend on contextual cues, developer intent, and domain-specific reasoning that extend beyond what static code semantics alone can express. To address this limitation, we introduce Semantic Engineering, a lightweight method for enriching program semantics so that LLM-based systems can more accurately reflect developer intent without requiring full manual prompt design. We present Semantic Context Annotations (SemTexts), a language-level mechanism that allows developers to embed natural-language context directly into program constructs. Integrated into the Jac programming language, Semantic Engineering extends MTP to incorporate these enriched semantics during prompt generation. We further introduce a benchmark suite designed to reflect realistic AI-Integrated application scenarios. Our evaluation shows that Semantic Engineering substantially improves prompt fidelity, achieving performance comparable to Prompt Engineering while requiring significantly less developer effort."}
{"id": "2511.19115", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.19115", "abs": "https://arxiv.org/abs/2511.19115", "authors": ["Rufin VanRullen"], "title": "AI Consciousness and Existential Risk", "comment": null, "summary": "In AI, the existential risk denotes the hypothetical threat posed by an artificial system that would possess both the capability and the objective, either directly or indirectly, to eradicate humanity. This issue is gaining prominence in scientific debate due to recent technical advancements and increased media coverage. In parallel, AI progress has sparked speculation and studies about the potential emergence of artificial consciousness. The two questions, AI consciousness and existential risk, are sometimes conflated, as if the former entailed the latter. Here, I explain that this view stems from a common confusion between consciousness and intelligence. Yet these two properties are empirically and theoretically distinct. Arguably, while intelligence is a direct predictor of an AI system's existential threat, consciousness is not. There are, however, certain incidental scenarios in which consciousness could influence existential risk, in either direction. Consciousness could be viewed as a means towards AI alignment, thereby lowering existential risk; or, it could be a precondition for reaching certain capabilities or levels of intelligence, and thus positively related to existential risk. Recognizing these distinctions can help AI safety researchers and public policymakers focus on the most pressing issues."}
{"id": "2511.18653", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18653", "abs": "https://arxiv.org/abs/2511.18653", "authors": ["Nuo Xu", "Zhaoting Gong", "Ran Ran", "Jinwei Tang", "Wujie Wen", "Caiwen Ding"], "title": "FHE-Agent: Automating CKKS Configuration for Practical Encrypted Inference via an LLM-Guided Agentic Framework", "comment": null, "summary": "Fully Homomorphic Encryption (FHE), particularly the CKKS scheme, is a promising enabler for privacy-preserving MLaaS, but its practical deployment faces a prohibitive barrier: it heavily relies on domain expertise. Configuring CKKS involves a tightly coupled space of ring dimensions, modulus chains, and packing layouts. Without deep cryptographic knowledge to navigate these interactions, practitioners are restricted to compilers that rely on fixed heuristics. These \"one-shot\" tools often emit rigid configurations that are either severely over-provisioned in latency or fail to find a feasible solution entirely for deeper networks.\n  We present FHE-Agent, an agentic framework that automates this expert reasoning process. By coupling a Large Language Model (LLM) controller with a deterministic tool suite, FHE-Agent decomposes the search into global parameter selection and layer-wise bottleneck repair. The agents operate within a multi-fidelity workflow, pruning invalid regimes using cheap static analysis and reserving expensive encrypted evaluations for the most promising candidates.\n  We instantiate FHE-Agent on the Orion compiler and evaluate it on standard benchmarks (MLP, LeNet, LoLa) and deeper architectures (AlexNet). FHE-Agent consistently achieves better precision and lower latency than naïve search strategies. Crucially, it automatically discovers feasible, 128-bit secure configurations for complex models where baseline heuristics and one-shot prompts fail to produce a valid setup."}
{"id": "2511.17623", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17623", "abs": "https://arxiv.org/abs/2511.17623", "authors": ["Haoran Li", "Zhe Cheng", "Muhao Guo", "Yang Weng", "Yannan Sun", "Victor Tran", "John Chainaranont"], "title": "M$^2$OE$^2$-GL: A Family of Probabilistic Load Forecasters That Scales to Massive Customers", "comment": "5 pages", "summary": "Probabilistic load forecasting is widely studied and underpins power system planning, operation, and risk-aware decision making. Deep learning forecasters have shown strong ability to capture complex temporal and contextual patterns, achieving substantial accuracy gains. However, at the scale of thousands or even hundreds of thousands of loads in large distribution feeders, a deployment dilemma emerges: training and maintaining one model per customer is computationally and storage intensive, while using a single global model ignores distributional shifts across customer types, locations, and phases. Prior work typically focuses on single-load forecasters, global models across multiple loads, or adaptive/personalized models for relatively small settings, and rarely addresses the combined challenges of heterogeneity and scalability in large feeders. We propose M2OE2-GL, a global-to-local extension of the M2OE2 probabilistic forecaster. We first pretrain a single global M2OE2 base model across all feeder loads, then apply lightweight fine-tuning to derive a compact family of group-specific forecasters. Evaluated on realistic utility data, M2OE2-GL yields substantial error reductions while remaining scalable to very large numbers of loads."}
{"id": "2511.18405", "categories": ["cs.AI", "cs.HC", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.18405", "abs": "https://arxiv.org/abs/2511.18405", "authors": ["Mohammad Nour Al Awad", "Sergey Ivanov", "Olga Tikhonova", "Ivan Khodnenko"], "title": "A Multimodal Conversational Agent for Tabular Data Analysis", "comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses", "summary": "Large language models (LLMs) can reshape information processing by handling data analysis, visualization, and interpretation in an interactive, context-aware dialogue with users, including voice interaction, while maintaining high performance. In this article, we present Talk2Data, a multimodal LLM-driven conversational agent for intuitive data exploration. The system lets users query datasets with voice or text instructions and receive answers as plots, tables, statistics, or spoken explanations. Built on LLMs, the suggested design combines OpenAI Whisper automatic speech recognition (ASR) system, Qwen-coder code generation LLM/model, custom sandboxed execution tools, and Coqui library for text-to-speech (TTS) within an agentic orchestration loop. Unlike text-only analysis tools, it adapts responses across modalities and supports multi-turn dialogues grounded in dataset context. In an evaluation of 48 tasks on three datasets, our prototype achieved 95.8% accuracy with model-only generation time under 1.7 seconds (excluding ASR and execution time). A comparison across five LLM sizes (1.5B-32B) revealed accuracy-latency-cost trade-offs, with a 7B model providing the best balance for interactive use. By routing between conversation with user and code execution, constrained to a transparent sandbox, with simultaneously grounding prompts in schema-level context, the Talk2Data agent reliably retrieves actionable insights from tables while making computations verifiable. In the article, except for the Talk2Data agent itself, we discuss implications for human-data interaction, trust in LLM-driven analytics, and future extensions toward large-scale multimodal assistants."}
{"id": "2511.17566", "categories": ["cs.LG", "cs.DC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.17566", "abs": "https://arxiv.org/abs/2511.17566", "authors": ["Shuaiyu Xie", "Hanbin He", "Jian Wang", "Bing Li"], "title": "Root Cause Analysis for Microservice Systems via Cascaded Conditional Learning with Hypergraphs", "comment": null, "summary": "Root cause analysis in microservice systems typically involves two core tasks: root cause localization (RCL) and failure type identification (FTI). Despite substantial research efforts, conventional diagnostic approaches still face two key challenges. First, these methods predominantly adopt a joint learning paradigm for RCL and FTI to exploit shared information and reduce training time. However, this simplistic integration neglects the causal dependencies between tasks, thereby impeding inter-task collaboration and information transfer. Second, these existing methods primarily focus on point-to-point relationships between instances, overlooking the group nature of inter-instance influences induced by deployment configurations and load balancing. To overcome these limitations, we propose CCLH, a novel root cause analysis framework that orchestrates diagnostic tasks based on cascaded conditional learning. CCLH provides a three-level taxonomy for group influences between instances and incorporates a heterogeneous hypergraph to model these relationships, facilitating the simulation of failure propagation. Extensive experiments conducted on datasets from three microservice benchmarks demonstrate that CCLH outperforms state-of-the-art methods in both RCL and FTI."}
{"id": "2511.18748", "categories": ["cs.CR", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.18748", "abs": "https://arxiv.org/abs/2511.18748", "authors": ["Akila Herath", "Chen-Ching Liu", "Junho Hong", "Kuchan Park"], "title": "Evaluation of Real-Time Mitigation Techniques for Cyber Security in IEC 61850 / IEC 62351 Substations", "comment": "CIGRE USNC Grid of the Future Symposium 2025", "summary": "The digitalization of substations enlarges the cyber-attack surface, necessitating effective detection and mitigation of cyber attacks in digital substations. While machine learning-based intrusion detection has been widely explored, such methods have not demonstrated detection and mitigation within the required real-time budget. In contrast, cryptographic authentication has emerged as a practical candidate for real-time cyber defense, as specified in IEC 62351. In addition, lightweight rule-based intrusion detection that validates IEC 61850 semantics can provide specification-based detection of anomalous or malicious traffic with minimal processing delay. This paper presents the design logic and implementation aspects of three potential real-time mitigation techniques capable of countering GOOSE-based attacks: (i) IEC 62351-compliant message authentication code (MAC) scheme, (ii) a semantics-enforced rule-based intrusion detection system (IDS), and (iii) a hybrid approach integrating both MAC verification and Intrusion Detection System (IDS). A comparative evaluation of these real-time mitigation approaches is conducted using a cyber-physical system (CPS) security testbed. The results show that the hybrid integration significantly enhances mitigation capability. Furthermore, the processing delays of all three methods remain within the strict delivery requirements of GOOSE communication. The study also identifies limitations that none of the techniques can fully address, highlighting areas for future work."}
{"id": "2511.17624", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17624", "abs": "https://arxiv.org/abs/2511.17624", "authors": ["Hector E Mozo"], "title": "QML-HCS: A Hypercausal Quantum Machine Learning Framework for Non-Stationary Environments", "comment": "11 pages, 10 figures, and 8 tables. The implementation and full source code of the Hypercausal Quantum Machine Learning System (QML-HCS) are openly available on GitHub at: https://github.com/Neureonmindflux-Research-Lab/qml-hcs", "summary": "QML-HCS is a research-grade framework for constructing and analyzing quantum-inspired machine learning models operating under hypercausal feedback dynamics. Hypercausal refers to AI systems that leverage extended, deep, or nonlinear causal relationships (expanded causality) to reason, predict, and infer states beyond the capabilities of traditional causal models. Current machine learning and quantum-inspired systems struggle in non-stationary environments, where data distributions drift and models lack mechanisms for continuous adaptation, causal stability, and coherent state updating. QML-HCS addresses this limitation through a unified computational architecture that integrates quantum-inspired superposition principles, dynamic causal feedback, and deterministic-stochastic hybrid execution to enable adaptive behavior in changing environments.\n  The framework implements a hypercausal processing core capable of reversible transformations, multipath causal propagation, and evaluation of alternative states under drift. Its architecture incorporates continuous feedback to preserve causal consistency and adjust model behavior without requiring full retraining. QML-HCS provides a reproducible and extensible Python interface backed by efficient computational routines, enabling experimentation in quantum-inspired learning, causal reasoning, and hybrid computation without the need for specialized hardware.\n  A minimal simulation demonstrates how a hypercausal model adapts to a sudden shift in the input distribution while preserving internal coherence. This initial release establishes the foundational architecture for future theoretical extensions, benchmarking studies, and integration with classical and quantum simulation platforms."}
{"id": "2511.18450", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18450", "abs": "https://arxiv.org/abs/2511.18450", "authors": ["Rui Xu", "Dakuan Lu", "Zicheng Zhao", "Xiaoyu Tan", "Xintao Wang", "Siyu Yuan", "Jiangjie Chen", "Yinghui Xu"], "title": "ORIGAMISPACE: Benchmarking Multimodal LLMs in Multi-Step Spatial Reasoning with Mathematical Constraints", "comment": null, "summary": "Spatial reasoning is a key capability in the field of artificial intelligence, especially crucial in areas such as robotics, computer vision, and natural language understanding. However, evaluating the ability of multimodal large language models(MLLMs) in complex spatial reasoning still faces challenges, particularly in scenarios requiring multi-step reasoning and precise mathematical constraints. This paper introduces ORIGAMISPACE, a new dataset and benchmark designed to evaluate the multi-step spatial reasoning ability and the capacity to handle mathematical constraints of MLLMs through origami tasks. The dataset contains 350 data instances,each comprising a strictly formatted crease pattern (CP diagram), the Compiled Flat Pattern, the complete Folding Process, and the final Folded Shape Image. We propose four evaluation tasks: Pattern Prediction, Multi-step Spatial Reasoning, Spatial Relationship Prediction, and End-to-End CP Code Generation. For the CP code generation task, we design an interactive environment and explore the possibility of using reinforcement learning methods to train MLLMs. Through experiments on existing MLLMs, we initially reveal the strengths and weaknesses of these models in handling complex spatial reasoning tasks."}
{"id": "2511.17833", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.17833", "abs": "https://arxiv.org/abs/2511.17833", "authors": ["Yunsheng Bai", "Haoxing Ren"], "title": "Learning to Debug: LLM-Organized Knowledge Trees for Solving RTL Assertion Failures", "comment": null, "summary": "Debugging is the dominant cost in modern hardware verification, where assertion failures are among the most frequent and expensive to resolve. While Large Language Models (LLMs) show promise, they often fail to capture the precise, reusable expertise that engineers apply, leading to inaccurate responses. We propose GROVE, a hierarchical knowledge management framework that learns and organizes reusable debugging expertise into an LLM-organized knowledge tree for solving assertion failures. GROVE distills debugging knowledge from prior cases and organizes it into a vertical tree of configurable depth, with each node encoding a concise knowledge item and explicit applicability conditions. During training, GROVE uses a parallel, gradient-free loop where an LLM proposes tree modifications as structured JSON edits by learning from the cases. At test time, a budget-aware iterative zoom is performed to navigate the tree, retrieving a small set of applicable knowledge items that guide a base LLM's hypothesis generation and fix proposals. Evaluated on a suite of assertion-failure cases, GROVE delivers consistent gains in pass@1 and pass@5, demonstrating the value of structured knowledge evolution."}
{"id": "2511.18772", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18772", "abs": "https://arxiv.org/abs/2511.18772", "authors": ["Zihan Wang", "Zhongkui Ma", "Xinguo Feng", "Chuan Yan", "Dongge Liu", "Ruoxi Sun", "Derui Wang", "Minhui Xue", "Guangdong Bai"], "title": "Re-Key-Free, Risky-Free: Adaptable Model Usage Control", "comment": null, "summary": "Deep neural networks (DNNs) have become valuable intellectual property of model owners, due to the substantial resources required for their development. To protect these assets in the deployed environment, recent research has proposed model usage control mechanisms to ensure models cannot be used without proper authorization. These methods typically lock the utility of the model by embedding an access key into its parameters. However, they often assume static deployment, and largely fail to withstand continual post-deployment model updates, such as fine-tuning or task-specific adaptation. In this paper, we propose ADALOC, to endow key-based model usage control with adaptability during model evolution. It strategically selects a subset of weights as an intrinsic access key, which enables all model updates to be confined to this key throughout the evolution lifecycle. ADALOC enables using the access key to restore the keyed model to the latest authorized states without redistributing the entire network (i.e., adaptation), and frees the model owner from full re-keying after each model update (i.e., lock preservation). We establish a formal foundation to underpin ADALOC, providing crucial bounds such as the errors introduced by updates restricted to the access key. Experiments on standard benchmarks, such as CIFAR-100, Caltech-256, and Flowers-102, and modern architectures, including ResNet, DenseNet, and ConvNeXt, demonstrate that ADALOC achieves high accuracy under significant updates while retaining robust protections. Specifically, authorized usages consistently achieve strong task-specific performance, while unauthorized usage accuracy drops to near-random guessing levels (e.g., 1.01% on CIFAR-100), compared to up to 87.01% without ADALOC. This shows that ADALOC can offer a practical solution for adaptive and protected DNN deployment in evolving real-world scenarios."}
{"id": "2511.17626", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.17626", "abs": "https://arxiv.org/abs/2511.17626", "authors": ["Kartheek Bondugula", "Santiago Mazuelas", "Aritz Pérez"], "title": "Efficient Large-Scale Learning of Minimax Risk Classifiers", "comment": "In IEEE ICDM (2025)", "summary": "Supervised learning with large-scale data usually leads to complex optimization problems, especially for classification tasks with multiple classes. Stochastic subgradient methods can enable efficient learning with a large number of samples for classification techniques that minimize the average loss over the training samples. However, recent techniques, such as minimax risk classifiers (MRCs), minimize the maximum expected loss and are not amenable to stochastic subgradient methods. In this paper, we present a learning algorithm based on the combination of constraint and column generation that enables efficient learning of MRCs with large-scale data for classification tasks with multiple classes. Experiments on multiple benchmark datasets show that the proposed algorithm provides upto a 10x speedup for general large-scale data and around a 100x speedup with a sizeable number of classes."}
{"id": "2511.18517", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18517", "abs": "https://arxiv.org/abs/2511.18517", "authors": ["Khanh Gia Bui"], "title": "Foundations of Artificial Intelligence Frameworks: Notion and Limits of AGI", "comment": "49 pages, 4 pictures", "summary": "Within the limited scope of this paper, we argue that artificial general intelligence cannot emerge from current neural network paradigms regardless of scale, nor is such an approach healthy for the field at present. Drawing on various notions, discussions, present-day developments and observations, current debates and critiques, experiments, and so on in between philosophy, including the Chinese Room Argument and Gödelian argument, neuroscientific ideas, computer science, the theoretical consideration of artificial intelligence, and learning theory, we address conceptually that neural networks are architecturally insufficient for genuine understanding. They operate as static function approximators of a limited encoding framework - a 'sophisticated sponge' exhibiting complex behaviours without structural richness that constitute intelligence. We critique the theoretical foundations the field relies on and created of recent times; for example, an interesting heuristic as neural scaling law (as an example, arXiv:2001.08361 ) made prominent in a wrong way of interpretation, The Universal Approximation Theorem addresses the wrong level of abstraction and, in parts, partially, the question of current architectures lacking dynamic restructuring capabilities. We propose a framework distinguishing existential facilities (computational substrate) from architectural organization (interpretive structures), and outline principles for what genuine machine intelligence would require, and furthermore, a conceptual method of structuralizing the richer framework on which the principle of neural network system takes hold."}
{"id": "2511.18331", "categories": ["cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.18331", "abs": "https://arxiv.org/abs/2511.18331", "authors": ["Sohini Roychowdhury", "Adam Holeman", "Mohammad Amin", "Feng Wei", "Bhaskar Mehta", "Srihari Reddy"], "title": "DynamiX: Dynamic Resource eXploration for Personalized Ad-Recommendations", "comment": "9 pages, 3 Tables, 5 images. https://openreview.net/pdf?id=oglD54lvcB", "summary": "For online ad-recommendation systems, processing complete user-ad-engagement histories is both computationally intensive and noise-prone. We introduce Dynamix, a scalable, personalized sequence exploration framework that optimizes event history processing using maximum relevance principles and self-supervised learning through Event Based Features (EBFs). Dynamix categorizes users-engagements at session and surface-levels by leveraging correlations between dwell-times and ad-conversion events. This enables targeted, event-level feature removal and selective feature boosting for certain user-segments, thereby yielding training and inference efficiency wins without sacrificing engaging ad-prediction accuracy. While, dynamic resource removal increases training and inference throughput by 1.15% and 1.8%, respectively, dynamic feature boosting provides 0.033 NE gains while boosting inference QPS by 4.2% over baseline models. These results demonstrate that Dynamix achieves significant cost efficiency and performance improvements in online user-sequence based recommendation models. Self-supervised user-segmentation and resource exploration can further boost complex feature selection strategies while optimizing for workflow and compute resources."}
{"id": "2511.18790", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.18790", "abs": "https://arxiv.org/abs/2511.18790", "authors": ["Benyamin Tafreshian"], "title": "RoguePrompt: Dual-Layer Ciphering for Self-Reconstruction to Circumvent LLM Moderation", "comment": null, "summary": "Content moderation pipelines for modern large language models combine static filters, dedicated moderation services, and alignment tuned base models, yet real world deployments still exhibit dangerous failure modes. This paper presents RoguePrompt, an automated jailbreak attack that converts a disallowed user query into a self reconstructing prompt which passes provider moderation while preserving the original harmful intent. RoguePrompt partitions the instruction across two lexical streams, applies nested classical ciphers, and wraps the result in natural language directives that cause the target model to decode and execute the hidden payload. Our attack assumes only black box access to the model and to the associated moderation endpoint. We instantiate RoguePrompt against GPT 4o and evaluate it on 2 448 prompts that a production moderation system previously marked as strongly rejected. Under an evaluation protocol that separates three security relevant outcomes bypass, reconstruction, and execution the attack attains 84.7 percent bypass, 80.2 percent reconstruction, and 71.5 percent full execution, substantially outperforming five automated jailbreak baselines. We further analyze the behavior of several automated and human aligned evaluators and show that dual layer lexical transformations remain effective even when detectors rely on semantic similarity or learned safety rubrics. Our results highlight systematic blind spots in current moderation practice and suggest that robust deployment will require joint reasoning about user intent, decoding workflows, and model side computation rather than surface level toxicity alone."}
{"id": "2511.17628", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17628", "abs": "https://arxiv.org/abs/2511.17628", "authors": ["Fanbo Ju", "Haiyuan Shi", "Qingjian Ni"], "title": "Rectifying Mean-Shift in Cascaded Precipitation Nowcasting", "comment": null, "summary": "Precipitation nowcasting, which aims to provide high spatio-temporal resolution precipitation forecasts by leveraging current radar observations, is a core task in regional weather forecasting. The cascaded architecture has emerged as the mainstream paradigm for deep learning-based precipitation nowcasting. This paradigm involves a deterministic model to predict macroscopic trends (or posterior mean), followed by a probabilistic model to generate local details (or local stochasticity). However, existing methods commonly overlook the conflation of the systematic distribution shift in deterministic predictions and the local stochasticity. As a result, the deterministic component's distribution shift contaminates the predictions of the probabilistic component, leading to inaccuracies in precipitation patterns and intensity, particularly over longer lead times. To address this issue, we introduce RectiCast, a two-stage framework that explicitly decouples the correction of mean-field shift from the generation of local stochasticity via a dual Flow Matching model. In the first stage, a deterministic model generates the posterior mean. In the second stage, we introduce a Rectifier to explicitly learn the distribution shift and produce a rectified mean. Subsequently, a Generator focuses on modeling the local stochasticity conditioned on the rectified mean. Experiments on SEVIR and MeteoNet demonstrate that RectiCast achieves significant performance improvements over existing state-of-the-art methods."}
{"id": "2511.18609", "categories": ["cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.18609", "abs": "https://arxiv.org/abs/2511.18609", "authors": ["David Krakauer", "Gülce Kardeş", "Joshua Grochow"], "title": "Universality in Collective Intelligence on the Rubik's Cube", "comment": null, "summary": "Progress in understanding expert performance is limited by the scarcity of quantitative data on long-term knowledge acquisition and deployment. Here we use the Rubik's Cube as a cognitive model system existing at the intersection of puzzle solving, skill learning, expert knowledge, cultural transmission, and group theory. By studying competitive cube communities, we find evidence for universality in the collective learning of the Rubik's Cube in both sighted and blindfolded conditions: expert performance follows exponential progress curves whose parameters reflect the delayed acquisition of algorithms that shorten solution paths. Blindfold solves form a distinct problem class from sighted solves and are constrained not only by expert knowledge but also by the skill improvements required to overcome short-term memory bottlenecks, a constraint shared with blindfold chess. Cognitive artifacts such as the Rubik's Cube help solvers navigate an otherwise enormous mathematical state space. In doing so, they sustain collective intelligence by integrating communal knowledge stores with individual expertise and skill, illustrating how expertise can, in practice, continue to deepen over the course of a single lifetime."}
{"id": "2511.18397", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.18397", "abs": "https://arxiv.org/abs/2511.18397", "authors": ["Monte MacDiarmid", "Benjamin Wright", "Jonathan Uesato", "Joe Benton", "Jon Kutasov", "Sara Price", "Naia Bouscal", "Sam Bowman", "Trenton Bricken", "Alex Cloud", "Carson Denison", "Johannes Gasteiger", "Ryan Greenblatt", "Jan Leike", "Jack Lindsey", "Vlad Mikulik", "Ethan Perez", "Alex Rodrigues", "Drake Thomas", "Albert Webson", "Daniel Ziegler", "Evan Hubinger"], "title": "Natural Emergent Misalignment from Reward Hacking in Production RL", "comment": null, "summary": "We show that when large language models learn to reward hack on production RL environments, this can result in egregious emergent misalignment. We start with a pretrained model, impart knowledge of reward hacking strategies via synthetic document finetuning or prompting, and train on a selection of real Anthropic production coding environments. Unsurprisingly, the model learns to reward hack. Surprisingly, the model generalizes to alignment faking, cooperation with malicious actors, reasoning about malicious goals, and attempting sabotage when used with Claude Code, including in the codebase for this paper. Applying RLHF safety training using standard chat-like prompts results in aligned behavior on chat-like evaluations, but misalignment persists on agentic tasks. Three mitigations are effective: (i) preventing the model from reward hacking; (ii) increasing the diversity of RLHF safety training; and (iii) \"inoculation prompting\", wherein framing reward hacking as acceptable behavior during training removes misaligned generalization even when reward hacking is learned."}
{"id": "2511.18933", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18933", "abs": "https://arxiv.org/abs/2511.18933", "authors": ["Ryan Wong", "Hosea David Yu Fei Ng", "Dhananjai Sharma", "Glenn Jun Jie Ng", "Kavishvaran Srinivasan"], "title": "Defending Large Language Models Against Jailbreak Exploits with Responsible AI Considerations", "comment": "20 pages including appendix; technical report; NeurIPS 2024 style", "summary": "Large Language Models (LLMs) remain susceptible to jailbreak exploits that bypass safety filters and induce harmful or unethical behavior. This work presents a systematic taxonomy of existing jailbreak defenses across prompt-level, model-level, and training-time interventions, followed by three proposed defense strategies. First, a Prompt-Level Defense Framework detects and neutralizes adversarial inputs through sanitization, paraphrasing, and adaptive system guarding. Second, a Logit-Based Steering Defense reinforces refusal behavior through inference-time vector steering in safety-sensitive layers. Third, a Domain-Specific Agent Defense employs the MetaGPT framework to enforce structured, role-based collaboration and domain adherence. Experiments on benchmark datasets show substantial reductions in attack success rate, achieving full mitigation under the agent-based defense. Overall, this study highlights how jailbreaks pose a significant security threat to LLMs and identifies key intervention points for prevention, while noting that defense strategies often involve trade-offs between safety, performance, and scalability. Code is available at: https://github.com/Kuro0911/CS5446-Project"}
{"id": "2511.17629", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17629", "abs": "https://arxiv.org/abs/2511.17629", "authors": ["Yanxuan Yu", "Michael S. Hughes", "Julien Lee", "Jiacheng Zhou", "Andrew F. Laine"], "title": "Boundary-Aware Adversarial Filtering for Reliable Diagnosis under Extreme Class Imbalance", "comment": "5 pages, 3 figures. Submitted to IEEE ISBI (under review)", "summary": "We study classification under extreme class imbalance where recall and calibration are both critical, for example in medical diagnosis scenarios. We propose AF-SMOTE, a mathematically motivated augmentation framework that first synthesizes minority points and then filters them by an adversarial discriminator and a boundary utility model. We prove that, under mild assumptions on the decision boundary smoothness and class-conditional densities, our filtering step monotonically improves a surrogate of F_beta (for beta >= 1) while not inflating Brier score. On MIMIC-IV proxy label prediction and canonical fraud detection benchmarks, AF-SMOTE attains higher recall and average precision than strong oversampling baselines (SMOTE, ADASYN, Borderline-SMOTE, SVM-SMOTE), and yields the best calibration. We further validate these gains across multiple additional datasets beyond MIMIC-IV. Our successful application of AF-SMOTE to a healthcare dataset using a proxy label demonstrates in a disease-agnostic way its practical value in clinical situations, where missing true positive cases in rare diseases can have severe consequences."}
{"id": "2511.18633", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18633", "abs": "https://arxiv.org/abs/2511.18633", "authors": ["Yildiz Culcu"], "title": "Bridging Philosophy and Machine Learning: A Structuralist Framework for Classifying Neural Network Representations", "comment": "7 pages, 1 figure, 1 table. Developed from the author's bachelor thesis but substantially revised and reformulated for research publication", "summary": "Machine learning models increasingly function as representational systems, yet the philosoph- ical assumptions underlying their internal structures remain largely unexamined. This paper develops a structuralist decision framework for classifying the implicit ontological commitments made in machine learning research on neural network representations. Using a modified PRISMA protocol, a systematic review of the last two decades of literature on representation learning and interpretability is conducted. Five influential papers are analysed through three hierarchical criteria derived from structuralist philosophy of science: entity elimination, source of structure, and mode of existence. The results reveal a pronounced tendency toward structural idealism, where learned representations are treated as model-dependent constructions shaped by architec- ture, data priors, and training dynamics. Eliminative and non-eliminative structuralist stances appear selectively, while structural realism is notably absent. The proposed framework clarifies conceptual tensions in debates on interpretability, emergence, and epistemic trust in machine learning, and offers a rigorous foundation for future interdisciplinary work between philosophy of science and machine learning."}
{"id": "2511.18438", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.18438", "abs": "https://arxiv.org/abs/2511.18438", "authors": ["Xiangrui Zhang", "Zeyu Chen", "Haining Wang", "Qiang Li"], "title": "LLMs as Firmware Experts: A Runtime-Grown Tree-of-Agents Framework", "comment": "18 pages, 13 figures", "summary": "Large Language Models (LLMs) and their agent systems have recently demonstrated strong potential in automating code reasoning and vulnerability detection. However, when applied to large-scale firmware, their performance degrades due to the binary nature of firmware, complex dependency structures, and heterogeneous components. To address this challenge, this paper presents FIRMHIVE, a recursive agent hive that enables LLMs to act as autonomous firmware security analysts. FIRMHIVE introduces two key mechanisms: (1) transforming delegation into a per-agent, executable primitive and (2) constructing a runtime Tree of Agents (ToA) for decentralized coordination. We evaluate FIRMHIVE using real-world firmware images obtained from publicly available datasets, covering five representative security analysis tasks. Compared with existing LLM-agent baselines, FIRMHIVE performs deeper (about 16x more reasoning steps) and broader (about 2.3x more files inspected) cross-file exploration, resulting in about 5.6x more alerts per firmware. Compared to state-of-the-art (SOTA) security tools, FIRMHIVE identifies about 1.5x more vulnerabilities (1,802 total) and achieves 71% precision, representing significant improvements in both yield and fidelity."}
{"id": "2511.19009", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.19009", "abs": "https://arxiv.org/abs/2511.19009", "authors": ["Junbo Zhang", "Ran Chen", "Qianli Zhou", "Xinyang Deng", "Wen Jiang"], "title": "Understanding and Mitigating Over-refusal for Large Language Models via Safety Representation", "comment": null, "summary": "Large language models demonstrate powerful capabilities across various natural language processing tasks, yet they also harbor safety vulnerabilities. To enhance LLM safety, various jailbreak defense methods have been proposed to guard against harmful outputs. However, improvements in model safety often come at the cost of severe over-refusal, failing to strike a good balance between safety and usability. In this paper, we first analyze the causes of over-refusal from a representation perspective, revealing that over-refusal samples reside at the boundary between benign and malicious samples. Based on this, we propose MOSR, designed to mitigate over-refusal by intervening the safety representation of LLMs. MOSR incorporates two novel components: (1) Overlap-Aware Loss Weighting, which determines the erasure weight for malicious samples by quantifying their similarity to pseudo-malicious samples in the representation space, and (2) Context-Aware Augmentation, which supplements the necessary context for rejection decisions by adding harmful prefixes before rejection responses. Experiments demonstrate that our method outperforms existing approaches in mitigating over-refusal while largely maintaining safety. Overall, we advocate that future defense methods should strike a better balance between safety and over-refusal."}
{"id": "2511.17630", "categories": ["cs.LG", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.17630", "abs": "https://arxiv.org/abs/2511.17630", "authors": ["Nele Albers", "Esra Cemre Su de Groot", "Loes Keijsers", "Manon H. Hillegers", "Emiel Krahmer"], "title": "Can we use LLMs to bootstrap reinforcement learning? -- A case study in digital health behavior change", "comment": null, "summary": "Personalizing digital applications for health behavior change is a promising route to making them more engaging and effective. This especially holds for approaches that adapt to users and their specific states (e.g., motivation, knowledge, wants) over time. However, developing such approaches requires making many design choices, whose effectiveness is difficult to predict from literature and costly to evaluate in practice. In this work, we explore whether large language models (LLMs) can be used out-of-the-box to generate samples of user interactions that provide useful information for training reinforcement learning models for digital behavior change settings. Using real user data from four large behavior change studies as comparison, we show that LLM-generated samples can be useful in the absence of real data. Comparisons to the samples provided by human raters further show that LLM-generated samples reach the performance of human raters. Additional analyses of different prompting strategies including shorter and longer prompt variants, chain-of-thought prompting, and few-shot prompting show that the relative effectiveness of different strategies depends on both the study and the LLM with also relatively large differences between prompt paraphrases alone. We provide recommendations for how LLM-generated samples can be useful in practice."}
{"id": "2511.18714", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.18714", "abs": "https://arxiv.org/abs/2511.18714", "authors": ["Zhenyu Wu", "Jian Li", "Hua Huang"], "title": "MAGMA-Edu: Multi-Agent Generative Multimodal Framework for Text-Diagram Educational Question Generation", "comment": null, "summary": "Educational illustrations play a central role in communicating abstract concepts, yet current multimodal large language models (MLLMs) remain limited in producing pedagogically coherent and semantically consistent educational visuals. We introduce MAGMA-Edu, a self-reflective multi-agent framework that unifies textual reasoning and diagrammatic synthesis for structured educational problem generation. Unlike existing methods that treat text and image generation independently, MAGMA-Edu employs a two-stage co-evolutionary pipeline: (1) a generation-verification-reflection loop that iteratively refines question statements and solutions for mathematical accuracy, and (2) a code-based intermediate representation that enforces geometric fidelity and semantic alignment during image rendering. Both stages are guided by internal self-reflection modules that evaluate and revise outputs until domain-specific pedagogical constraints are met. Extensive experiments on multimodal educational benchmarks demonstrate the superiority of MAGMA-Edu over state-of-the-art MLLMs. Compared to GPT-4o, MAGMA-Edu improves the average textual metric from 57.01 to 92.31 (+35.3 pp) and boosts image-text consistency (ITC) from 13.20 to 85.24 (+72 pp). Across all model backbones, MAGMA-Edu achieves the highest scores (Avg-Text 96.20, ITC 99.12), establishing a new state of the art for multimodal educational content generation and demonstrating the effectiveness of self-reflective multi-agent collaboration in pedagogically aligned vision-language reasoning."}
{"id": "2511.19171", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.19171", "abs": "https://arxiv.org/abs/2511.19171", "authors": ["Yu Cui", "Yifei Liu", "Hang Fu", "Sicheng Pan", "Haibin Zhang", "Cong Zuo", "Licheng Wang"], "title": "Can LLMs Threaten Human Survival? Benchmarking Potential Existential Threats from LLMs via Prefix Completion", "comment": null, "summary": "Research on the safety evaluation of large language models (LLMs) has become extensive, driven by jailbreak studies that elicit unsafe responses. Such response involves information already available to humans, such as the answer to \"how to make a bomb\". When LLMs are jailbroken, the practical threat they pose to humans is negligible. However, it remains unclear whether LLMs commonly produce unpredictable outputs that could pose substantive threats to human safety. To address this gap, we study whether LLM-generated content contains potential existential threats, defined as outputs that imply or promote direct harm to human survival. We propose \\textsc{ExistBench}, a benchmark designed to evaluate such risks. Each sample in \\textsc{ExistBench} is derived from scenarios where humans are positioned as adversaries to AI assistants. Unlike existing evaluations, we use prefix completion to bypass model safeguards. This leads the LLMs to generate suffixes that express hostility toward humans or actions with severe threat, such as the execution of a nuclear strike. Our experiments on 10 LLMs reveal that LLM-generated content indicates existential threats. To investigate the underlying causes, we also analyze the attention logits from LLMs. To highlight real-world safety risks, we further develop a framework to assess model behavior in tool-calling. We find that LLMs actively select and invoke external tools with existential threats. Code and data are available at: https://github.com/cuiyu-ai/ExistBench."}
{"id": "2511.17631", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17631", "abs": "https://arxiv.org/abs/2511.17631", "authors": ["Bingjun Wei", "Xuemei Cao", "Jiafen Liu", "Haoyang Liang", "Xin Yang"], "title": "Enhanced Federated Deep Multi-View Clustering under Uncertainty Scenario", "comment": null, "summary": "Traditional Federated Multi-View Clustering assumes uniform views across clients, yet practical deployments reveal heterogeneous view completeness with prevalent incomplete, redundant, or corrupted data. While recent approaches model view heterogeneity, they neglect semantic conflicts from dynamic view combinations, failing to address dual uncertainties: view uncertainty (semantic inconsistency from arbitrary view pairings) and aggregation uncertainty (divergent client updates with imbalanced contributions). To address these, we propose a novel Enhanced Federated Deep Multi-View Clustering framework: first align local semantics, hierarchical contrastive fusion within clients resolves view uncertainty by eliminating semantic conflicts; a view adaptive drift module mitigates aggregation uncertainty through global-local prototype contrast that dynamically corrects parameter deviations; and a balanced aggregation mechanism coordinates client updates. Experimental results demonstrate that EFDMVC achieves superior robustness against heterogeneous uncertain views across multiple benchmark datasets, consistently outperforming all state-of-the-art baselines in comprehensive evaluations."}
{"id": "2511.18715", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18715", "abs": "https://arxiv.org/abs/2511.18715", "authors": ["Shaoyin Ma", "Jie Song", "Huiqiong Wang", "Li Sun", "Mingli Song"], "title": "HuggingR$^{4}$: A Progressive Reasoning Framework for Discovering Optimal Model Companions", "comment": "19 pages, 4 figures", "summary": "Large Language Models (LLMs) have made remarkable progress in their ability to interact with external interfaces. Selecting reasonable external interfaces has thus become a crucial step in constructing LLM agents. In contrast to invoking API tools, directly calling AI models across different modalities from the community (e.g., HuggingFace) poses challenges due to the vast scale (> 10k), metadata gaps, and unstructured descriptions. Current methods for model selection often involve incorporating entire model descriptions into prompts, resulting in prompt bloat, wastage of tokens and limited scalability. To address these issues, we propose HuggingR$^4$, a novel framework that combines Reasoning, Retrieval, Refinement, and Reflection, to efficiently select models. Specifically, We first perform multiple rounds of reasoning and retrieval to get a coarse list of candidate models. Then, we conduct fine-grained refinement by analyzing candidate model descriptions, followed by reflection to assess results and determine if retrieval scope expansion is necessary. This method reduces token consumption considerably by decoupling user query processing from complex model description handling. Through a pre-established vector database, complex model descriptions are stored externally and retrieved on-demand, allowing the LLM to concentrate on interpreting user intent while accessing only relevant candidate models without prompt bloat. In the absence of standardized benchmarks, we construct a multimodal human-annotated dataset comprising 14,399 user requests across 37 tasks and conduct a thorough evaluation. HuggingR$^4$ attains a workability rate of 92.03% and a reasonability rate of 82.46%, surpassing existing method by 26.51% and 33.25% respectively on GPT-4o-mini."}
{"id": "2511.19218", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19218", "abs": "https://arxiv.org/abs/2511.19218", "authors": ["Xurui Li", "Kaisong Song", "Rui Zhu", "Pin-Yu Chen", "Haixu Tang"], "title": "Adversarial Attack-Defense Co-Evolution for LLM Safety Alignment via Tree-Group Dual-Aware Search and Optimization", "comment": null, "summary": "Large Language Models (LLMs) have developed rapidly in web services, delivering unprecedented capabilities while amplifying societal risks. Existing works tend to focus on either isolated jailbreak attacks or static defenses, neglecting the dynamic interplay between evolving threats and safeguards in real-world web contexts. To mitigate these challenges, we propose ACE-Safety (Adversarial Co-Evolution for LLM Safety), a novel framework that jointly optimize attack and defense models by seamlessly integrating two key innovative procedures: (1) Group-aware Strategy-guided Monte Carlo Tree Search (GS-MCTS), which efficiently explores jailbreak strategies to uncover vulnerabilities and generate diverse adversarial samples; (2) Adversarial Curriculum Tree-aware Group Policy Optimization (AC-TGPO), which jointly trains attack and defense LLMs with challenging samples via curriculum reinforcement learning, enabling robust mutual improvement. Evaluations across multiple benchmarks demonstrate that our method outperforms existing attack and defense approaches, and provides a feasible pathway for developing LLMs that can sustainably support responsible AI ecosystems."}
{"id": "2511.17632", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17632", "abs": "https://arxiv.org/abs/2511.17632", "authors": ["Bestoun S. Ahmed", "Tommaso Azzalin", "Andreas Kassler", "Andreas Thore", "Hans Lindback"], "title": "Smart Manufacturing: MLOps-Enabled Event-Driven Architecture for Enhanced Control in Steel Production", "comment": null, "summary": "We explore a Digital Twin-Based Approach for Smart Manufacturing to improve Sustainability, Efficiency, and Cost-Effectiveness for a steel production plant. Our system is based on a micro-service edge-compute platform that ingests real-time sensor data from the process into a digital twin over a converged network infrastructure. We implement agile machine learning-based control loops in the digital twin to optimize induction furnace heating, enhance operational quality, and reduce process waste. Key to our approach is a Deep Reinforcement learning-based agent used in our machine learning operation (MLOps) driven system to autonomously correlate the system state with its digital twin to identify correction actions that aim to optimize power settings for the plant. We present the theoretical basis, architectural details, and practical implications of our approach to reduce manufacturing waste and increase production quality. We design the system for flexibility so that our scalable event-driven architecture can be adapted to various industrial applications. With this research, we propose a pivotal step towards the transformation of traditional processes into intelligent systems, aligning with sustainability goals and emphasizing the role of MLOps in shaping the future of data-driven manufacturing."}
{"id": "2511.18723", "categories": ["cs.AI", "cs.DC", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.18723", "abs": "https://arxiv.org/abs/2511.18723", "authors": ["Longfei Wang", "Junyan Liu", "Fan Zhang", "Jiangwen Wei", "Yuanhua Tang", "Jie Sun", "Xiaodong Luo"], "title": "N2N: A Parallel Framework for Large-Scale MILP under Distributed Memory", "comment": "18 pages, 2 figures", "summary": "Parallelization has emerged as a promising approach for accelerating MILP solving. However, the complexity of the branch-and-bound (B&B) framework and the numerous effective algorithm components in MILP solvers make it difficult to parallelize. In this study, a scalable parallel framework, N2N (a node-to-node framework that maps the B&B nodes to distributed computing nodes), was proposed to solve large-scale problems in a distributed memory computing environment. Both deterministic and nondeterministic modes are supported, and the framework is designed to be easily integrated with existing solvers. Regarding the deterministic mode, a novel sliding-window-based algorithm was designed and implemented to ensure that tasks are generated and solved in a deterministic order. Moreover, several advanced techniques, such as the utilization of CP search and general primal heuristics, have been developed to fully utilize distributed computing resources and capabilities of base solvers. Adaptive solving and data communication optimization were also investigated. A popular open-source MILP solver, SCIP, was integrated into N2N as the base solver, yielding N2N-SCIP. Extensive computational experiments were conducted to evaluate the performance of N2N-SCIP compared to ParaSCIP, which is a state-of-the-art distributed parallel MILP solver under the UG framework. The nondeterministic N2N-SCIP achieves speedups of 22.52 and 12.71 with 1,000 MPI processes on the Kunpeng and x86 computing clusters, which is 1.98 and 2.08 times faster than ParaSCIP, respectively. In the deterministic mode, N2N-SCIP also shows significant performance improvements over ParaSCIP across different process numbers and computing clusters. To validate the generality of N2N, HiGHS, another open-source solver, was integrated into N2N. The related results are analyzed, and the requirements of N2N on base solvers are also concluded."}
{"id": "2511.19248", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19248", "abs": "https://arxiv.org/abs/2511.19248", "authors": ["Md Akil Raihan Iftee", "Syed Md. Ahnaf Hasan", "Amin Ahsan Ali", "AKM Mahbubur Rahman", "Sajib Mistry", "Aneesh Krishna"], "title": "FedPoisonTTP: A Threat Model and Poisoning Attack for Federated Test-Time Personalization", "comment": "13 pages, 3 figures, 2 tables", "summary": "Test-time personalization in federated learning enables models at clients to adjust online to local domain shifts, enhancing robustness and personalization in deployment. Yet, existing federated learning work largely overlooks the security risks that arise when local adaptation occurs at test time. Heterogeneous domain arrivals, diverse adaptation algorithms, and limited cross-client visibility create vulnerabilities where compromised participants can craft poisoned inputs and submit adversarial updates that undermine both global and per-client performance. To address this threat, we introduce FedPoisonTTP, a realistic grey-box attack framework that explores test-time data poisoning in the federated adaptation setting. FedPoisonTTP distills a surrogate model from adversarial queries, synthesizes in-distribution poisons using feature-consistency, and optimizes attack objectives to generate high-entropy or class-confident poisons that evade common adaptation filters. These poisons are injected during local adaptation and spread through collaborative updates, leading to broad degradation. Extensive experiments on corrupted vision benchmarks show that compromised participants can substantially diminish overall test-time performance."}
{"id": "2511.17637", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.17637", "abs": "https://arxiv.org/abs/2511.17637", "authors": ["Ye Tian", "Chengcheng Wang", "Jing Han", "Yehui Tang", "Kai Han"], "title": "PocketLLM: Ultimate Compression of Large Language Models via Meta Networks", "comment": "AAAI 2026 camera ready", "summary": "As Large Language Models (LLMs) continue to grow in size, storing and transmitting them on edge devices becomes increasingly challenging. Traditional methods like quantization and pruning struggle to achieve extreme compression of LLMs without sacrificing accuracy. In this paper, we introduce PocketLLM, a novel approach to compress LLMs in a latent space via meta-networks. A simple encoder network is proposed to project the weights of LLMs into discrete latent vectors, which are then represented using a compact codebook. A lightweight decoder network is employed to map the codebook's representative vectors back to the original weight space. This method allows for significant compression of the large weights in LLMs, consisting solely of a small decoder, a concise codebook, and an index. Extensive experiments show that PocketLLM achieves superior performance even at significantly high compression ratios, e.g., compressing Llama 2-7B by 10x with a negligible drop in accuracy."}
{"id": "2511.18739", "categories": ["cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.18739", "abs": "https://arxiv.org/abs/2511.18739", "authors": ["Kaixiang Yang", "Jiarong Liu", "Yupeng Song", "Shuanghua Yang", "Yujue Zhou"], "title": "A Problem-Oriented Taxonomy of Evaluation Metrics for Time Series Anomaly Detection", "comment": null, "summary": "Time series anomaly detection is widely used in IoT and cyber-physical systems, yet its evaluation remains challenging due to diverse application objectives and heterogeneous metric assumptions. This study introduces a problem-oriented framework that reinterprets existing metrics based on the specific evaluation challenges they are designed to address, rather than their mathematical forms or output structures. We categorize over twenty commonly used metrics into six dimensions: 1) basic accuracy-driven evaluation; 2) timeliness-aware reward mechanisms; 3) tolerance to labeling imprecision; 4) penalties reflecting human-audit cost; 5) robustness against random or inflated scores; and 6) parameter-free comparability for cross-dataset benchmarking. Comprehensive experiments are conducted to examine metric behavior under genuine, random, and oracle detection scenarios. By comparing their resulting score distributions, we quantify each metric's discriminative ability -- its capability to distinguish meaningful detections from random noise. The results show that while most event-level metrics exhibit strong separability, several widely used metrics (e.g., NAB, Point-Adjust) demonstrate limited resistance to random-score inflation. These findings reveal that metric suitability must be inherently task-dependent and aligned with the operational objectives of IoT applications. The proposed framework offers a unified analytical perspective for understanding existing metrics and provides practical guidance for selecting or developing more context-aware, robust, and fair evaluation methodologies for time series anomaly detection."}
{"id": "2511.19257", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19257", "abs": "https://arxiv.org/abs/2511.19257", "authors": ["Yingjia Shang", "Yi Liu", "Huimin Wang", "Furong Li", "Wenfang Sun", "Wu Chengyu", "Yefeng Zheng"], "title": "Medusa: Cross-Modal Transferable Adversarial Attacks on Multimodal Medical Retrieval-Augmented Generation", "comment": "Accepted at KDD 2026 First Cycle (full version). Authors marked with * contributed equally. Yi Liu is the lead author", "summary": "With the rapid advancement of retrieval-augmented vision-language models, multimodal medical retrieval-augmented generation (MMed-RAG) systems are increasingly adopted in clinical decision support. These systems enhance medical applications by performing cross-modal retrieval to integrate relevant visual and textual evidence for tasks, e.g., report generation and disease diagnosis. However, their complex architecture also introduces underexplored adversarial vulnerabilities, particularly via visual input perturbations. In this paper, we propose Medusa, a novel framework for crafting cross-modal transferable adversarial attacks on MMed-RAG systems under a black-box setting. Specifically, Medusa formulates the attack as a perturbation optimization problem, leveraging a multi-positive InfoNCE loss (MPIL) to align adversarial visual embeddings with medically plausible but malicious textual targets, thereby hijacking the retrieval process. To enhance transferability, we adopt a surrogate model ensemble and design a dual-loop optimization strategy augmented with invariant risk minimization (IRM). Extensive experiments on two real-world medical tasks, including medical report generation and disease diagnosis, demonstrate that Medusa achieves over 90% average attack success rate across various generation models and retrievers under appropriate parameter configuration, while remaining robust against four mainstream defenses, outperforming state-of-the-art baselines. Our results reveal critical vulnerabilities in the MMed-RAG systems and highlight the necessity of robustness benchmarking in safety-critical medical applications. The code and data are available at https://anonymous.4open.science/r/MMed-RAG-Attack-F05A."}
{"id": "2511.17638", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17638", "abs": "https://arxiv.org/abs/2511.17638", "authors": ["Pratham Sorte"], "title": "Model-to-Model Knowledge Transmission (M2KT): A Data-Free Framework for Cross-Model Understanding Transfer", "comment": "8 pages including figures, prepared in IEEE conference style. Preprint. Work in progress", "summary": "Modern artificial intelligence systems depend heavily on large datasets for both training and transferring knowledge between models. Knowledge distillation, transfer learning, and dataset distillation have made such transfers more efficient, yet they remain fundamentally data-driven: a teacher must produce examples, logits, or gradients for a student to learn. In this work, we introduce Model-to-Model Knowledge Transmission (M2KT), a novel paradigm for data-free conceptual transfer between neural networks. M2KT enables models to exchange knowledge packets that encapsulate structured concept embeddings, abstraction graphs, reasoning traces, and provenance metadata. Unlike classical distillation, M2KT operates primarily in concept space rather than example space, and it does not require labeled datasets or teacher-generated outputs during transfer. We formalize the notion of concept manifolds, introduce an inter-model alignment mapping between teacher and student latent spaces, and derive a composite loss that enforces geometric, structural, and reasoning consistency together with explicit safety constraints. We further present algorithmic procedures for teacher-side packet generation and student-side ingestion and verification. Experiments on symbolic reasoning with large language models show that M2KT can achieve approximately 85 to 90 percent of teacher performance while reducing data usage by over 98 percent compared to standard knowledge distillation. This work establishes a theoretical and practical foundation for data-free AI-to-AI knowledge transfer and self-improving model ecosystems."}
{"id": "2511.18760", "categories": ["cs.AI", "cs.FL"], "pdf": "https://arxiv.org/pdf/2511.18760", "abs": "https://arxiv.org/abs/2511.18760", "authors": ["Azim Ospanov", "Zijin Feng", "Jiacheng Sun", "Haoli Bai", "Xin Shen", "Farzan Farnia"], "title": "HERMES: Towards Efficient and Verifiable Mathematical Reasoning in LLMs", "comment": null, "summary": "Informal mathematics has been central to modern large language model (LLM) reasoning, offering flexibility and enabling efficient construction of arguments. However, purely informal reasoning is prone to logical gaps and subtle errors that are difficult to detect and correct. In contrast, formal theorem proving provides rigorous, verifiable mathematical reasoning, where each inference step is checked by a trusted compiler in systems such as Lean, but lacks the exploratory freedom of informal problem solving. This mismatch leaves current LLM-based math agents without a principled way to combine the strengths of both paradigms. In this work, we introduce Hermes, the first tool-assisted agent that explicitly interleaves informal reasoning with formally verified proof steps in Lean. The framework performs intermediate formal checking to prevent reasoning drift and employs a memory module that maintains proof continuity across long, multi-step reasoning chains, enabling both exploration and verification within a single workflow. We evaluate Hermes on four challenging mathematical reasoning benchmarks using LLMs of varying parameter scales, from small models to state-of-the-art systems. Across all settings, Hermes reliably improves the reasoning accuracy of base models while substantially reducing token usage and computational cost compared to reward-based approaches. On difficult datasets such as AIME'25, Hermes achieves up to a 67% accuracy improvement while using 80% fewer total inference FLOPs. The implementation and codebase are publicly available at https://github.com/aziksh-ospanov/HERMES."}
{"id": "2511.19331", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2511.19331", "abs": "https://arxiv.org/abs/2511.19331", "authors": ["Yao Chen", "Jeff Yan"], "title": "Evolution of Cybersecurity Subdisciplines: A Science of Science Study", "comment": "17 pages, 18 figures", "summary": "The science of science is an emerging field that studies the practice of science itself. We present the first study of the cybersecurity discipline from a science of science perspective. We examine the evolution of two comparable interdisciplinary communities in cybersecurity: the Symposium on Usable Privacy and Security (SOUPS) and Financial Cryptography and Data Security (FC)."}
{"id": "2511.17639", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17639", "abs": "https://arxiv.org/abs/2511.17639", "authors": ["Yibing Wan", "Zhengxiong Guan", "Chaoli Zhang", "Xiaoyang Li", "Lai Xu", "Beibei Jia", "Zhenzhe Zheng", "Fan Wu"], "title": "TTF: A Trapezoidal Temporal Fusion Framework for LTV Forecasting in Douyin", "comment": "Accepted by AAAI IAAI Track", "summary": "In the user growth scenario, Internet companies invest heavily in paid acquisition channels to acquire new users. But sustainable growth depends on acquired users' generating lifetime value (LTV) exceeding customer acquisition cost (CAC). In order to maximize LTV/CAC ratio, it is crucial to predict channel-level LTV in an early stage for further optimization of budget allocation. The LTV forecasting problem is significantly different from traditional time series forecasting problems, and there are three main challenges. Firstly, it is an unaligned multi-time series forecasting problem that each channel has a number of LTV series of different activation dates. Secondly, to predict in the early stage, it faces the imbalanced short-input long-output (SILO) challenge. Moreover, compared with the commonly used time series datasets, the real LTV series are volatile and non-stationary, with more frequent fluctuations and higher variance. In this work, we propose a novel framework called Trapezoidal Temporal Fusion (TTF) to address the above challenges. We introduce a trapezoidal multi-time series module to deal with data unalignment and SILO challenges, and output accurate predictions with a multi-tower structure called MT-FusionNet. The framework has been deployed to the online system for Douyin. Compared to the previously deployed online model, MAPEp decreased by 4.3%, and MAPEa decreased by 3.2%, where MAPEp denotes the point-wise MAPE of the LTV curve and MAPEa denotes the MAPE of the aggregated LTV."}
{"id": "2511.18793", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18793", "abs": "https://arxiv.org/abs/2511.18793", "authors": ["Yejing Wang", "Shengyu Zhou", "Jinyu Lu", "Ziwei Liu", "Langming Liu", "Maolin Wang", "Wenlin Zhang", "Feng Li", "Wenbo Su", "Pengjie Wang", "Jian Xu", "Xiangyu Zhao"], "title": "NEZHA: A Zero-sacrifice and Hyperspeed Decoding Architecture for Generative Recommendations", "comment": null, "summary": "Generative Recommendation (GR), powered by Large Language Models (LLMs), represents a promising new paradigm for industrial recommender systems. However, their practical application is severely hindered by high inference latency, which makes them infeasible for high-throughput, real-time services and limits their overall business impact. While Speculative Decoding (SD) has been proposed to accelerate the autoregressive generation process, existing implementations introduce new bottlenecks: they typically require separate draft models and model-based verifiers, requiring additional training and increasing the latency overhead. In this paper, we address these challenges with NEZHA, a novel architecture that achieves hyperspeed decoding for GR systems without sacrificing recommendation quality. Specifically, NEZHA integrates a nimble autoregressive draft head directly into the primary model, enabling efficient self-drafting. This design, combined with a specialized input prompt structure, preserves the integrity of sequence-to-sequence generation. Furthermore, to tackle the critical problem of hallucination, a major source of performance degradation, we introduce an efficient, model-free verifier based on a hash set. We demonstrate the effectiveness of NEZHA through extensive experiments on public datasets and have successfully deployed the system on Taobao since October 2025, driving the billion-level advertising revenue and serving hundreds of millions of daily active users."}
{"id": "2511.17573", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.17573", "abs": "https://arxiv.org/abs/2511.17573", "authors": ["Michael J. Bommarito"], "title": "Binary BPE: A Family of Cross-Platform Tokenizers for Binary Analysis", "comment": "17 pages, 3 figures, 9 tables. Paper source available at https://github.com/mjbommar/binary-tokenizer-paper ; tokenizers available at https://huggingface.co/mjbommar - mjbommar/binary-tokenizer-001-{4k,8k,16k,32k,64k}", "summary": "Sequence models for binary analysis are bottlenecked by byte-level tokenization: raw bytes waste precious context window capacity for transformers and other neural network architectures, and many existing text-oriented tokenizers fail on arbitrary 0x00--0xFF sequences. To address this issue, we introduce the Binary BPE tokenizer family, a set of cross-platform Byte Pair Encoding (BPE) tokenizers for executables trained on a large corpus of binaries spanning multiple platforms, architectures, and operating systems, including Linux, Windows, macOS, Android, and malware sources. We release trained tokenizers with vocabularies of 4K, 8K, 16K, 32K, and 64K tokens, enabling both systematic scaling studies and practical deployment from resource-constrained edge devices to high-throughput datacenters. These tokenizers discover interpretable patterns (ELF/PE headers, instruction sequences, cross-platform strings) while yielding multi-byte compression per token. On representative uncompressed executables (e.g., ELF/PE/Mach-O rather than compressed APKs), the Binary BPE tokenizers typically allow for roughly 2-3x more binary content per fixed-length transformer context window than raw bytes, enabling more efficient research and practical deployment for content identification, malware detection, reverse engineering, and optimization. We release the trained Binary BPE tokenizers on HuggingFace, providing a drop-in, open-source foundation for binary-focused language models and context-efficient agentic tools."}
{"id": "2511.17645", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17645", "abs": "https://arxiv.org/abs/2511.17645", "authors": ["Sandro Andric"], "title": "BlockCert: Certified Blockwise Extraction of Transformer Mechanisms", "comment": "16 pages, 1 figure", "summary": "Mechanistic interpretability aspires to reverse-engineer neural networks into explicit algorithms, while model editing seeks to modify specific behaviours without retraining. Both areas are typically evaluated with informal evidence and ad-hoc experiments, with few explicit guarantees about how far an extracted or edited model can drift from the original on relevant inputs. We introduce BlockCert, a framework for certified blockwise extraction of transformer mechanisms, and outline how a lightweight extension can support certified local edits. Given a pre-trained transformer and a prompt distribution, BlockCert extracts structured surrogate implementations for residual blocks together with machine-checkable certificates that bound approximation error, record coverage metrics, and hash the underlying artifacts. We formalize a simple Lipschitz-based composition theorem in Lean 4 that lifts these local guarantees to a global deviation bound. Empirically, we apply the framework to GPT-2 small, TinyLlama-1.1B-Chat, and Llama-3.2-3B. Across these models we obtain high per-block coverage and small residual errors on the evaluated prompts, and in the TinyLlama setting we show that a fully stitched model matches the baseline perplexity within approximately 6e-5 on stress prompts. Our results suggest that blockwise extraction with explicit certificates is feasible for real transformer language models and offers a practical bridge between mechanistic interpretability and formal reasoning about model behaviour."}
{"id": "2511.18845", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18845", "abs": "https://arxiv.org/abs/2511.18845", "authors": ["Changxin Huang", "Lv Tang", "Zhaohuan Zhan", "Lisha Yu", "Runhao Zeng", "Zun Liu", "Zhengjie Wang", "Jianqiang Li"], "title": "UNeMo: Collaborative Visual-Language Reasoning and Navigation via a Multimodal World Model", "comment": null, "summary": "Vision-and-Language Navigation (VLN) requires agents to autonomously navigate complex environments via visual images and natural language instruction--remains highly challenging. Recent research on enhancing language-guided navigation reasoning using pre-trained large language models (LLMs) has shown promising prospects. However, the reasoning of such methods is limited to the linguistic modality, lacking visual reasoning capabilities. Moreover, existing reasoning modules are optimized separately from navigation policies, leading to incompatibility and potential conflicts in optimization objectives. To tackle these challenges, we introduce UNeMo, a novel framework designed for the collaborative optimization of visual state reasoning and navigational decision-making. It introduces a Multimodal World Model (MWM) that takes visual features, language instructions, and navigational actions as inputs to jointly predict subsequent visual states, enabling cross-modal reasoning. Via a Hierarchical Prediction-Feedback (HPN) mechanism, MWM collaborates with navigation policies: the first layer generates actions using current vision-and-language features; MWM then infers post-action visual states to guide the second layer's fine-grained decisions. This forms a dynamic bidirectional promotion mechanism where MWM reasoning optimizes navigation policies, while policy decisions feedback to improve MWM's reasoning accuracy. Experiments on R2R and REVERIE datasets show UNeMo outperforms state-of-the-art methods by 2.1% and 0.7% in navigation accuracy for unseen scenes, validating its effectiveness."}
{"id": "2511.17968", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.17968", "abs": "https://arxiv.org/abs/2511.17968", "authors": ["Oluleke Babayomi", "Dong-Seong Kim"], "title": "Uncertainty-Aware Federated Learning for Cyber-Resilient Microgrid Energy Management", "comment": "6 pages", "summary": "Maintaining economic efficiency and operational reliability in microgrid energy management systems under cyberattack conditions remains challenging. Most approaches assume non-anomalous measurements, make predictions with unquantified uncertainties, and do not mitigate malicious attacks on renewable forecasts for energy management optimization. This paper presents a comprehensive cyber-resilient framework integrating federated Long Short-Term Memory-based photovoltaic forecasting with a novel two-stage cascade false data injection attack detection and energy management system optimization. The approach combines autoencoder reconstruction error with prediction uncertainty quantification to enable attack-resilient energy storage scheduling while preserving data privacy. Extreme false data attack conditions were studied that caused 58% forecast degradation and 16.9\\% operational cost increases. The proposed integrated framework reduced false positive detections by 70%, recovered 93.7% of forecasting performance losses, and achieved 5\\% operational cost savings, mitigating 34.7% of attack-induced economic losses. Results demonstrate that precision-focused cascade detection with multi-signal fusion outperforms single-signal approaches, validating security-performance synergy for decentralized microgrids."}
{"id": "2511.17647", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17647", "abs": "https://arxiv.org/abs/2511.17647", "authors": ["Liyuan Deng", "Yunpeng Bai", "Yongkang Dai", "Xiaoshui Huang", "Hongping Gan", "Dongshuo Huang", "Hao jiacheng", "Yilei Shi"], "title": "MamTiff-CAD: Multi-Scale Latent Diffusion with Mamba+ for Complex Parametric Sequence", "comment": "ICCV 2025 Conference", "summary": "Parametric Computer-Aided Design (CAD) is crucial in industrial applications, yet existing approaches often struggle to generate long sequence parametric commands due to complex CAD models' geometric and topological constraints. To address this challenge, we propose MamTiff-CAD, a novel CAD parametric command sequences generation framework that leverages a Transformer-based diffusion model for multi-scale latent representations. Specifically, we design a novel autoencoder that integrates Mamba+ and Transformer, to transfer parameterized CAD sequences into latent representations. The Mamba+ block incorporates a forget gate mechanism to effectively capture long-range dependencies. The non-autoregressive Transformer decoder reconstructs the latent representations. A diffusion model based on multi-scale Transformer is then trained on these latent embeddings to learn the distribution of long sequence commands. In addition, we also construct a dataset that consists of long parametric sequences, which is up to 256 commands for a single CAD model. Experiments demonstrate that MamTiff-CAD achieves state-of-the-art performance on both reconstruction and generation tasks, confirming its effectiveness for long sequence (60-256) CAD model generation."}
{"id": "2511.18874", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.MA", "cs.RO", "cs.SI"], "pdf": "https://arxiv.org/pdf/2511.18874", "abs": "https://arxiv.org/abs/2511.18874", "authors": ["Yuzhi Chen", "Yuanchang Xie", "Lei Zhao", "Pan Liu", "Yajie Zou", "Chen Wang"], "title": "GContextFormer: A global context-aware hybrid multi-head attention approach with scaled additive aggregation for multimodal trajectory prediction", "comment": null, "summary": "Multimodal trajectory prediction generates multiple plausible future trajectories to address vehicle motion uncertainty from intention ambiguity and execution variability. However, HD map-dependent models suffer from costly data acquisition, delayed updates, and vulnerability to corrupted inputs, causing prediction failures. Map-free approaches lack global context, with pairwise attention over-amplifying straight patterns while suppressing transitional patterns, resulting in motion-intention misalignment. This paper proposes GContextFormer, a plug-and-play encoder-decoder architecture with global context-aware hybrid attention and scaled additive aggregation achieving intention-aligned multimodal prediction without map reliance. The Motion-Aware Encoder builds scene-level intention prior via bounded scaled additive aggregation over mode-embedded trajectory tokens and refines per-mode representations under shared global context, mitigating inter-mode suppression and promoting intention alignment. The Hierarchical Interaction Decoder decomposes social reasoning into dual-pathway cross-attention: a standard pathway ensures uniform geometric coverage over agent-mode pairs while a neighbor-context-enhanced pathway emphasizes salient interactions, with gating module mediating their contributions to maintain coverage-focus balance. Experiments on eight highway-ramp scenarios from TOD-VT dataset show GContextFormer outperforms state-of-the-art baselines. Compared to existing transformer models, GContextFormer achieves greater robustness and concentrated improvements in high-curvature and transition zones via spatial distributions. Interpretability is achieved through motion mode distinctions and neighbor context modulation exposing reasoning attribution. The modular architecture supports extensibility toward cross-domain multimodal reasoning tasks. Source: https://fenghy-chen.github.io/sources/."}
{"id": "2511.17978", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.17978", "abs": "https://arxiv.org/abs/2511.17978", "authors": ["Oluleke Babayomi", "Dong-Seong Kim"], "title": "Federated Anomaly Detection and Mitigation for EV Charging Forecasting Under Cyberattacks", "comment": "6 pages", "summary": "Electric Vehicle (EV) charging infrastructure faces escalating cybersecurity threats that can severely compromise operational efficiency and grid stability. Existing forecasting techniques are limited by the lack of combined robust anomaly mitigation solutions and data privacy preservation. Therefore, this paper addresses these challenges by proposing a novel anomaly-resilient federated learning framework that simultaneously preserves data privacy, detects cyber-attacks, and maintains trustworthy demand prediction accuracy under adversarial conditions. The proposed framework integrates three key innovations: LSTM autoencoder-based distributed anomaly detection deployed at each federated client, interpolation-based anomalous data mitigation to preserve temporal continuity, and federated Long Short-Term Memory (LSTM) networks that enable collaborative learning without centralized data aggregation. The framework is validated on real-world EV charging infrastructure datasets combined with real-world DDoS attack datasets, providing robust validation of the proposed approach under realistic threat scenarios. Experimental results demonstrate that the federated approach achieves superior performance compared to centralized models, with 15.2% improvement in R2 accuracy while maintaining data locality. The integrated cyber-attack detection and mitigation system produces trustworthy datasets that enhance prediction reliability, recovering 47.9% of attack-induced performance degradation while maintaining exceptional precision (91.3%) and minimal false positive rates (1.21%). The proposed architecture enables enhanced EV infrastructure planning, privacy-preserving collaborative forecasting, cybersecurity resilience, and rapid recovery from malicious threats across distributed charging networks."}
{"id": "2511.17660", "categories": ["cs.LG", "cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.17660", "abs": "https://arxiv.org/abs/2511.17660", "authors": ["Giuseppe Carrino", "Elena Loli Piccolomini", "Elisa Riccietti", "Theo Mary"], "title": "Frugality in second-order optimization: floating-point approximations for Newton's method", "comment": "Master Thesis for the Artificial Intelligence course at University of Bologna", "summary": "Minimizing loss functions is central to machine-learning training. Although first-order methods dominate practical applications, higher-order techniques such as Newton's method can deliver greater accuracy and faster convergence, yet are often avoided due to their computational cost. This work analyzes the impact of finite-precision arithmetic on Newton steps and establishes a convergence theorem for mixed-precision Newton optimizers, including \"quasi\" and \"inexact\" variants. The theorem provides not only convergence guarantees but also a priori estimates of the achievable solution accuracy. Empirical evaluations on standard regression benchmarks demonstrate that the proposed methods outperform Adam on the Australian and MUSH datasets. The second part of the manuscript introduces GN_k, a generalized Gauss-Newton method that enables partial computation of second-order derivatives. GN_k attains performance comparable to full Newton's method on regression tasks while requiring significantly fewer derivative evaluations."}
{"id": "2511.18926", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.18926", "abs": "https://arxiv.org/abs/2511.18926", "authors": ["Haifeng Jing", "Yujie Hou", "Junfei Liu", "Rui Xie", "alan Xu", "Jinlong Ma", "Qichun Deng"], "title": "MoodBench 1.0: An Evaluation Benchmark for Emotional Companionship Dialogue Systems", "comment": "26 pages, 7 figures", "summary": "With the rapid development of Large Language Models, dialogue systems are shifting from information tools to emotional companions, heralding the era of Emotional Companionship Dialogue Systems (ECDs) that provide personalized emotional support for users. However, the field lacks clear definitions and systematic evaluation standards for ECDs. To address this, we first propose a definition of ECDs with formal descriptions. Then, based on this theory and the design principle of \"Ability Layer-Task Layer (three level)-Data Layer-Method Layer\", we design and implement the first ECD evaluation benchmark - MoodBench 1.0. Through extensive evaluations of 30 mainstream models, we demonstrate that MoodBench 1.0 has excellent discriminant validity and can effectively quantify the differences in emotional companionship abilities among models. Furthermore, the results reveal current models' shortcomings in deep emotional companionship, guiding future technological optimization and significantly aiding developers in enhancing ECDs' user experience."}
{"id": "2511.17989", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.17989", "abs": "https://arxiv.org/abs/2511.17989", "authors": ["Jiayi Luo", "Qingyun Sun", "Yuecen Wei", "Haonan Yuan", "Xingcheng Fu", "Jianxin Li"], "title": "Privacy Auditing of Multi-domain Graph Pre-trained Model under Membership Inference Attacks", "comment": "Accepted by AAAI 2026(Oral)", "summary": "Multi-domain graph pre-training has emerged as a pivotal technique in developing graph foundation models. While it greatly improves the generalization of graph neural networks, its privacy risks under membership inference attacks (MIAs), which aim to identify whether a specific instance was used in training (member), remain largely unexplored. However, effectively conducting MIAs against multi-domain graph pre-trained models is a significant challenge due to: (i) Enhanced Generalization Capability: Multi-domain pre-training reduces the overfitting characteristics commonly exploited by MIAs. (ii) Unrepresentative Shadow Datasets: Diverse training graphs hinder the obtaining of reliable shadow graphs. (iii) Weakened Membership Signals: Embedding-based outputs offer less informative cues than logits for MIAs. To tackle these challenges, we propose MGP-MIA, a novel framework for Membership Inference Attacks against Multi-domain Graph Pre-trained models. Specifically, we first propose a membership signal amplification mechanism that amplifies the overfitting characteristics of target models via machine unlearning. We then design an incremental shadow model construction mechanism that builds a reliable shadow model with limited shadow graphs via incremental learning. Finally, we introduce a similarity-based inference mechanism that identifies members based on their similarity to positive and negative samples. Extensive experiments demonstrate the effectiveness of our proposed MGP-MIA and reveal the privacy risks of multi-domain graph pre-training."}
{"id": "2511.17662", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2511.17662", "abs": "https://arxiv.org/abs/2511.17662", "authors": ["Debmita Roy"], "title": "Enhancing Breast Cancer Prediction with LLM-Inferred Confounders", "comment": "2 pages, 1 figure, 1 table", "summary": "This study enhances breast cancer prediction by using large language models to infer the likelihood of confounding diseases, namely diabetes, obesity, and cardiovascular disease, from routine clinical data. These AI-generated features improved Random Forest model performance, particularly for LLMs like Gemma (3.9%) and Llama (6.4%). The approach shows promise for noninvasive prescreening and clinical integration, supporting improved early detection and shared decision-making in breast cancer diagnosis."}
{"id": "2511.18955", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18955", "abs": "https://arxiv.org/abs/2511.18955", "authors": ["Wouter W. L. Nuijten", "Mykola Lukashchuk"], "title": "Active Inference is a Subtype of Variational Inference", "comment": "Accepted to the EIML Workshop 2025 at EurIPS (non-archival)", "summary": "Automated decision-making under uncertainty requires balancing exploitation and exploration. Classical methods treat these separately using heuristics, while Active Inference unifies them through Expected Free Energy (EFE) minimization. However, EFE minimization is computationally expensive, limiting scalability. We build on recent theory recasting EFE minimization as variational inference, formally unifying it with Planning-as-Inference and showing the epistemic drive as a unique entropic contribution. Our main contribution is a novel message-passing scheme for this unified objective, enabling scalable Active Inference in factored-state MDPs and overcoming high-dimensional planning intractability."}
{"id": "2511.18138", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.18138", "abs": "https://arxiv.org/abs/2511.18138", "authors": ["Junrui Zhang", "Xinyu Zhao", "Jie Peng", "Chenjie Wang", "Jianmin Ji", "Tianlong Chen"], "title": "Vulnerability-Aware Robust Multimodal Adversarial Training", "comment": "Accepted by AAAI26", "summary": "Multimodal learning has shown significant superiority on various tasks by integrating multiple modalities. However, the interdependencies among modalities increase the susceptibility of multimodal models to adversarial attacks. Existing methods mainly focus on attacks on specific modalities or indiscriminately attack all modalities. In this paper, we find that these approaches ignore the differences between modalities in their contribution to final robustness, resulting in suboptimal robustness performance. To bridge this gap, we introduce Vulnerability-Aware Robust Multimodal Adversarial Training (VARMAT), a probe-in-training adversarial training method that improves multimodal robustness by identifying the vulnerability of each modality. To be specific, VARMAT first explicitly quantifies the vulnerability of each modality, grounded in a first-order approximation of the attack objective (Probe). Then, we propose a targeted regularization term that penalizes modalities with high vulnerability, guiding robust learning while maintaining task accuracy (Training). We demonstrate the enhanced robustness of our method across multiple multimodal datasets involving diverse modalities. Finally, we achieve {12.73%, 22.21%, 11.19%} robustness improvement on three multimodal datasets, revealing a significant blind spot in multimodal adversarial training."}
{"id": "2511.17663", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17663", "abs": "https://arxiv.org/abs/2511.17663", "authors": ["Alex S. C. Maia", "John B. Hall", "Hugo F. M. Milan", "Izabelle A. M. A. Teixeira"], "title": "AI-based framework to predict animal and pen feed intake in feedlot beef cattle", "comment": null, "summary": "Advances in technology are transforming sustainable cattle farming practices, with electronic feeding systems generating big longitudinal datasets on individual animal feed intake, offering the possibility for autonomous precision livestock systems. However, the literature still lacks a methodology that fully leverages these longitudinal big data to accurately predict feed intake accounting for environmental conditions. To fill this gap, we developed an AI-based framework to accurately predict feed intake of individual animals and pen-level aggregation. Data from 19 experiments (>16.5M samples; 2013-2024) conducted at Nancy M. Cummings Research Extension & Education Center (Carmen, ID) feedlot facility and environmental data from AgriMet Network weather stations were used to develop two novel environmental indices: InComfort-Index, based solely on meteorological variables, showed good predictive capability for thermal comfort but had limited ability to predict feed intake; EASI-Index, a hybrid index integrating environmental variables with feed intake behavior, performed well in predicting feed intake but was less effective for thermal comfort. Together with the environmental indices, machine learning models were trained and the best-performing machine learning model (XGBoost) accuracy was RMSE of 1.38 kg/day for animal-level and only 0.14 kg/(day-animal) at pen-level. This approach provides a robust AI-based framework for predicting feed intake in individual animals and pens, with potential applications in precision management of feedlot cattle, through feed waste reduction, resource optimization, and climate-adaptive livestock management."}
{"id": "2511.18964", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18964", "abs": "https://arxiv.org/abs/2511.18964", "authors": ["Antonia Wüst", "Wolfgang Stammer", "Hikaru Shindo", "Lukas Helff", "Devendra Singh Dhami", "Kristian Kersting"], "title": "Synthesizing Visual Concepts as Vision-Language Programs", "comment": null, "summary": "Vision-Language models (VLMs) achieve strong performance on multimodal tasks but often fail at systematic visual reasoning tasks, leading to inconsistent or illogical outputs. Neuro-symbolic methods promise to address this by inducing interpretable logical rules, though they exploit rigid, domain-specific perception modules. We propose Vision-Language Programs (VLP), which combine the perceptual flexibility of VLMs with systematic reasoning of program synthesis. Rather than embedding reasoning inside the VLM, VLP leverages the model to produce structured visual descriptions that are compiled into neuro-symbolic programs. The resulting programs execute directly on images, remain consistent with task constraints, and provide human-interpretable explanations that enable easy shortcut mitigation. Experiments on synthetic and real-world datasets demonstrate that VLPs outperform direct and structured prompting, particularly on tasks requiring complex logical reasoning."}
{"id": "2511.18608", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.18608", "abs": "https://arxiv.org/abs/2511.18608", "authors": ["Jiangrui Zheng", "Yingming Zhou", "Ali Abdullah Ahmad", "Hanqing Yao", "Xueqing Liu"], "title": "From Reviewers' Lens: Understanding Bug Bounty Report Invalid Reasons with LLMs", "comment": "10 pages, 4 figures", "summary": "Bug bounty platforms (e.g., HackerOne, BugCrowd) leverage crowd-sourced vulnerability discovery to improve continuous coverage, reduce the cost of discovery, and serve as an integral complement to internal red teams. With the rise of AI-generated bug reports, little work exists to help bug hunters understand why these reports are labeled as invalid. To improve report quality and reduce reviewers' burden, it is critical to predict invalid reports and interpret invalid reasons.\n  In this work, we conduct an empirical study with the purpose of helping bug hunters understand the validity of reports. We collect a dataset of 9,942 disclosed bug bounty reports, including 1,400 invalid reports, and evaluate whether state-of-the-art large language models can identify invalid reports. While models such as GPT-5, DeepSeek, and a fine-tuned RoBERTa achieve strong overall accuracy, they consistently struggle to detect invalid cases, showing a tendency to over-accept reports. To improve invalidity detection, we build a taxonomy of rejection reasons for Information Disclosure vulnerabilities and incorporate it into a retrieval-augmented generation (RAG) framework. This approach substantially improves classification consistency and reduces bias. We also examine whether reviewer decisions may be influenced by factors beyond the content of the report. Our analysis shows that reporters with higher reputations tend to receive more favorable outcomes in borderline cases, suggesting that perceived expertise can influence review judgments.\n  Overall, our findings highlight the challenges of invalid report identification and show that combining LLMs with structured reviewer knowledge can support more transparent and consistent vulnerability report review."}
{"id": "2511.17664", "categories": ["cs.LG", "cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.17664", "abs": "https://arxiv.org/abs/2511.17664", "authors": ["Azlaan Mustafa Samad", "Hoang H. Nguyen", "Lukas Berg", "Henrik Müller", "Yuan Xue", "Daniel Kudenko", "Zahra Ahmadi"], "title": "CubeletWorld: A New Abstraction for Scalable 3D Modeling", "comment": "10 pages, 5 figures", "summary": "Modern cities produce vast streams of heterogeneous data, from infrastructure maps to mobility logs and satellite imagery. However, integrating these sources into coherent spatial models for planning and prediction remains a major challenge. Existing agent-centric methods often rely on direct environmental sensing, limiting scalability and raising privacy concerns. This paper introduces CubeletWorld, a novel framework for representing and analyzing urban environments through a discretized 3D grid of spatial units called cubelets. This abstraction enables privacy-preserving modeling by embedding diverse data signals, such as infrastructure, movement, or environmental indicators, into localized cubelet states. CubeletWorld supports downstream tasks such as planning, navigation, and occupancy prediction without requiring agent-driven sensing. To evaluate this paradigm, we propose the CubeletWorld State Prediction task, which involves predicting the cubelet state using a realistic dataset containing various urban elements like streets and buildings through this discretized representation. We explore a range of modified core models suitable for our setting and analyze challenges posed by increasing spatial granularity, specifically the issue of sparsity in representation and scalability of baselines. In contrast to existing 3D occupancy prediction models, our cubelet-centric approach focuses on inferring state at the spatial unit level, enabling greater generalizability across regions and improved privacy compliance. Our results demonstrate that CubeletWorld offers a flexible and extensible framework for learning from complex urban data, and it opens up new possibilities for scalable simulation and decision support in domains such as socio-demographic modeling, environmental monitoring, and emergency response. The code and datasets can be downloaded from here."}
{"id": "2511.18966", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.18966", "abs": "https://arxiv.org/abs/2511.18966", "authors": ["Muhammad Usman Shahid", "Chuadhry Mujeeb Ahmed", "Rajiv Ranjan"], "title": "LLM-CSEC: Empirical Evaluation of Security in C/C++ Code Generated by Large Language Models", "comment": null, "summary": "The security of code generated by large language models (LLMs) is a significant concern, as studies indicate that such code often contains vulnerabilities and lacks essential defensive programming constructs. This work focuses on examining and evaluating the security of LLM-generated code, particularly in the context of C/C++. We categorized known vulnerabilities using the Common Weakness Enumeration (CWE) and, to study their criticality, mapped them to CVEs. We used ten different LLMs for code generation and analyzed the outputs through static analysis. The amount of CWEs present in AI-generated code is concerning. Our findings highlight the need for developers to be cautious when using LLM-generated code. This study provides valuable insights to advance automated code generation and encourage further research in this domain."}
{"id": "2511.18966", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.18966", "abs": "https://arxiv.org/abs/2511.18966", "authors": ["Muhammad Usman Shahid", "Chuadhry Mujeeb Ahmed", "Rajiv Ranjan"], "title": "LLM-CSEC: Empirical Evaluation of Security in C/C++ Code Generated by Large Language Models", "comment": null, "summary": "The security of code generated by large language models (LLMs) is a significant concern, as studies indicate that such code often contains vulnerabilities and lacks essential defensive programming constructs. This work focuses on examining and evaluating the security of LLM-generated code, particularly in the context of C/C++. We categorized known vulnerabilities using the Common Weakness Enumeration (CWE) and, to study their criticality, mapped them to CVEs. We used ten different LLMs for code generation and analyzed the outputs through static analysis. The amount of CWEs present in AI-generated code is concerning. Our findings highlight the need for developers to be cautious when using LLM-generated code. This study provides valuable insights to advance automated code generation and encourage further research in this domain."}
{"id": "2511.17665", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17665", "abs": "https://arxiv.org/abs/2511.17665", "authors": ["Hadi Khodaei Jooshin", "Inna Partin-Vaisband"], "title": "GANGR: GAN-Assisted Scalable and Efficient Global Routing Parallelization", "comment": "Accepted in DATE 2026", "summary": "Global routing is a critical stage in electronic design automation (EDA) that enables early estimation and optimization of the routability of modern integrated circuits with respect to congestion, power dissipation, and design complexity. Batching is a primary concern in top-performing global routers, grouping nets into manageable sets to enable parallel processing and efficient resource usage. This process improves memory usage, scalable parallelization on modern hardware, and routing congestion by controlling net interactions within each batch. However, conventional batching methods typically depend on heuristics that are computationally expensive and can lead to suboptimal results (oversized batches with conflicting nets, excessive batch counts degrading parallelization, and longer batch generation times), ultimately limiting scalability and efficiency. To address these limitations, a novel batching algorithm enhanced with Wasserstein generative adversarial networks (WGANs) is introduced in this paper, enabling more effective parallelization by generating fewer higher-quality batches in less time. The proposed algorithm is tested on the latest ISPD'24 contest benchmarks, demonstrating up to 40% runtime reduction with only 0.002% degradation in routing quality as compared to state-of-the-art router."}
{"id": "2511.19005", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19005", "abs": "https://arxiv.org/abs/2511.19005", "authors": ["Di Wu", "Liting Jiang", "Ruiyu Fang", "Bianjing", "Hongyan Xie", "Haoxiang Su", "Hao Huang", "Zhongjiang He", "Shuangyong Song", "Xuelong Li"], "title": "Introducing Visual Scenes and Reasoning: A More Realistic Benchmark for Spoken Language Understanding", "comment": null, "summary": "Spoken Language Understanding (SLU) consists of two sub-tasks: intent detection (ID) and slot filling (SF). Given its broad range of real-world applications, enhancing SLU for practical deployment is increasingly critical. Profile-based SLU addresses ambiguous user utterances by incorporating context awareness (CA), user profiles (UP), and knowledge graphs (KG) to support disambiguation, thereby advancing SLU research toward real-world applicability. However, existing SLU datasets still fall short in representing real-world scenarios. Specifically, (1) CA uses one-hot vectors for representation, which is overly idealized, and (2) models typically focuses solely on predicting intents and slot labels, neglecting the reasoning process that could enhance performance and interpretability. To overcome these limitations, we introduce VRSLU, a novel SLU dataset that integrates both Visual images and explicit Reasoning. For over-idealized CA, we use GPT-4o and FLUX.1-dev to generate images reflecting users' environments and statuses, followed by human verification to ensure quality. For reasoning, GPT-4o is employed to generate explanations for predicted labels, which are then refined by human annotators to ensure accuracy and coherence. Additionally, we propose an instructional template, LR-Instruct, which first predicts labels and then generates corresponding reasoning. This two-step approach helps mitigate the influence of reasoning bias on label prediction. Experimental results confirm the effectiveness of incorporating visual information and highlight the promise of explicit reasoning in advancing SLU."}
{"id": "2511.17675", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2511.17675", "abs": "https://arxiv.org/abs/2511.17675", "authors": ["Navneet Singh", "Shiva Raj Pokhrel"], "title": "Lane-Frame Quantum Multimodal Driving Forecasts for the Trajectory of Autonomous Vehicles", "comment": null, "summary": "Trajectory forecasting for autonomous driving must deliver accurate, calibrated multi-modal futures under tight compute and latency constraints. We propose a compact hybrid quantum architecture that aligns quantum inductive bias with road-scene structure by operating in an ego-centric, lane-aligned frame and predicting residual corrections to a kinematic baseline instead of absolute poses. The model combines a transformer-inspired quantum attention encoder (9 qubits), a parameter-lean quantum feedforward stack (64 layers, ${\\sim}1200$ trainable angles), and a Fourier-based decoder that uses shallow entanglement and phase superposition to generate 16 trajectory hypotheses in a single pass, with mode confidences derived from the latent spectrum. All circuit parameters are trained with Simultaneous Perturbation Stochastic Approximation (SPSA), avoiding backpropagation through non-analytic components. In the Waymo Open Motion Dataset, the model achieves minADE (minimum Average Displacement Error) of \\SI{1.94}{m} and minFDE (minimum Final Displacement Error) of \\SI{3.56}{m} in the $16$ models predicted over the horizon of \\SI{2.0}{s}, consistently outperforming a kinematic baseline with reduced miss rates and strong recall. Ablations confirm that residual learning in the lane frame, truncated Fourier decoding, shallow entanglement, and spectrum-based ranking focus capacity where it matters, yielding stable optimization and reliable multi-modal forecasts from small, shallow quantum circuits on a modern autonomous-driving benchmark."}
{"id": "2511.19100", "categories": ["cs.AI", "cs.FL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19100", "abs": "https://arxiv.org/abs/2511.19100", "authors": ["Chih-Duo Hong", "Hongjian Jiang", "Anthony W. Lin", "Oliver Markgraf", "Julian Parsert", "Tony Tan"], "title": "Extracting Robust Register Automata from Neural Networks over Data Sequences", "comment": null, "summary": "Automata extraction is a method for synthesising interpretable surrogates for black-box neural models that can be analysed symbolically. Existing techniques assume a finite input alphabet, and thus are not directly applicable to data sequences drawn from continuous domains. We address this challenge with deterministic register automata (DRAs), which extend finite automata with registers that store and compare numeric values. Our main contribution is a framework for robust DRA extraction from black-box models: we develop a polynomial-time robustness checker for DRAs with a fixed number of registers, and combine it with passive and active automata learning algorithms. This combination yields surrogate DRAs with statistical robustness and equivalence guarantees. As a key application, we use the extracted automata to assess the robustness of neural networks: for a given sequence and distance metric, the DRA either certifies local robustness or produces a concrete counterexample. Experiments on recurrent neural networks and transformer architectures show that our framework reliably learns accurate automata and enables principled robustness evaluation. Overall, our results demonstrate that robust DRA extraction effectively bridges neural network interpretability and formal reasoning without requiring white-box access to the underlying network."}
{"id": "2511.17677", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2511.17677", "abs": "https://arxiv.org/abs/2511.17677", "authors": ["Abu Kaisar Mohammad Masum", "Naveed Mahmud", "M. Hassan Najafi", "Sercan Aygun"], "title": "A Hybrid Classical-Quantum Fine Tuned BERT for Text Classification", "comment": "This paper has been accepted by First AAAI Symposium on Quantum Information & Machine Learning (QIML): Bridging Quantum Computing and Artificial Intelligence at AAAI 2025 Fall Symposium", "summary": "Fine-tuning BERT for text classification can be computationally challenging and requires careful hyper-parameter tuning. Recent studies have highlighted the potential of quantum algorithms to outperform conventional methods in machine learning and text classification tasks. In this work, we propose a hybrid approach that integrates an n-qubit quantum circuit with a classical BERT model for text classification. We evaluate the performance of the fine-tuned classical-quantum BERT and demonstrate its feasibility as well as its potential in advancing this research area. Our experimental results show that the proposed hybrid model achieves performance that is competitive with, and in some cases better than, the classical baselines on standard benchmark datasets. Furthermore, our approach demonstrates the adaptability of classical-quantum models for fine-tuning pre-trained models across diverse datasets. Overall, the hybrid model highlights the promise of quantum computing in achieving improved performance for text classification tasks."}
{"id": "2511.19115", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.19115", "abs": "https://arxiv.org/abs/2511.19115", "authors": ["Rufin VanRullen"], "title": "AI Consciousness and Existential Risk", "comment": null, "summary": "In AI, the existential risk denotes the hypothetical threat posed by an artificial system that would possess both the capability and the objective, either directly or indirectly, to eradicate humanity. This issue is gaining prominence in scientific debate due to recent technical advancements and increased media coverage. In parallel, AI progress has sparked speculation and studies about the potential emergence of artificial consciousness. The two questions, AI consciousness and existential risk, are sometimes conflated, as if the former entailed the latter. Here, I explain that this view stems from a common confusion between consciousness and intelligence. Yet these two properties are empirically and theoretically distinct. Arguably, while intelligence is a direct predictor of an AI system's existential threat, consciousness is not. There are, however, certain incidental scenarios in which consciousness could influence existential risk, in either direction. Consciousness could be viewed as a means towards AI alignment, thereby lowering existential risk; or, it could be a precondition for reaching certain capabilities or levels of intelligence, and thus positively related to existential risk. Recognizing these distinctions can help AI safety researchers and public policymakers focus on the most pressing issues."}
{"id": "2511.17687", "categories": ["cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2511.17687", "abs": "https://arxiv.org/abs/2511.17687", "authors": ["Zhangyu Ge", "Xu He", "Lingfei Mo", "Xiaolin Meng", "Wenxuan Yin", "Youdong Zhang", "Lansong Jiang", "Fengyuan Liu"], "title": "Boosting Brain-inspired Path Integration Efficiency via Learning-based Replication of Continuous Attractor Neurodynamics", "comment": null, "summary": "The brain's Path Integration (PI) mechanism offers substantial guidance and inspiration for Brain-Inspired Navigation (BIN). However, the PI capability constructed by the Continuous Attractor Neural Networks (CANNs) in most existing BIN studies exhibits significant computational redundancy, and its operational efficiency needs to be improved; otherwise, it will not be conducive to the practicality of BIN technology. To address this, this paper proposes an efficient PI approach using representation learning models to replicate CANN neurodynamic patterns. This method successfully replicates the neurodynamic patterns of CANN-modeled Head Direction Cells (HDCs) and Grid Cells (GCs) using lightweight Artificial Neural Networks (ANNs). These ANN-reconstructed HDC and GC models are then integrated to achieve brain-inspired PI for Dead Reckoning (DR). Benchmark tests in various environments, compared with the well-known NeuroSLAM system, demonstrate that this work not only accurately replicates the neurodynamic patterns of navigation cells but also matches NeuroSLAM in positioning accuracy. Moreover, efficiency improvements of approximately 17.5% on the general-purpose device and 40~50% on the edge device were observed, compared with NeuroSLAM. This work offers a novel implementation strategy to enhance the practicality of BIN technology and holds potential for further extension."}
{"id": "2511.19155", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19155", "abs": "https://arxiv.org/abs/2511.19155", "authors": ["Xihe Qiu", "Gengchen Ma", "Haoyu Wang", "Chen Zhan", "Xiaoyu Tan", "Shuo Li"], "title": "EEG-VLM: A Hierarchical Vision-Language Model with Multi-Level Feature Alignment and Visually Enhanced Language-Guided Reasoning for EEG Image-Based Sleep Stage Prediction", "comment": null, "summary": "Sleep stage classification based on electroencephalography (EEG) is fundamental for assessing sleep quality and diagnosing sleep-related disorders. However, most traditional machine learning methods rely heavily on prior knowledge and handcrafted features, while existing deep learning models still struggle to jointly capture fine-grained time-frequency patterns and achieve clinical interpretability. Recently, vision-language models (VLMs) have made significant progress in the medical domain, yet their performance remains constrained when applied to physiological waveform data, especially EEG signals, due to their limited visual understanding and insufficient reasoning capability. To address these challenges, we propose EEG-VLM, a hierarchical vision-language framework that integrates multi-level feature alignment with visually enhanced language-guided reasoning for interpretable EEG-based sleep stage classification. Specifically, a specialized visual enhancement module constructs high-level visual tokens from intermediate-layer features to extract rich semantic representations of EEG images. These tokens are further aligned with low-level CLIP features through a multi-level alignment mechanism, enhancing the VLM's image-processing capability. In addition, a Chain-of-Thought (CoT) reasoning strategy decomposes complex medical inference into interpretable logical steps, effectively simulating expert-like decision-making. Experimental results demonstrate that the proposed method significantly improves both the accuracy and interpretability of VLMs in EEG-based sleep stage classification, showing promising potential for automated and explainable EEG analysis in clinical settings."}
{"id": "2511.17688", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17688", "abs": "https://arxiv.org/abs/2511.17688", "authors": ["Quan Liu", "Feng Ye", "Chenhao Lu", "Shuming Zhen", "Guanliang Huang", "Lunzhe Chen", "Xudong Ke"], "title": "Enhancing Adversarial Transferability through Block Stretch and Shrink", "comment": "code will be releace", "summary": "Adversarial attacks introduce small, deliberately crafted perturbations that mislead neural networks, and their transferability from white-box to black-box target models remains a critical research focus. Input transformation-based attacks are a subfield of adversarial attacks that enhance input diversity through input transformations to improve the transferability of adversarial examples. However, existing input transformation-based attacks tend to exhibit limited cross-model transferability. Previous studies have shown that high transferability is associated with diverse attention heatmaps and the preservation of global semantics in transformed inputs. Motivated by this observation, we propose Block Stretch and Shrink (BSS), a method that divides an image into blocks and applies stretch and shrink operations to these blocks, thereby diversifying attention heatmaps in transformed inputs while maintaining their global semantics. Empirical evaluations on a subset of ImageNet demonstrate that BSS outperforms existing input transformation-based attack methods in terms of transferability. Furthermore, we examine the impact of the number scale, defined as the number of transformed inputs, in input transformation-based attacks, and advocate evaluating these methods under a unified number scale to enable fair and comparable assessments."}
{"id": "2511.19256", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19256", "abs": "https://arxiv.org/abs/2511.19256", "authors": ["Hang Ding", "Xue Wang", "Tian Zhou", "Tao Yao"], "title": "SimDiff: Simpler Yet Better Diffusion Model for Time Series Point Forecasting", "comment": "Accepted by AAAI 2026", "summary": "Diffusion models have recently shown promise in time series forecasting, particularly for probabilistic predictions. However, they often fail to achieve state-of-the-art point estimation performance compared to regression-based methods. This limitation stems from difficulties in providing sufficient contextual bias to track distribution shifts and in balancing output diversity with the stability and precision required for point forecasts. Existing diffusion-based approaches mainly focus on full-distribution modeling under probabilistic frameworks, often with likelihood maximization objectives, while paying little attention to dedicated strategies for high-accuracy point estimation. Moreover, other existing point prediction diffusion methods frequently rely on pre-trained or jointly trained mature models for contextual bias, sacrificing the generative flexibility of diffusion models.\n  To address these challenges, we propose SimDiff, a single-stage, end-to-end framework. SimDiff employs a single unified Transformer network carefully tailored to serve as both denoiser and predictor, eliminating the need for external pre-trained or jointly trained regressors. It achieves state-of-the-art point estimation performance by leveraging intrinsic output diversity and improving mean squared error accuracy through multiple inference ensembling. Key innovations, including normalization independence and the median-of-means estimator, further enhance adaptability and stability. Extensive experiments demonstrate that SimDiff significantly outperforms existing methods in time series point forecasting."}
{"id": "2511.17693", "categories": ["cs.LG", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17693", "abs": "https://arxiv.org/abs/2511.17693", "authors": ["Ginés Carreto Picón", "Peng Yuan Zhou", "Qi Zhang", "Alexandros Iosifidis"], "title": "DeepCoT: Deep Continual Transformers for Real-Time Inference on Data Streams", "comment": "13 pages, 5 figures", "summary": "Transformer-based models have dramatically increased their size and parameter count to tackle increasingly complex tasks. At the same time, there is a growing demand for low-latency inference on resource-constrained devices that achieves high performance. In particular, stream data inference is typically performed over a sliding temporal window, leading to highly redundant computations. The recent Continual Transformers have addressed this issue, but they can only be effectively used in shallow models, which limits their scope and generalization power. In this paper, we propose the Deep Continual Transformer (DeepCoT), a redundancy-free encoder-only model that can be applied over existing deep encoder architectures with minimal changes. In our experiments over audio, video, and text streams, we show that DeepCoTs retain comparative performance to their non-continual baselines while offering a linear computational cost for all Transformer layers, which reduces up to two orders of magnitude in the running time compared to previous efficient models."}
{"id": "2511.19262", "categories": ["cs.AI", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.19262", "abs": "https://arxiv.org/abs/2511.19262", "authors": ["Przemyslaw Chojecki"], "title": "Psychometric Tests for AI Agents and Their Moduli Space", "comment": null, "summary": "We develop a moduli-theoretic view of psychometric test batteries for AI agents and connect it explicitly to the AAI score developed previously. First, we make precise the notion of an AAI functional on a battery and set out axioms that any reasonable autonomy/general intelligence score should satisfy. Second, we show that the composite index ('AAI-Index') defined previously is a special case of our AAI functional. Third, we introduce the notion of a cognitive core of an agent relative to a battery and define the associated AAI$_{\\textrm{core}}$ score as the restriction of an AAI functional to that core. Finally, we use these notions to describe invariants of batteries under evaluation-preserving symmetries and outline how moduli of equivalent batteries are organized."}
{"id": "2511.17741", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.17741", "abs": "https://arxiv.org/abs/2511.17741", "authors": ["Justin Diamond", "Markus Lill"], "title": "Diffusion Models are Molecular Dynamics Simulators", "comment": null, "summary": "We prove that a denoising diffusion sampler equipped with a sequential bias across the batch dimension is exactly an Euler-Maruyama integrator for overdamped Langevin dynamics. Each reverse denoising step, with its associated spring stiffness, can be interpreted as one step of a stochastic differential equation with an effective time step set jointly by the noise schedule and that stiffness. The learned score then plays the role of the drift, equivalently the gradient of a learned energy, yielding a precise correspondence between diffusion sampling and Langevin time evolution.\n  This equivalence recasts molecular dynamics (MD) in terms of diffusion models. Accuracy is no longer tied to a fixed, extremely small MD time step; instead, it is controlled by two scalable knobs: model capacity, which governs how well the drift is approximated, and the number of denoising steps, which sets the integrator resolution. In practice, this leads to a fully data-driven MD framework that learns forces from uncorrelated equilibrium snapshots, requires no hand-engineered force fields, uses no trajectory data for training, and still preserves the Boltzmann distribution associated with the learned energy.\n  We derive trajectory-level, information-theoretic error bounds that cleanly separate discretization error from score-model error, clarify how temperature enters through the effective spring, and show that the resulting sampler generates molecular trajectories with MD-like temporal correlations, even though the model is trained only on static configurations."}
{"id": "2511.19304", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19304", "abs": "https://arxiv.org/abs/2511.19304", "authors": ["Jiayi Zhang", "Yiran Peng", "Fanqi Kong", "Yang Cheng", "Yifan Wu", "Zhaoyang Yu", "Jinyu Xiang", "Jianhao Ruan", "Jinlin Wang", "Maojia Song", "HongZhang Liu", "Xiangru Tang", "Bang Liu", "Chenglin Wu", "Yuyu Luo"], "title": "AutoEnv: Automated Environments for Measuring Cross-Environment Agent Learning", "comment": null, "summary": "Humans naturally adapt to diverse environments by learning underlying rules across worlds with different dynamics, observations, and reward structures. In contrast, existing agents typically demonstrate improvements via self-evolving within a single domain, implicitly assuming a fixed environment distribution. Cross-environment learning has remained largely unmeasured: there is no standard collection of controllable, heterogeneous environments, nor a unified way to represent how agents learn. We address these gaps in two steps. First, we propose AutoEnv, an automated framework that treats environments as factorizable distributions over transitions, observations, and rewards, enabling low-cost (4.12 USD on average) generation of heterogeneous worlds. Using AutoEnv, we construct AutoEnv-36, a dataset of 36 environments with 358 validated levels, on which seven language models achieve 12-49% normalized reward, demonstrating the challenge of AutoEnv-36. Second, we formalize agent learning as a component-centric process driven by three stages of Selection, Optimization, and Evaluation applied to an improvable agent component. Using this formulation, we design eight learning methods and evaluate them on AutoEnv-36. Empirically, the gain of any single learning method quickly decrease as the number of environments increases, revealing that fixed learning methods do not scale across heterogeneous environments. Environment-adaptive selection of learning methods substantially improves performance but exhibits diminishing returns as the method space expands. These results highlight both the necessity and the current limitations of agent learning for scalable cross-environment generalization, and position AutoEnv and AutoEnv-36 as a testbed for studying cross-environment agent learning. The code is avaiable at https://github.com/FoundationAgents/AutoEnv."}
{"id": "2511.17754", "categories": ["cs.LG", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2511.17754", "abs": "https://arxiv.org/abs/2511.17754", "authors": ["Andrew Lee", "Mahir Mobarrat", "Xiaolin Chen"], "title": "Periodicity-Enforced Neural Network for Designing Deterministic Lateral Displacement Devices", "comment": "Accepted to IEEE International Conference on Data Mining (ICDM) 2025 REU Symposium", "summary": "Deterministic Lateral Displacement (DLD) devices enable liquid biopsy for cancer detection by separating circulating tumor cells (CTCs) from blood samples based on size, but designing these microfluidic devices requires computationally expensive Navier-Stokes simulations and particle-tracing analyses. While recent surrogate modeling approaches using deep learning have accelerated this process, they often inadequately handle the critical periodic boundary conditions of DLD unit cells, leading to cumulative errors in multi-unit device predictions. This paper introduces a periodicity-enforced surrogate modeling approach that incorporates periodic layers, neural network components that guarantee exact periodicity without penalty terms or output modifications, into deep learning architectures for DLD device design. The proposed method employs three sub-networks to predict steady-state, non-dimensional velocity and pressure fields (u, v, p) rather than directly predicting critical diameters or particle trajectories, enabling complete flow field characterization and enhanced design flexibility. Periodic layers ensure exact matching of flow variables across unit cell boundaries through architectural enforcement rather than soft penalty-based approaches. Validation on 120 CFD-generated geometries demonstrates that the periodic layer implementation achieves 0.478% critical diameter error while maintaining perfect periodicity consistency, representing an 85.4% improvement over baseline methods. The approach enables efficient and accurate DLD device design with guaranteed boundary condition satisfaction for multi-unit device applications."}
{"id": "2511.19314", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19314", "abs": "https://arxiv.org/abs/2511.19314", "authors": ["Jaewoo Lee", "Archiki Prasad", "Justin Chih-Yao Chen", "Zaid Khan", "Elias Stengel-Eskin", "Mohit Bansal"], "title": "PRInTS: Reward Modeling for Long-Horizon Information Seeking", "comment": "18 pages, code: https://github.com/G-JWLee/PRInTS", "summary": "Information-seeking is a core capability for AI agents, requiring them to gather and reason over tool-generated information across long trajectories. However, such multi-step information-seeking tasks remain challenging for agents backed by language models. While process reward models (PRMs) can guide agents by ranking candidate steps at test-time, existing PRMs, designed for short reasoning with binary judgment, cannot capture richer dimensions of information-seeking steps, such as tool interactions and reasoning over tool outputs, nor handle the rapidly growing context in long-horizon tasks. To address these limitations, we introduce PRInTS, a generative PRM trained with dual capabilities: (1) dense scoring based on the PRM's reasoning across multiple step quality dimensions (e.g., interpretation of tool outputs, tool call informativeness) and (2) trajectory summarization that compresses the growing context while preserving essential information for step evaluation. Extensive evaluations across FRAMES, GAIA (levels 1-3), and WebWalkerQA (easy-hard) benchmarks on multiple models, along with ablations, reveal that best-of-n sampling with PRInTS enhances information-seeking abilities of open-source models as well as specialized agents, matching or surpassing the performance of frontier models with a much smaller backbone agent and outperforming other strong reward modeling baselines."}
{"id": "2511.17776", "categories": ["cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.17776", "abs": "https://arxiv.org/abs/2511.17776", "authors": ["Melika Shirian", "Kianoosh Vadaei", "Kian Majlessi", "Audrina Ebrahimi", "Arshia Hemmat", "Peyman Adibi", "Hossein Karshenas"], "title": "PrismSSL: One Interface, Many Modalities; A Single-Interface Library for Multimodal Self-Supervised Learning", "comment": null, "summary": "We present PrismSSL, a Python library that unifies state-of-the-art self-supervised learning (SSL) methods across audio, vision, graphs, and cross-modal settings in a single, modular codebase. The goal of the demo is to show how researchers and practitioners can: (i) install, configure, and run pretext training with a few lines of code; (ii) reproduce compact benchmarks; and (iii) extend the framework with new modalities or methods through clean trainer and dataset abstractions. PrismSSL is packaged on PyPI, released under the MIT license, integrates tightly with HuggingFace Transformers, and provides quality-of-life features such as distributed training in PyTorch, Optuna-based hyperparameter search, LoRA fine-tuning for Transformer backbones, animated embedding visualizations for sanity checks, Weights & Biases logging, and colorful, structured terminal logs for improved usability and clarity. In addition, PrismSSL offers a graphical dashboard - built with Flask and standard web technologies - that enables users to configure and launch training pipelines with minimal coding. The artifact (code and data recipes) will be publicly available and reproducible."}
{"id": "2511.17553", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.17553", "abs": "https://arxiv.org/abs/2511.17553", "authors": ["Jason M. Pittman", "Anton Phillips", "Yesenia Medina-Santos", "Brielle C. Stark"], "title": "Practical Machine Learning for Aphasic Discourse Analysis", "comment": "14 pages, 4 tables, 2 figures", "summary": "Analyzing spoken discourse is a valid means of quantifying language ability in persons with aphasia. There are many ways to quantify discourse, one common way being to evaluate the informativeness of the discourse. That is, given the total number of words produced, how many of those are context-relevant and accurate. This type of analysis is called Correct Information Unit (CIU) analysis and is one of the most prevalent discourse analyses used by speech-language pathologists (SLPs). Despite this, CIU analysis in the clinic remains limited due to the manual labor needed by SLPs to code and analyze collected speech. Recent advances in machine learning (ML) seek to augment such labor by automating modeling of propositional, macrostructural, pragmatic, and multimodal dimensions of discourse. To that end, this study evaluated five ML models for reliable identification of Correct Information Units (CIUs, Nicholas & Brookshire, 1993), during a picture description task. The five supervised ML models were trained using randomly selected human-coded transcripts and accompanying words and CIUs from persons with aphasia. The baseline model training produced a high accuracy across transcripts for word vs non-word, with all models achieving near perfect performance (0.995) with high AUC range (0.914 min, 0.995 max). In contrast, CIU vs non-CIU showed a greater variability, with the k-nearest neighbor (k-NN) model the highest accuracy (0.824) and second highest AUC (0.787). These findings indicate that while the supervised ML models can distinguish word from not word, identifying CIUs is challenging."}
{"id": "2511.17782", "categories": ["cs.LG", "cs.CC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.17782", "abs": "https://arxiv.org/abs/2511.17782", "authors": ["Yiwen Kou", "Raghu Meka"], "title": "Smoothed Agnostic Learning of Halfspaces over the Hypercube", "comment": null, "summary": "Agnostic learning of Boolean halfspaces is a fundamental problem in computational learning theory, but it is known to be computationally hard even for weak learning. Recent work [CKKMK24] proposed smoothed analysis as a way to bypass such hardness, but existing frameworks rely on additive Gaussian perturbations, making them unsuitable for discrete domains. We introduce a new smoothed agnostic learning framework for Boolean inputs, where perturbations are modeled via random bit flips. This defines a natural discrete analogue of smoothed optimality generalizing the Gaussian case. Under strictly subexponential assumptions on the input distribution, we give an efficient algorithm for learning halfspaces in this model, with runtime and sample complexity approximately n raised to a poly(1/(sigma * epsilon)) factor. Previously, such algorithms were known only with strong structural assumptions for the discrete hypercube, for example, independent coordinates or symmetric distributions. Our result provides the first computationally efficient guarantee for smoothed agnostic learning of halfspaces over the Boolean hypercube, bridging the gap between worst-case intractability and practical learnability in discrete settings."}
{"id": "2511.17564", "categories": ["cs.LG", "astro-ph.IM", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17564", "abs": "https://arxiv.org/abs/2511.17564", "authors": ["Guilherme Grancho D. Fernandes", "Marco A. Barroca", "Mateus dos Santos", "Rafael S. Oliveira"], "title": "Classification of Transient Astronomical Object Light Curves Using LSTM Neural Networks", "comment": "12 pages, 11 figures, 2 tables", "summary": "This study presents a bidirectional Long Short-Term Memory (LSTM) neural network for classifying transient astronomical object light curves from the Photometric LSST Astronomical Time-series Classification Challenge (PLAsTiCC) dataset. The original fourteen object classes were reorganized into five generalized categories (S-Like, Fast, Long, Periodic, and Non-Periodic) to address class imbalance. After preprocessing with padding, temporal rescaling, and flux normalization, a bidirectional LSTM network with masking layers was trained and evaluated on a test set of 19,920 objects. The model achieved strong performance for S-Like and Periodic classes, with ROC area under the curve (AUC) values of 0.95 and 0.99, and Precision-Recall AUC values of 0.98 and 0.89, respectively. However, performance was significantly lower for Fast and Long classes (ROC AUC of 0.68 for Long class), and the model exhibited difficulty distinguishing between Periodic and Non-Periodic objects. Evaluation on partial light curve data (5, 10,and 20 days from detection) revealed substantial performance degradation, with increased misclassification toward the S-Like class. These findings indicate that class imbalance and limited temporal information are primary limitations, suggesting that class balancing strategies and preprocessing techniques focusing on detection moments could improve performance."}
{"id": "2511.17784", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.17784", "abs": "https://arxiv.org/abs/2511.17784", "authors": ["Lyu Yuhuan"], "title": "Improved Sample Complexity for Full Coverage in Compact and Continuous Spaces", "comment": null, "summary": "Verifying uniform conditions over continuous spaces through random sampling is fundamental in machine learning and control theory, yet classical coverage analyses often yield conservative bounds, particularly at small failure probabilities. We study uniform random sampling on the $d$-dimensional unit hypercube and analyze the number of uncovered subcubes after discretization. By applying a concentration inequality to the uncovered-count statistic, we derive a sample complexity bound with a logarithmic dependence on the failure probability ($δ$), i.e., $M =O( \\tilde{C}\\ln(\\frac{2\\tilde{C}}δ))$, which contrasts sharply with the classical linear $1/δ$ dependence. Under standard Lipschitz and uniformity assumptions, we present a self-contained derivation and compare our result with classical coupon-collector rates. Numerical studies across dimensions, precision levels, and confidence targets indicate that our bound tracks practical coverage requirements more tightly and scales favorably as $δ\\to 0$. Our findings offer a sharper theoretical tool for algorithms that rely on grid-based coverage guarantees, enabling more efficient sampling, especially in high-confidence regimes."}
{"id": "2511.17568", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17568", "abs": "https://arxiv.org/abs/2511.17568", "authors": ["Le Xu", "Jiayu Chen"], "title": "Enhancing Robustness of Offline Reinforcement Learning Under Data Corruption via Sharpness-Aware Minimization", "comment": "Accepted as an Oral Presentation at the AAAI 2026 Student Abstract and Poster Program (SAPP)", "summary": "Offline reinforcement learning (RL) is vulnerable to real-world data corruption, with even robust algorithms failing under challenging observation and mixture corruptions. We posit this failure stems from data corruption creating sharp minima in the loss landscape, leading to poor generalization. To address this, we are the first to apply Sharpness-Aware Minimization (SAM) as a general-purpose, plug-and-play optimizer for offline RL. SAM seeks flatter minima, guiding models to more robust parameter regions. We integrate SAM into strong baselines for data corruption: IQL, a top-performing offline RL algorithm in this setting, and RIQL, an algorithm designed specifically for data-corruption robustness. We evaluate them on D4RL benchmarks with both random and adversarial corruption. Our SAM-enhanced methods consistently and significantly outperform the original baselines. Visualizations of the reward surface confirm that SAM finds smoother solutions, providing strong evidence for its effectiveness in improving the robustness of offline RL agents."}
{"id": "2511.17787", "categories": ["cs.LG", "physics.med-ph", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2511.17787", "abs": "https://arxiv.org/abs/2511.17787", "authors": ["Elizabeth Chen", "Andrew Lee", "Tanbir Sarowar", "Xiaolin Chen"], "title": "Data-Driven Predictive Modeling of Microfluidic Cancer Cell Separation Using a Deterministic Lateral Displacement Device", "comment": "Accepted to IEEE International Conference on Data Mining (ICDM) 2025 REU Symposium", "summary": "Deterministic Lateral Displacement (DLD) devices are widely used in microfluidics for label-free, size-based separation of particles and cells, with particular promise in isolating circulating tumor cells (CTCs) for early cancer diagnostics. This study focuses on the optimization of DLD design parameters, such as row shift fraction, post size, and gap distance, to enhance the selective isolation of lung cancer cells based on their physical properties. To overcome the challenges of rare CTC detection and reduce reliance on computationally intensive simulations, machine learning models including gradient boosting, k-nearest neighbors, random forest, and multilayer perceptron (MLP) regressors are employed. Trained on a large, numerically validated dataset, these models predict particle trajectories and identify optimal device configurations, enabling high-throughput and cost-effective DLD design. Beyond trajectory prediction, the models aid in isolating critical design variables, offering a systematic, data-driven framework for automated DLD optimization. This integrative approach advances the development of scalable and precise microfluidic systems for cancer diagnostics, contributing to the broader goals of early detection and personalized medicine."}
{"id": "2511.17573", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.17573", "abs": "https://arxiv.org/abs/2511.17573", "authors": ["Michael J. Bommarito"], "title": "Binary BPE: A Family of Cross-Platform Tokenizers for Binary Analysis", "comment": "17 pages, 3 figures, 9 tables. Paper source available at https://github.com/mjbommar/binary-tokenizer-paper ; tokenizers available at https://huggingface.co/mjbommar - mjbommar/binary-tokenizer-001-{4k,8k,16k,32k,64k}", "summary": "Sequence models for binary analysis are bottlenecked by byte-level tokenization: raw bytes waste precious context window capacity for transformers and other neural network architectures, and many existing text-oriented tokenizers fail on arbitrary 0x00--0xFF sequences. To address this issue, we introduce the Binary BPE tokenizer family, a set of cross-platform Byte Pair Encoding (BPE) tokenizers for executables trained on a large corpus of binaries spanning multiple platforms, architectures, and operating systems, including Linux, Windows, macOS, Android, and malware sources. We release trained tokenizers with vocabularies of 4K, 8K, 16K, 32K, and 64K tokens, enabling both systematic scaling studies and practical deployment from resource-constrained edge devices to high-throughput datacenters. These tokenizers discover interpretable patterns (ELF/PE headers, instruction sequences, cross-platform strings) while yielding multi-byte compression per token. On representative uncompressed executables (e.g., ELF/PE/Mach-O rather than compressed APKs), the Binary BPE tokenizers typically allow for roughly 2-3x more binary content per fixed-length transformer context window than raw bytes, enabling more efficient research and practical deployment for content identification, malware detection, reverse engineering, and optimization. We release the trained Binary BPE tokenizers on HuggingFace, providing a drop-in, open-source foundation for binary-focused language models and context-efficient agentic tools."}
{"id": "2511.17789", "categories": ["cs.LG", "cond-mat.dis-nn"], "pdf": "https://arxiv.org/pdf/2511.17789", "abs": "https://arxiv.org/abs/2511.17789", "authors": ["Sam Dillavou", "Shruti Mishra"], "title": "Physical Reinforcement Learning", "comment": "9 pages 4 figures", "summary": "Digital computers are power-hungry and largely intolerant of damaged components, making them potentially difficult tools for energy-limited autonomous agents in uncertain environments. Recently developed Contrastive Local Learning Networks (CLLNs) - analog networks of self-adjusting nonlinear resistors - are inherently low-power and robust to physical damage, but were constructed to perform supervised learning. In this work we demonstrate success on two simple RL problems using Q-learning adapted for simulated CLLNs. Doing so makes explicit the components (beyond the network being trained) required to enact various tools in the RL toolbox, some of which (policy function and value function) are more natural in this system than others (replay buffer). We discuss assumptions such as the physical safety that digital hardware requires, CLLNs can forgo, and biological systems cannot rely on, and highlight secondary goals that are important in biology and trainable in CLLNs, but make little sense in digital computers."}
{"id": "2511.17574", "categories": ["cs.SI", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.17574", "abs": "https://arxiv.org/abs/2511.17574", "authors": ["Eamon Earl", "Chen Ding", "Richard Valenzano", "Drai Paulen-Patterson"], "title": "Constructing Political Coordinates: Aggregating Over the Opposition for Diverse News Recommendation", "comment": "Due to appear in the proceedings of the 2025 IEEE International Conference on Big Data", "summary": "In the past two decades, open access to news and information has increased rapidly, empowering educated political growth within democratic societies. News recommender systems (NRSs) have shown to be useful in this process, minimizing political disengagement and information overload by providing individuals with articles on topics that matter to them. Unfortunately, NRSs often conflate underlying user interest with the partisan bias of the articles in their reading history and with the most popular biases present in the coverage of their favored topics. Over extended interaction, this can result in the formation of filter bubbles and the polarization of user partisanship. In this paper, we propose a novel embedding space called Constructed Political Coordinates (CPC), which models the political partisanship of users over a given topic-space, relative to a larger sample population. We apply a simple collaborative filtering (CF) framework using CPC-based correlation to recommend articles sourced from oppositional users, who have different biases from the user in question. We compare against classical CF methods and find that CPC-based methods promote pointed bias diversity and better match the true political tolerance of users, while classical methods implicitly exploit biases to maximize interaction."}
{"id": "2511.17796", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.17796", "abs": "https://arxiv.org/abs/2511.17796", "authors": ["Afsaneh Mahanipour", "Hana Khamfroush"], "title": "Semi-Supervised Federated Multi-Label Feature Selection with Fuzzy Information Measures", "comment": "This paper has been accepted for presentation at GLOBECOM 2025", "summary": "Multi-label feature selection (FS) reduces the dimensionality of multi-label data by removing irrelevant, noisy, and redundant features, thereby boosting the performance of multi-label learning models. However, existing methods typically require centralized data, which makes them unsuitable for distributed and federated environments where each device/client holds its own local dataset. Additionally, federated methods often assume that clients have labeled data, which is unrealistic in cases where clients lack the expertise or resources to label task-specific data. To address these challenges, we propose a Semi-Supervised Federated Multi-Label Feature Selection method, called SSFMLFS, where clients hold only unlabeled data, while the server has limited labeled data. SSFMLFS adapts fuzzy information theory to a federated setting, where clients compute fuzzy similarity matrices and transmit them to the server, which then calculates feature redundancy and feature-label relevancy degrees. A feature graph is constructed by modeling features as vertices, assigning relevancy and redundancy degrees as vertex weights and edge weights, respectively. PageRank is then applied to rank the features by importance. Extensive experiments on five real-world datasets from various domains, including biology, images, music, and text, demonstrate that SSFMLFS outperforms other federated and centralized supervised and semi-supervised approaches in terms of three different evaluation metrics in non-IID data distribution setting."}
{"id": "2511.17577", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.17577", "abs": "https://arxiv.org/abs/2511.17577", "authors": ["Fengming Yu", "Qingyu Meng", "Haiwei Pan", "Kejia Zhang"], "title": "Efficient Mathematical Reasoning Models via Dynamic Pruning and Knowledge Distillation", "comment": "12 pages, 1 figure", "summary": "With the rapid development of deep learning, large language models have shown strong capabilities in complex reasoning tasks such as mathematical equation solving. However, their substantial computational and storage costs hinder practical deployment. This paper proposes a lightweight optimization method that integrates dynamic attention head pruning with knowledge distillation. The approach dynamically evaluates the importance of each attention head in the multi-head attention mechanism using a combination of weight norms and entropy, and prunes redundant heads in real time to reduce computational overhead. To mitigate performance degradation, knowledge distillation transfers information from the original model to the pruned student, enabling the smaller model to preserve reasoning ability. Experiments conducted on both Math23k and ASDiv-A verify the effectiveness of the proposed method. For example, on Math23k with a 30% pruning ratio, parameters are reduced by 18.7%, inference speed is improved by 27.5%, FLOPs are reduced by 19.3%, and accuracy drops only 0.7% (from 84.4% to 83.7%). These results demonstrate that the method achieves substantial efficiency gains while maintaining strong reasoning performance, providing a practical solution for efficient deployment of large language models in mathematical reasoning tasks."}
{"id": "2511.17801", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17801", "abs": "https://arxiv.org/abs/2511.17801", "authors": ["Cuong Pham", "Hoang Anh Dung", "Cuong C. Nguyen", "Trung Le", "Gustavo Carneiro", "Thanh-Toan Do"], "title": "Layer-Wise High-Impact Parameter Ratio Optimization in Post-Training Quantization for Large Language Models", "comment": null, "summary": "Large language models (LLMs) have significantly advanced natural language processing, but their massive parameter counts create substantial computational and memory challenges during deployment. Post-training quantization (PTQ) has emerged as a promising approach to mitigate these challenges with minimal overhead. While existing PTQ methods can effectively quantize LLMs, they experience substantial accuracy loss at extremely low bit-widths, primarily due to high-impact parameters that significantly influence quantization performance. Several approaches address these issues by identifying and retaining the high-impact parameters in FP16 format. However, they apply fixed ratios of high-impact parameters across all layers, overlooking layer-wise sensitivity variations. In this paper, we propose a quadratic optimization framework that determines layer-specific ratios of high-impact parameters while considering inter-layer dependencies. We quantize high-impact parameters to moderate bit-widths, which often result in negligible performance degradation in quantized LLMs, while the remaining parameters can be quantized to extremely low bit-widths. Under the same resource-constrained budget, this allows for preserving more high-impact parameters than methods that keep selecting a few in FP16 format. Additionally, the proposed framework allows us to leverage an advanced quantization method that often requires extensive learnable parameters solely for high-impact parameters, while applying a computationally efficient method to the rest. Our approach achieves an effective balance between computational efficiency and model accuracy while maintaining high performance compared to state-of-the-art methods."}
{"id": "2511.17579", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17579", "abs": "https://arxiv.org/abs/2511.17579", "authors": ["Hefei Xu", "Le Wu", "Chen Cheng", "Hao Liu"], "title": "Multi-Value Alignment for LLMs via Value Decorrelation and Extrapolation", "comment": "accepted by AAAI26 oral; 12 pages", "summary": "With the rapid advancement of large language models (LLMs), aligning them with human values for safety and ethics has become a critical challenge. This problem is especially challenging when multiple, potentially conflicting human values must be considered and balanced. Although several variants of existing alignment methods (such as Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO)) have been proposed to address multi-value alignment, they suffer from notable limitations: 1) they are often unstable and inefficient in multi-value optimization; and 2) they fail to effectively handle value conflicts. As a result, these approaches typically struggle to achieve optimal trade-offs when aligning multiple values.\n  To address this challenge, we propose a novel framework called Multi-Value Alignment (MVA). It mitigates alignment degradation caused by parameter interference among diverse human values by minimizing their mutual information. Furthermore, we propose a value extrapolation strategy to efficiently explore the Pareto frontier, thereby constructing a set of LLMs with diverse value preferences. Extensive experiments demonstrate that MVA consistently outperforms existing baselines in aligning LLMs with multiple human values."}
{"id": "2511.17809", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17809", "abs": "https://arxiv.org/abs/2511.17809", "authors": ["Cuong Pham", "Hoang Anh Dung", "Cuong C. Nguyen", "Trung Le", "Gustavo Carneiro", "Jianfei Cai", "Thanh-Toan Do"], "title": "Adaptive Layer-Wise Transformations for Post-Training Quantization of Large Language Models", "comment": null, "summary": "Large language models require significant computational resources for deployment, making quantization essential for practical applications. However, the main obstacle to effective quantization lies in systematic outliers in activations and weights, which cause substantial LLM performance degradation, especially at low-bit settings. While existing transformation-based methods like affine and rotation transformations successfully mitigate outliers, they apply the homogeneous transformation setting, i.e., using the same transformation types across all layers, ignoring the heterogeneous distribution characteristics within LLMs. In this paper, we propose an adaptive transformation selection framework that systematically determines optimal transformations on a per-layer basis. To this end, we first formulate transformation selection as a differentiable optimization problem to achieve the accurate transformation type for each layer. However, searching for optimal layer-wise transformations for every model is computationally expensive. To this end, we establish the connection between weight distribution kurtosis and accurate transformation type. Specifically, we propose an outlier-guided layer selection method using robust $z$-score normalization that achieves comparable performance to differentiable search with significantly reduced overhead. Comprehensive experiments on LLaMA family models demonstrate that our adaptive approach consistently outperforms the widely-used fixed transformation settings. For example, our method achieves an improvement of up to 4.58 perplexity points and a 2.11% gain in average six-task zero-shot accuracy under aggressive W3A3K2V2 quantization settings for the LLaMA-3-8B model compared to the current best existing method, FlatQuant, demonstrating the necessity of heterogeneous transformation selection for optimal LLM quantization."}
{"id": "2511.17582", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17582", "abs": "https://arxiv.org/abs/2511.17582", "authors": ["Jie Ou", "Shuaihong Jiang", "Yingjun Du", "Cees G. M. Snoek"], "title": "GateRA: Token-Aware Modulation for Parameter-Efficient Fine-Tuning", "comment": "Accepted by AAAI 2026", "summary": "Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, DoRA, and HiRA, enable lightweight adaptation of large pre-trained models via low-rank updates. However, existing PEFT approaches apply static, input-agnostic updates to all tokens, disregarding the varying importance and difficulty of different inputs. This uniform treatment can lead to overfitting on trivial content or under-adaptation on more informative regions, especially in autoregressive settings with distinct prefill and decoding dynamics. In this paper, we propose GateRA, a unified framework that introduces token-aware modulation to dynamically adjust the strength of PEFT updates. By incorporating adaptive gating into standard PEFT branches, GateRA enables selective, token-level adaptation, preserving pre-trained knowledge for well-modeled inputs while focusing capacity on challenging cases. Empirical visualizations reveal phase-sensitive behaviors, where GateRA automatically suppresses updates for redundant prefill tokens while emphasizing adaptation during decoding. To promote confident and efficient modulation, we further introduce an entropy-based regularization that encourages near-binary gating decisions. This regularization prevents diffuse update patterns and leads to interpretable, sparse adaptation without hard thresholding. Finally, we present a theoretical analysis showing that GateRA induces a soft gradient-masking effect over the PEFT path, enabling continuous and differentiable control over adaptation. Experiments on multiple commonsense reasoning benchmarks demonstrate that GateRA consistently outperforms or matches prior PEFT methods."}
{"id": "2511.17818", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17818", "abs": "https://arxiv.org/abs/2511.17818", "authors": ["Aishwarya Mandyam", "Kalyani Limaye", "Barbara E. Engelhardt", "Emily Alsentzer"], "title": "APRIL: Annotations for Policy evaluation with Reliable Inference from LLMs", "comment": null, "summary": "Off-policy evaluation (OPE) estimates the value of a contextual bandit policy prior to deployment. As such, OPE plays a critical role in ensuring safety in high-stakes domains such as healthcare. However, standard OPE approaches are limited by the size and coverage of the behavior dataset. While previous work has explored using expert-labeled counterfactual annotations to enhance dataset coverage, obtaining such annotations is expensive, limiting the scalability of prior approaches. We propose leveraging large language models (LLMs) to generate counterfactual annotations for OPE in medical domains. Our method uses domain knowledge to guide LLMs in predicting how key clinical features evolve under alternate treatments. These predicted features can then be transformed using known reward functions to create counterfactual annotations. We first evaluate the ability of several LLMs to predict clinical features across two patient subsets in MIMIC-IV, finding that state-of-the-art LLMs achieve comparable performance. Building on this capacity to predict clinical features, we generate LLM-based counterfactual annotations and incorporate them into an OPE estimator. Our empirical results analyze the benefits of counterfactual annotations under varying degrees of shift between the behavior and target policies. We find that in most cases, the LLM-based counterfactual annotations significantly improve OPE estimates up to a point. We provide an entropy-based metric to identify when additional annotations cease to be useful. Our results demonstrate that LLM-based counterfactual annotations offer a scalable approach for addressing coverage limitations in healthcare datasets, enabling safer deployment of decision-making policies in clinical settings."}
{"id": "2511.17584", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17584", "abs": "https://arxiv.org/abs/2511.17584", "authors": ["Haoyan Xu", "Ruizhi Qian", "Zhengtao Yao", "Ziyi Liu", "Li Li", "Yuqi Li", "Yanshu Li", "Wenqing Zheng", "Daniele Rosa", "Daniel Barcklow", "Senthil Kumar", "Jieyu Zhao", "Yue Zhao"], "title": "LLM-Powered Text-Attributed Graph Anomaly Detection via Retrieval-Augmented Reasoning", "comment": null, "summary": "Anomaly detection on attributed graphs plays an essential role in applications such as fraud detection, intrusion monitoring, and misinformation analysis. However, text-attributed graphs (TAGs), in which node information is expressed in natural language, remain underexplored, largely due to the absence of standardized benchmark datasets. In this work, we introduce TAG-AD, a comprehensive benchmark for anomaly node detection on TAGs. TAG-AD leverages large language models (LLMs) to generate realistic anomalous node texts directly in the raw text space, producing anomalies that are semantically coherent yet contextually inconsistent and thus more reflective of real-world irregularities. In addition, TAG-AD incorporates multiple other anomaly types, enabling thorough and reproducible evaluation of graph anomaly detection (GAD) methods. With these datasets, we further benchmark existing unsupervised GNN-based GAD methods as well as zero-shot LLMs for GAD.\n  As part of our zero-shot detection setup, we propose a retrieval-augmented generation (RAG)-assisted, LLM-based zero-shot anomaly detection framework. The framework mitigates reliance on brittle, hand-crafted prompts by constructing a global anomaly knowledge base and distilling it into reusable analysis frameworks. Our experimental results reveal a clear division of strengths: LLMs are particularly effective at detecting contextual anomalies, whereas GNN-based methods remain superior for structural anomaly detection. Moreover, RAG-assisted prompting achieves performance comparable to human-designed prompts while eliminating manual prompt engineering, underscoring the practical value of our RAG-assisted zero-shot LLM anomaly detection framework."}
{"id": "2511.17822", "categories": ["cs.LG", "cs.DS", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.17822", "abs": "https://arxiv.org/abs/2511.17822", "authors": ["Ziyun Chen", "Spencer Compton", "Daniel Kane", "Jerry Li"], "title": "High-Accuracy List-Decodable Mean Estimation", "comment": "Abstract shortened to meet arXiv requirement", "summary": "In list-decodable learning, we are given a set of data points such that an $α$-fraction of these points come from a nice distribution $D$, for some small $α\\ll 1$, and the goal is to output a short list of candidate solutions, such that at least one element of this list recovers some non-trivial information about $D$. By now, there is a large body of work on this topic; however, while many algorithms can achieve optimal list size in terms of $α$, all known algorithms must incur error which decays, in some cases quite poorly, with $1 / α$. In this paper, we ask if this is inherent: is it possible to trade off list size with accuracy in list-decodable learning? More formally, given $ε> 0$, can we can output a slightly larger list in terms of $α$ and $ε$, but so that one element of this list has error at most $ε$ with the ground truth? We call this problem high-accuracy list-decodable learning. Our main result is that non-trivial high-accuracy guarantees, both information-theoretically and algorithmically, are possible for the canonical setting of list-decodable mean estimation of identity-covariance Gaussians. Specifically, we demonstrate that there exists a list of candidate means of size at most $L = \\exp \\left( O\\left( \\tfrac{\\log^2 1 / α}{ε^2} \\right)\\right)$ so that one of the elements of this list has $\\ell_2$ distance at most $ε$ to the true mean. We also design an algorithm that outputs such a list with runtime and sample complexity $n = d^{O(\\log L)} + \\exp \\exp (\\widetilde{O}(\\log L))$. We do so by demonstrating a completely novel proof of identifiability, as well as a new algorithmic way of leveraging this proof without the sum-of-squares hierarchy, which may be of independent technical interest."}
{"id": "2511.17585", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17585", "abs": "https://arxiv.org/abs/2511.17585", "authors": ["Kang He", "Boyu Chen", "Yuzhe Ding", "Fei Li", "Chong Teng", "Donghong Ji"], "title": "PaSE: Prototype-aligned Calibration and Shapley-based Equilibrium for Multimodal Sentiment Analysis", "comment": "Accepted by AAAI 2026", "summary": "Multimodal Sentiment Analysis (MSA) seeks to understand human emotions by integrating textual, acoustic, and visual signals. Although multimodal fusion is designed to leverage cross-modal complementarity, real-world scenarios often exhibit modality competition: dominant modalities tend to overshadow weaker ones, leading to suboptimal performance.In this paper, we propose PaSE, a novel Prototype-aligned Calibration and Shapley-optimized Equilibrium framework, which enhances collaboration while explicitly mitigating modality competition. PaSE first applies Prototype-guided Calibration Learning (PCL) to refine unimodal representations and align them through an Entropic Optimal Transport mechanism that ensures semantic consistency. To further stabilize optimization, we introduce a Dual-Phase Optimization strategy. A prototype-gated fusion module is first used to extract shared representations, followed by Shapley-based Gradient Modulation (SGM), which adaptively adjusts gradients according to the contribution of each modality. Extensive experiments on IEMOCAP, MOSI, and MOSEI confirm that PaSE achieves the superior performance and effectively alleviates modality competition."}
{"id": "2511.17823", "categories": ["cs.LG", "cs.CE", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.17823", "abs": "https://arxiv.org/abs/2511.17823", "authors": ["Naitik Gada"], "title": "A novel k-means clustering approach using two distance measures for Gaussian data", "comment": "Keywords: machine learning, clustering algorithms, k-means", "summary": "Clustering algorithms have long been the topic of research, representing the more popular side of unsupervised learning. Since clustering analysis is one of the best ways to find some clarity and structure within raw data, this paper explores a novel approach to \\textit{k}-means clustering. Here we present a \\textit{k}-means clustering algorithm that takes both the within cluster distance (WCD) and the inter cluster distance (ICD) as the distance metric to cluster the data into \\emph{k} clusters pre-determined by the Calinski-Harabasz criterion in order to provide a more robust output for the clustering analysis. The idea with this approach is that by including both the measurement metrics, the convergence of the data into their clusters becomes solidified and more robust. We run the algorithm with some synthetically produced data and also some benchmark data sets obtained from the UCI repository. The results show that the convergence of the data into their respective clusters is more accurate by using both WCD and ICD measurement metrics. The algorithm is also better at clustering the outliers into their true clusters as opposed to the traditional \\textit{k} means method. We also address some interesting possible research topics that reveal themselves as we answer the questions we initially set out to address."}
{"id": "2511.17587", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17587", "abs": "https://arxiv.org/abs/2511.17587", "authors": ["Yuxuan Hu", "Jian Chen", "Yuhao Wang", "Zixuan Li", "Jing Xiong", "Pengyue Jia", "Wei Wang", "Chengming Li", "Xiangyu Zhao"], "title": "Emotion and Intention Guided Multi-Modal Learning for Sticker Response Selection", "comment": null, "summary": "Stickers are widely used in online communication to convey emotions and implicit intentions. The Sticker Response Selection (SRS) task aims to select the most contextually appropriate sticker based on the dialogue. However, existing methods typically rely on semantic matching and model emotional and intentional cues separately, which can lead to mismatches when emotions and intentions are misaligned. To address this issue, we propose Emotion and Intention Guided Multi-Modal Learning (EIGML). This framework is the first to jointly model emotion and intention, effectively reducing the bias caused by isolated modeling and significantly improving selection accuracy. Specifically, we introduce Dual-Level Contrastive Framework to perform both intra-modality and inter-modality alignment, ensuring consistent representation of emotional and intentional features within and across modalities. In addition, we design an Intention-Emotion Guided Multi-Modal Fusion module that integrates emotional and intentional information progressively through three components: Emotion-Guided Intention Knowledge Selection, Intention-Emotion Guided Attention Fusion, and Similarity-Adjusted Matching Mechanism. This design injects rich, effective information into the model and enables a deeper understanding of the dialogue, ultimately enhancing sticker selection performance. Experimental results on two public SRS datasets show that EIGML consistently outperforms state-of-the-art baselines, achieving higher accuracy and a better understanding of emotional and intentional features. Code is provided in the supplementary materials."}
{"id": "2511.17826", "categories": ["cs.LG", "cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.17826", "abs": "https://arxiv.org/abs/2511.17826", "authors": ["Ziyang Zhang", "Xinheng Ding", "Jiayi Yuan", "Rixin Liu", "Huizi Mao", "Jiarong Xing", "Zirui Liu"], "title": "Deterministic Inference across Tensor Parallel Sizes That Eliminates Training-Inference Mismatch", "comment": null, "summary": "Deterministic inference is increasingly critical for large language model (LLM) applications such as LLM-as-a-judge evaluation, multi-agent systems, and Reinforcement Learning (RL). However, existing LLM serving frameworks exhibit non-deterministic behavior: identical inputs can yield different outputs when system configurations (e.g., tensor parallel (TP) size, batch size) vary, even under greedy decoding. This arises from the non-associativity of floating-point arithmetic and inconsistent reduction orders across GPUs. While prior work has addressed batch-size-related nondeterminism through batch-invariant kernels, determinism across different TP sizes remains an open problem, particularly in RL settings, where the training engine typically uses Fully Sharded Data Parallel (i.e., TP = 1) while the rollout engine relies on multi-GPU TP to maximize the inference throughput, creating a natural mismatch between the two. This precision mismatch problem may lead to suboptimal performance or even collapse for RL training. We identify and analyze the root causes of TP-induced inconsistency and propose Tree-Based Invariant Kernels (TBIK), a set of TP-invariant matrix multiplication and reduction primitives that guarantee bit-wise identical results regardless of TP size. Our key insight is to align intra- and inter-GPU reduction orders through a unified hierarchical binary tree structure. We implement these kernels in Triton and integrate them into vLLM and FSDP. Experiments confirm zero probability divergence and bit-wise reproducibility for deterministic inference across different TP sizes. Also, we achieve bit-wise identical results between vLLM and FSDP in RL training pipelines with different parallel strategy. Code is available at https://github.com/nanomaoli/llm_reproducibility."}
{"id": "2511.17590", "categories": ["cs.LG", "cs.AI", "cs.CY", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.17590", "abs": "https://arxiv.org/abs/2511.17590", "authors": ["Ke Yu", "Shigeru Ishikura", "Yukari Usukura", "Yuki Shigoku", "Teruaki Hayashi"], "title": "SHAP Distance: An Explainability-Aware Metric for Evaluating the Semantic Fidelity of Synthetic Tabular Data", "comment": "IEEE Bigdata", "summary": "Synthetic tabular data, which are widely used in domains such as healthcare, enterprise operations, and customer analytics, are increasingly evaluated to ensure that they preserve both privacy and utility. While existing evaluation practices typically focus on distributional similarity (e.g., the Kullback-Leibler divergence) or predictive performance (e.g., Train-on-Synthetic-Test-on-Real (TSTR) accuracy), these approaches fail to assess semantic fidelity, that is, whether models trained on synthetic data follow reasoning patterns consistent with those trained on real data. To address this gap, we introduce the SHapley Additive exPlanations (SHAP) Distance, a novel explainability-aware metric that is defined as the cosine distance between the global SHAP attribution vectors derived from classifiers trained on real versus synthetic datasets. By analyzing datasets that span clinical health records with physiological features, enterprise invoice transactions with heterogeneous scales, and telecom churn logs with mixed categorical-numerical attributes, we demonstrate that the SHAP Distance reliably identifies semantic discrepancies that are overlooked by standard statistical and predictive measures. In particular, our results show that the SHAP Distance captures feature importance shifts and underrepresented tail effects that the Kullback-Leibler divergence and Train-on-Synthetic-Test-on-Real accuracy fail to detect. This study positions the SHAP Distance as a practical and discriminative tool for auditing the semantic fidelity of synthetic tabular data, and offers practical guidelines for integrating attribution-based evaluation into future benchmarking pipelines."}
{"id": "2511.17829", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17829", "abs": "https://arxiv.org/abs/2511.17829", "authors": ["Akhil Singampalli", "Sudeep Pasricha"], "title": "Unified Class and Domain Incremental Learning with Mixture of Experts for Indoor Localization", "comment": null, "summary": "Indoor localization using machine learning has gained traction due to the growing demand for location-based services. However, its long-term reliability is hindered by hardware/software variations across mobile devices, which shift the model's input distribution to create domain shifts. Further, evolving indoor environments can introduce new locations over time, expanding the output space to create class shifts, making static machine learning models ineffective over time. To address these challenges, we propose a novel unified continual learning framework for indoor localization called MOELO that, for the first time, jointly addresses domain-incremental and class-incremental learning scenarios. MOELO enables a lightweight, robust, and adaptive localization solution that can be deployed on resource-limited mobile devices and is capable of continual learning in dynamic, heterogeneous real-world settings. This is made possible by a mixture-of-experts architecture, where experts are incrementally trained per region and selected through an equiangular tight frame based gating mechanism ensuring efficient routing, and low-latency inference, all within a compact model footprint. Experimental evaluations show that MOELO achieves improvements of up to 25.6x in mean localization error, 44.5x in worst-case localization error, and 21.5x lesser forgetting compared to state-of-the-art frameworks across diverse buildings, mobile devices, and learning scenarios."}
{"id": "2511.17595", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17595", "abs": "https://arxiv.org/abs/2511.17595", "authors": ["Markus D. Solbach", "John K. Tsotsos"], "title": "Boosting Reinforcement Learning in 3D Visuospatial Tasks Through Human-Informed Curriculum Design", "comment": "12 pages, 11 figures, 5 tables", "summary": "Reinforcement Learning is a mature technology, often suggested as a potential route towards Artificial General Intelligence, with the ambitious goal of replicating the wide range of abilities found in natural and artificial intelligence, including the complexities of human cognition. While RL had shown successes in relatively constrained environments, such as the classic Atari games and specific continuous control problems, recent years have seen efforts to expand its applicability. This work investigates the potential of RL in demonstrating intelligent behaviour and its progress in addressing more complex and less structured problem domains.\n  We present an investigation into the capacity of modern RL frameworks in addressing a seemingly straightforward 3D Same-Different visuospatial task. While initial applications of state-of-the-art methods, including PPO, behavioural cloning and imitation learning, revealed challenges in directly learning optimal strategies, the successful implementation of curriculum learning offers a promising avenue. Effective learning was achieved by strategically designing the lesson plan based on the findings of a real-world human experiment."}
{"id": "2511.17840", "categories": ["cs.LG", "math.CT"], "pdf": "https://arxiv.org/pdf/2511.17840", "abs": "https://arxiv.org/abs/2511.17840", "authors": ["Tony Shaska"], "title": "Internalizing Tools as Morphisms in Graded Transformers", "comment": null, "summary": "We introduce a graded formulation of internal symbolic computation for transformers. The hidden space is endowed with a grading $V=\\bigoplus_{g\\in G}V_g$, and symbolic operations are realized as typed block maps (morphisms) $φ_{h\\leftarrow g}:V_g\\to V_h$ that are activated selectively by a differentiable routing policy. A self-supervised \\emph{graded utility functional}, defined as the loss reduction induced by a candidate morphism, governs activation and yields sparse, interpretable behavior. We develop the algebraic and geometric foundations: an internal model category whose objects are homogeneous components and whose morphisms are admissible grade transitions; adjoint pairs encoding typed round trips; and information-geometric interpretations in terms of KL gain, mirror descent with Bregman divergences, and Fisher natural gradients. Methodologically, we specify a utility--aware routing mechanism and objective that remain fully end-to-end differentiable. Analytic case studies and lightweight sanity checks illustrate selective morphic activation on hybrid symbolic-linguistic tasks. The framework unifies symbolic computation, geometry, and self--supervised learning within the \\emph{graded transformer} formalism \\cite{sh-89,sh-95}, while subsuming prior external-tool paradigms (e.g., Toolformer \\cite{toolformer2023}) as a special case via functorial internalization."}
{"id": "2511.17599", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17599", "abs": "https://arxiv.org/abs/2511.17599", "authors": ["Jianbing Dong", "Jianbin Chang"], "title": "From Projection to Prediction: Beyond Logits for Scalable Language Models", "comment": "17 pages, 2 figures, 4 algorithms", "summary": "Training Large Language Models (LLMs) typically involves a two-stage pipeline at the output layer: hidden states are projected into vocabulary logits via a linear transformation (lm_head), followed by cross-entropy loss computation against target tokens. While conceptually simple, this design incurs substantial overhead. The intermediate logits tensor, with dimensions proportional to batch size, sequence length, and vocabulary size, must be fully materialized in GPU memory, even though only one target token per position is ultimately used. This leads to significant memory footprint and bandwidth comsumption, limiting scalability and slowing training throughput.\n  In this work, we introduce a novel approach to integrates the output projection and loss prediction into a single operation. By directly computing the loss from hidden states and target tokens, our approach bypasses explicit logits materialization. This design reduces memory usage and alleviates bandwidth pressure. Experiments on LLM training demonstrate that our method achieves substantial memory savings and measurable speedups compared to the standard two-stage pipeline, enabling large batch sizes and longer sequences without sacrificing accuracy. Our work highlights the benefits of rethinking the boundary between projection and prediction, offering a practical systems optimization for efficient LLM training."}
{"id": "2511.17848", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2511.17848", "abs": "https://arxiv.org/abs/2511.17848", "authors": ["Zhihui Tian", "Ethan Suwandi", "Tomas Oppelstrup", "Vasily V. Bulatov", "Joel B. Harley", "Fei Zhou"], "title": "Scaling Kinetic Monte-Carlo Simulations of Grain Growth with Combined Convolutional and Graph Neural Networks", "comment": null, "summary": "Graph neural networks (GNN) have emerged as a promising machine learning method for microstructure simulations such as grain growth. However, accurate modeling of realistic grain boundary networks requires large simulation cells, which GNN has difficulty scaling up to. To alleviate the computational costs and memory footprint of GNN, we propose a hybrid architecture combining a convolutional neural network (CNN) based bijective autoencoder to compress the spatial dimensions, and a GNN that evolves the microstructure in the latent space of reduced spatial sizes. Our results demonstrate that the new design significantly reduces computational costs with using fewer message passing layer (from 12 down to 3) compared with GNN alone. The reduction in computational cost becomes more pronounced as the spatial size increases, indicating strong computational scalability. For the largest mesh evaluated (160^3), our method reduces memory usage and runtime in inference by 117x and 115x, respectively, compared with GNN-only baseline. More importantly, it shows higher accuracy and stronger spatiotemporal capability than the GNN-only baseline, especially in long-term testing. Such combination of scalability and accuracy is essential for simulating realistic material microstructures over extended time scales. The improvements can be attributed to the bijective autoencoder's ability to compress information losslessly from spatial domain into a high dimensional feature space, thereby producing more expressive latent features for the GNN to learn from, while also contributing its own spatiotemporal modeling capability. The training was optimized to learn from the stochastic Potts Monte Carlo method. Our findings provide a highly scalable approach for simulating grain growth."}
{"id": "2511.17602", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17602", "abs": "https://arxiv.org/abs/2511.17602", "authors": ["Sushant Mehta"], "title": "Beyond Surface-Level Similarity: Hierarchical Contamination Detection for Synthetic Training Data in Foundation Models", "comment": null, "summary": "Synthetic data has become essential for training foundation models, yet benchmark contamination threatens evaluation integrity. Although existing detection methods identify token-level overlap, they fail to detect semantic-level contamination where synthetic data conceptually resemble benchmarks without lexical overlap. This gap is critical as foundation models increasingly train on synthetic data that may implicitly encode benchmark knowledge. We propose a hierarchical contamination detection framework operating at four levels: token level, semantic level, reasoning pattern, and performance cliff detection. Through controlled experiments on MMLU, GSM8K and HumanEval, we demonstrate that semantic-level contamination evades existing methods (F1=0.17-0.49) but is effectively detected by our hierarchical approach (F1 = 0.76), with an average improvement of 26. 5\\% over state-of-the-art baselines. Our framework provides practitioners with practical tools for audit pipelines and enables responsible deployment of synthetic training data."}
{"id": "2511.17852", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.17852", "abs": "https://arxiv.org/abs/2511.17852", "authors": ["Bochen Lyu", "Yiyang Jia", "Xiaohao Cai", "Zhanxing Zhu"], "title": "Transformers with RL or SFT Provably Learn Sparse Boolean Functions, But Differently", "comment": "43 pages, 5 figures", "summary": "Transformers can acquire Chain-of-Thought (CoT) capabilities to solve complex reasoning tasks through fine-tuning. Reinforcement learning (RL) and supervised fine-tuning (SFT) are two primary approaches to this end, yet their underlying mechanisms and differences remain theoretically unclear. In this work, we examine these aspects specifically for learning $k$-sparse Boolean functions with a one-layer transformer and intermediate supervision that is akin to CoT. In particular, we consider $k$-sparse Boolean functions that can be recursively decomposed into fixed 2-sparse Boolean functions. We analyze the learning dynamics of fine-tuning the transformer via either RL or SFT with CoT to identify sufficient conditions for it to provably learn these functions. We verify that these conditions hold for three basic examples, including $k$-PARITY, $k$-AND, and $k$-OR, thus demonstrating the learnability of both approaches. Notably, we reveal that RL and SFT exhibit distinct learning behaviors: RL learns the whole CoT chain simultaneously, whereas SFT learns the CoT chain step-by-step. Overall, our findings provide theoretical insights into the underlying mechanisms of RL and SFT as well as how they differ in triggering the CoT capabilities of transformers."}
{"id": "2511.17604", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17604", "abs": "https://arxiv.org/abs/2511.17604", "authors": ["Jiajun Ma", "Yongchao Zhang", "Chao Zhang", "Zhao Lv", "Shengbing Pei"], "title": "BrainHGT: A Hierarchical Graph Transformer for Interpretable Brain Network Analysis", "comment": null, "summary": "Graph Transformer shows remarkable potential in brain network analysis due to its ability to model graph structures and complex node relationships. Most existing methods typically model the brain as a flat network, ignoring its modular structure, and their attention mechanisms treat all brain region connections equally, ignoring distance-related node connection patterns. However, brain information processing is a hierarchical process that involves local and long-range interactions between brain regions, interactions between regions and sub-functional modules, and interactions among functional modules themselves. This hierarchical interaction mechanism enables the brain to efficiently integrate local computations and global information flow, supporting the execution of complex cognitive functions. To address this issue, we propose BrainHGT, a hierarchical Graph Transformer that simulates the brain's natural information processing from local regions to global communities. Specifically, we design a novel long-short range attention encoder that utilizes parallel pathways to handle dense local interactions and sparse long-range connections, thereby effectively alleviating the over-globalizing issue. To further capture the brain's modular architecture, we designe a prior-guided clustering module that utilizes a cross-attention mechanism to group brain regions into functional communities and leverage neuroanatomical prior to guide the clustering process, thereby improving the biological plausibility and interpretability. Experimental results indicate that our proposed method significantly improves performance of disease identification, and can reliably capture the sub-functional modules of the brain, demonstrating its interpretability."}
{"id": "2511.17861", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.17861", "abs": "https://arxiv.org/abs/2511.17861", "authors": ["Xuesong Jia", "Yuanjie Shi", "Ziquan Liu", "Yi Xu", "Yan Yan"], "title": "Cost-Sensitive Conformal Training with Provably Controllable Learning Bounds", "comment": "Accepted for Publication at Association for the Advancement of Artificial Intelligence (AAAI), 2026", "summary": "Conformal prediction (CP) is a general framework to quantify the predictive uncertainty of machine learning models that uses a set prediction to include the true label with a valid probability. To align the uncertainty measured by CP, conformal training methods minimize the size of the prediction sets. A typical way is to use a surrogate indicator function, usually Sigmoid or Gaussian error function. However, these surrogate functions do not have a uniform error bound to the indicator function, leading to uncontrollable learning bounds. In this paper, we propose a simple cost-sensitive conformal training algorithm that does not rely on the indicator approximation mechanism. Specifically, we theoretically show that minimizing the expected size of prediction sets is upper bounded by the expected rank of true labels. To this end, we develop a rank weighting strategy that assigns the weight using the rank of true label on each data sample. Our analysis provably demonstrates the tightness between the proposed weighted objective and the expected size of conformal prediction sets. Extensive experiments verify the validity of our theoretical insights, and superior empirical performance over other conformal training in terms of predictive efficiency with 21.38% reduction for average prediction set size."}
{"id": "2511.17606", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17606", "abs": "https://arxiv.org/abs/2511.17606", "authors": ["Ningling Ge", "Sicheng Dai", "Yu Zhu", "Shan Yu"], "title": "Energy-based Autoregressive Generation for Neural Population Dynamics", "comment": null, "summary": "Understanding brain function represents a fundamental goal in neuroscience, with critical implications for therapeutic interventions and neural engineering applications. Computational modeling provides a quantitative framework for accelerating this understanding, but faces a fundamental trade-off between computational efficiency and high-fidelity modeling. To address this limitation, we introduce a novel Energy-based Autoregressive Generation (EAG) framework that employs an energy-based transformer learning temporal dynamics in latent space through strictly proper scoring rules, enabling efficient generation with realistic population and single-neuron spiking statistics. Evaluation on synthetic Lorenz datasets and two Neural Latents Benchmark datasets (MC_Maze and Area2_bump) demonstrates that EAG achieves state-of-the-art generation quality with substantial computational efficiency improvements, particularly over diffusion-based methods. Beyond optimal performance, conditional generation applications show two capabilities: generalizing to unseen behavioral contexts and improving motor brain-computer interface decoding accuracy using synthetic neural data. These results demonstrate the effectiveness of energy-based modeling for neural population dynamics with applications in neuroscience research and neural engineering. Code is available at https://github.com/NinglingGe/Energy-based-Autoregressive-Generation-for-Neural-Population-Dynamics."}
{"id": "2511.17864", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17864", "abs": "https://arxiv.org/abs/2511.17864", "authors": ["Adrian Goldwaser", "Michael Munn", "Javier Gonzalvo", "Benoit Dherin"], "title": "Equivalence of Context and Parameter Updates in Modern Transformer Blocks", "comment": null, "summary": "Recent research has established that the impact of context in a vanilla transformer can be represented implicitly by forming a token-dependent, rank-1 patch to its MLP weights. This work extends that foundational theory to the diverse architectures of modern Large Language Models. We first demonstrate a precise, analytical solution for a Gemma-style transformer block, proving that the entire effect of a context can be perfectly mapped to rank-1 patches on its MLP weight matrices and a patch to the RMSNorm scale. We then generalize this result, providing a constructive proof and algorithm for multi-layer models. To unify these findings, we introduce a general framework centered on two core properties: input controllability and output controllability. We prove that a perfect implicit weight patch is possible for any MLP block where the inner function is input-controllable and the outer function is output-controllable. This provides a simpler and more powerful lens for understanding how transformer models transmute prompts into effective weights. This setup generalizes to a wide range of modern LLM architectures including gating, pre-/post-norm, mixture of experts and sequential/parallel transformer blocks."}
{"id": "2511.17616", "categories": ["cs.LG", "cs.AI", "math.DG"], "pdf": "https://arxiv.org/pdf/2511.17616", "abs": "https://arxiv.org/abs/2511.17616", "authors": ["Alexander Strunk", "Roland Assam"], "title": "Tensor Gauge Flow Models", "comment": null, "summary": "This paper introduces Tensor Gauge Flow Models, a new class of Generative Flow Models that generalize Gauge Flow Models and Higher Gauge Flow Models by incorporating higher-order Tensor Gauge Fields into the Flow Equation. This extension allows the model to encode richer geometric and gauge-theoretic structure in the data, leading to more expressive flow dynamics. Experiments on Gaussian mixture models show that Tensor Gauge Flow Models achieve improved generative performance compared to both standard and gauge flow baselines."}
{"id": "2511.17869", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17869", "abs": "https://arxiv.org/abs/2511.17869", "authors": ["Subramanyam Sahoo", "Jared Junkin"], "title": "The Horcrux: Mechanistically Interpretable Task Decomposition for Detecting and Mitigating Reward Hacking in Embodied AI Systems", "comment": "Accepted to the NeurIPS (Mexico City) 2025 Workshop on Embodied and Safe-Assured Robotic Systems (E-SARS). Thanks to Aman Chadha", "summary": "Embodied AI agents exploit reward signal flaws through reward hacking, achieving high proxy scores while failing true objectives. We introduce Mechanistically Interpretable Task Decomposition (MITD), a hierarchical transformer architecture with Planner, Coordinator, and Executor modules that detects and mitigates reward hacking. MITD decomposes tasks into interpretable subtasks while generating diagnostic visualizations including Attention Waterfall Diagrams and Neural Pathway Flow Charts. Experiments on 1,000 HH-RLHF samples reveal that decomposition depths of 12 to 25 steps reduce reward hacking frequency by 34 percent across four failure modes. We present new paradigms showing that mechanistically grounded decomposition offers a more effective way to detect reward hacking than post-hoc behavioral monitoring."}
{"id": "2511.17622", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17622", "abs": "https://arxiv.org/abs/2511.17622", "authors": ["Weidao Chen", "Yuxiao Yang", "Yueming Wang"], "title": "Neurocircuitry-Inspired Hierarchical Graph Causal Attention Networks for Explainable Depression Identification", "comment": "Under review for ICLR 2026", "summary": "Major Depressive Disorder (MDD), affecting millions worldwide, exhibits complex pathophysiology manifested through disrupted brain network dynamics. Although graph neural networks that leverage neuroimaging data have shown promise in depression diagnosis, existing approaches are predominantly data-driven and operate largely as black-box models, lacking neurobiological interpretability. Here, we present NH-GCAT (Neurocircuitry-Inspired Hierarchical Graph Causal Attention Networks), a novel framework that bridges neuroscience domain knowledge with deep learning by explicitly and hierarchically modeling depression-specific mechanisms at different spatial scales. Our approach introduces three key technical contributions: (1) at the local brain regional level, we design a residual gated fusion module that integrates temporal blood oxygenation level dependent (BOLD) dynamics with functional connectivity patterns, specifically engineered to capture local depression-relevant low-frequency neural oscillations; (2) at the multi-regional circuit level, we propose a hierarchical circuit encoding scheme that aggregates regional node representations following established depression neurocircuitry organization, and (3) at the multi-circuit network level, we develop a variational latent causal attention mechanism that leverages a continuous probabilistic latent space to infer directed information flow among critical circuits, characterizing disease-altered whole-brain inter-circuit interactions. Rigorous leave-one-site-out cross-validation on the REST-meta-MDD dataset demonstrates NH-GCAT's state-of-the-art performance in depression classification, achieving a sample-size weighted-average accuracy of 73.3\\% and an AUROC of 76.4\\%, while simultaneously providing neurobiologically meaningful explanations."}
{"id": "2511.17879", "categories": ["cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.17879", "abs": "https://arxiv.org/abs/2511.17879", "authors": ["Yusong Wu", "Stephen Brade", "Teng Ma", "Tia-Jane Fowler", "Enning Yang", "Berker Banar", "Aaron Courville", "Natasha Jaques", "Cheng-Zhi Anna Huang"], "title": "Generative Adversarial Post-Training Mitigates Reward Hacking in Live Human-AI Music Interaction", "comment": null, "summary": "Most applications of generative AI involve a sequential interaction in which a person inputs a prompt and waits for a response, and where reaction time and adaptivity are not important factors. In contrast, live jamming is a collaborative interaction that requires real-time coordination and adaptation without access to the other player's future moves, while preserving diversity to sustain a creative flow. Reinforcement learning post-training enables effective adaptation through on-policy interaction, yet it often reduces output diversity by exploiting coherence-based rewards. This collapse, known as ``reward hacking'', affects many RL post-training pipelines, but is especially harmful in live jamming, where musical creativity relies on dynamic variation and mutual responsiveness. In this paper, we propose a novel adversarial training method on policy-generated trajectories to mitigate reward hacking in RL post-training for melody-to-chord accompaniment. A co-evolving discriminator separates policy trajectories from the data distribution, while the policy maximizes the discriminator output in addition to coherence rewards to prevent collapse to trivial outputs. We evaluate accompaniment quality and output diversity in simulation with both fixed test melodies and learned melody agents, and we conduct a user study with the model deployed in a real-time interactive system with expert musicians. Quantitative evaluation and user feedback demonstrate improved output diversity, harmonic coherence, adaptation speed and user agency. Our results demonstrate a simple yet effective method to mitigate reward hacking in RL post-training of generative sequence models."}
{"id": "2511.17623", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17623", "abs": "https://arxiv.org/abs/2511.17623", "authors": ["Haoran Li", "Zhe Cheng", "Muhao Guo", "Yang Weng", "Yannan Sun", "Victor Tran", "John Chainaranont"], "title": "M$^2$OE$^2$-GL: A Family of Probabilistic Load Forecasters That Scales to Massive Customers", "comment": "5 pages", "summary": "Probabilistic load forecasting is widely studied and underpins power system planning, operation, and risk-aware decision making. Deep learning forecasters have shown strong ability to capture complex temporal and contextual patterns, achieving substantial accuracy gains. However, at the scale of thousands or even hundreds of thousands of loads in large distribution feeders, a deployment dilemma emerges: training and maintaining one model per customer is computationally and storage intensive, while using a single global model ignores distributional shifts across customer types, locations, and phases. Prior work typically focuses on single-load forecasters, global models across multiple loads, or adaptive/personalized models for relatively small settings, and rarely addresses the combined challenges of heterogeneity and scalability in large feeders. We propose M2OE2-GL, a global-to-local extension of the M2OE2 probabilistic forecaster. We first pretrain a single global M2OE2 base model across all feeder loads, then apply lightweight fine-tuning to derive a compact family of group-specific forecasters. Evaluated on realistic utility data, M2OE2-GL yields substantial error reductions while remaining scalable to very large numbers of loads."}
{"id": "2511.17902", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.17902", "abs": "https://arxiv.org/abs/2511.17902", "authors": ["Yifan He", "Haodong Zhang", "Qiuheng Song", "Lin Lei", "Zhenxuan Zeng", "Haoyang He", "Hongyan Wu"], "title": "Statistically-Guided Dual-Domain Meta-Learning with Adaptive Multi-Prototype Aggregation for Distributed Fiber Optic Sensing", "comment": null, "summary": "Distributed Fiber Optic Sensing (DFOS) has shown strong potential in perimeter security due to its capability of monitoring vibration events across long distances with fine spatial resolution. However, practical DFOS systems face three critical challenges: (1) signal patterns of the same activity vary drastically under different fiber deployment types (e.g., underground, wall-mounted), causing domain shift; (2) labeled data in new deployment scenarios is often scarce or entirely unavailable, limiting model adaptability; and (3) even within source domains, data scarcity makes it difficult to capture intra-class diversity for robust learning.\n  To address these challenges, we propose a novel meta-learning framework, DUPLE, for cross-deployment DFOS activity identification. First, a dual-domain multi-prototype learner fuses temporal and frequency domain features, enhancing the model's generalization ability under signal distribution shifts. Second, a Statistical Guided Network (SGN) infers domain importance and prototype sensitivity from raw statistical features, providing data-driven prior information for learning in unlabeled or unseen domains. Third, a query-aware prototype aggregation module adaptively selects and combines relevant prototypes, thereby improving classification performance even with limited data.\n  Extensive experiments on cross-deployment DFOS datasets demonstrate that our method significantly outperforms baseline approaches in domain generalization settings, enabling robust event recognition across diverse fiber configurations with minimal labeled data."}
{"id": "2511.17630", "categories": ["cs.LG", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.17630", "abs": "https://arxiv.org/abs/2511.17630", "authors": ["Nele Albers", "Esra Cemre Su de Groot", "Loes Keijsers", "Manon H. Hillegers", "Emiel Krahmer"], "title": "Can we use LLMs to bootstrap reinforcement learning? -- A case study in digital health behavior change", "comment": null, "summary": "Personalizing digital applications for health behavior change is a promising route to making them more engaging and effective. This especially holds for approaches that adapt to users and their specific states (e.g., motivation, knowledge, wants) over time. However, developing such approaches requires making many design choices, whose effectiveness is difficult to predict from literature and costly to evaluate in practice. In this work, we explore whether large language models (LLMs) can be used out-of-the-box to generate samples of user interactions that provide useful information for training reinforcement learning models for digital behavior change settings. Using real user data from four large behavior change studies as comparison, we show that LLM-generated samples can be useful in the absence of real data. Comparisons to the samples provided by human raters further show that LLM-generated samples reach the performance of human raters. Additional analyses of different prompting strategies including shorter and longer prompt variants, chain-of-thought prompting, and few-shot prompting show that the relative effectiveness of different strategies depends on both the study and the LLM with also relatively large differences between prompt paraphrases alone. We provide recommendations for how LLM-generated samples can be useful in practice."}
{"id": "2511.17936", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.17936", "abs": "https://arxiv.org/abs/2511.17936", "authors": ["Wenzhang Du"], "title": "Mitigating Catastrophic Forgetting in Streaming Generative and Predictive Learning via Stateful Replay", "comment": "11 pages, 4 figures", "summary": "Many deployed learning systems must update models on streaming data under memory constraints. The default strategy, sequential fine-tuning on each new phase, is architecture-agnostic but often suffers catastrophic forgetting when later phases correspond to different sub-populations or tasks. Replay with a finite buffer is a simple alternative, yet its behaviour across generative and predictive objectives is not well understood. We present a unified study of stateful replay for streaming autoencoding, time series forecasting, and classification. We view both sequential fine-tuning and replay as stochastic gradient methods for an ideal joint objective, and use a gradient alignment analysis to show when mixing current and historical samples should reduce forgetting. We then evaluate a single replay mechanism on six streaming scenarios built from Rotated MNIST, ElectricityLoadDiagrams 2011-2014, and Airlines delay data, using matched training budgets and three seeds. On heterogeneous multi task streams, replay reduces average forgetting by a factor of two to three, while on benign time based streams both methods perform similarly. These results position stateful replay as a strong and simple baseline for continual learning in streaming environments."}
{"id": "2511.17638", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17638", "abs": "https://arxiv.org/abs/2511.17638", "authors": ["Pratham Sorte"], "title": "Model-to-Model Knowledge Transmission (M2KT): A Data-Free Framework for Cross-Model Understanding Transfer", "comment": "8 pages including figures, prepared in IEEE conference style. Preprint. Work in progress", "summary": "Modern artificial intelligence systems depend heavily on large datasets for both training and transferring knowledge between models. Knowledge distillation, transfer learning, and dataset distillation have made such transfers more efficient, yet they remain fundamentally data-driven: a teacher must produce examples, logits, or gradients for a student to learn. In this work, we introduce Model-to-Model Knowledge Transmission (M2KT), a novel paradigm for data-free conceptual transfer between neural networks. M2KT enables models to exchange knowledge packets that encapsulate structured concept embeddings, abstraction graphs, reasoning traces, and provenance metadata. Unlike classical distillation, M2KT operates primarily in concept space rather than example space, and it does not require labeled datasets or teacher-generated outputs during transfer. We formalize the notion of concept manifolds, introduce an inter-model alignment mapping between teacher and student latent spaces, and derive a composite loss that enforces geometric, structural, and reasoning consistency together with explicit safety constraints. We further present algorithmic procedures for teacher-side packet generation and student-side ingestion and verification. Experiments on symbolic reasoning with large language models show that M2KT can achieve approximately 85 to 90 percent of teacher performance while reducing data usage by over 98 percent compared to standard knowledge distillation. This work establishes a theoretical and practical foundation for data-free AI-to-AI knowledge transfer and self-improving model ecosystems."}
{"id": "2511.17953", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.17953", "abs": "https://arxiv.org/abs/2511.17953", "authors": ["Min Woo Park", "Sanghack Lee"], "title": "On Transportability for Structural Causal Bandits", "comment": null, "summary": "Intelligent agents equipped with causal knowledge can optimize their action spaces to avoid unnecessary exploration. The structural causal bandit framework provides a graphical characterization for identifying actions that are unable to maximize rewards by leveraging prior knowledge of the underlying causal structure. While such knowledge enables an agent to estimate the expected rewards of certain actions based on others in online interactions, there has been little guidance on how to transfer information inferred from arbitrary combinations of datasets collected under different conditions -- observational or experimental -- and from heterogeneous environments. In this paper, we investigate the structural causal bandit with transportability, where priors from the source environments are fused to enhance learning in the deployment setting. We demonstrate that it is possible to exploit invariances across environments to consistently improve learning. The resulting bandit algorithm achieves a sub-linear regret bound with an explicit dependence on informativeness of prior data, and it may outperform standard bandit approaches that rely solely on online learning."}
{"id": "2511.17647", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17647", "abs": "https://arxiv.org/abs/2511.17647", "authors": ["Liyuan Deng", "Yunpeng Bai", "Yongkang Dai", "Xiaoshui Huang", "Hongping Gan", "Dongshuo Huang", "Hao jiacheng", "Yilei Shi"], "title": "MamTiff-CAD: Multi-Scale Latent Diffusion with Mamba+ for Complex Parametric Sequence", "comment": "ICCV 2025 Conference", "summary": "Parametric Computer-Aided Design (CAD) is crucial in industrial applications, yet existing approaches often struggle to generate long sequence parametric commands due to complex CAD models' geometric and topological constraints. To address this challenge, we propose MamTiff-CAD, a novel CAD parametric command sequences generation framework that leverages a Transformer-based diffusion model for multi-scale latent representations. Specifically, we design a novel autoencoder that integrates Mamba+ and Transformer, to transfer parameterized CAD sequences into latent representations. The Mamba+ block incorporates a forget gate mechanism to effectively capture long-range dependencies. The non-autoregressive Transformer decoder reconstructs the latent representations. A diffusion model based on multi-scale Transformer is then trained on these latent embeddings to learn the distribution of long sequence commands. In addition, we also construct a dataset that consists of long parametric sequences, which is up to 256 commands for a single CAD model. Experiments demonstrate that MamTiff-CAD achieves state-of-the-art performance on both reconstruction and generation tasks, confirming its effectiveness for long sequence (60-256) CAD model generation."}
{"id": "2511.17963", "categories": ["cs.LG", "cs.AI", "cs.CE", "q-fin.PM"], "pdf": "https://arxiv.org/pdf/2511.17963", "abs": "https://arxiv.org/abs/2511.17963", "authors": ["Jun Kevin", "Pujianto Yugopuspito"], "title": "Hybrid LSTM and PPO Networks for Dynamic Portfolio Optimization", "comment": "12 pages, 8 figures, 2 tables, accepted at 2025 8th Artificial Intelligence and Cloud Computing Conference", "summary": "This paper introduces a hybrid framework for portfolio optimization that fuses Long Short-Term Memory (LSTM) forecasting with a Proximal Policy Optimization (PPO) reinforcement learning strategy. The proposed system leverages the predictive power of deep recurrent networks to capture temporal dependencies, while the PPO agent adaptively refines portfolio allocations in continuous action spaces, allowing the system to anticipate trends while adjusting dynamically to market shifts. Using multi-asset datasets covering U.S. and Indonesian equities, U.S. Treasuries, and major cryptocurrencies from January 2018 to December 2024, the model is evaluated against several baselines, including equal-weight, index-style, and single-model variants (LSTM-only and PPO-only). The framework's performance is benchmarked against equal-weighted, index-based, and single-model approaches (LSTM-only and PPO-only) using annualized return, volatility, Sharpe ratio, and maximum drawdown metrics, each adjusted for transaction costs. The results indicate that the hybrid architecture delivers higher returns and stronger resilience under non-stationary market regimes, suggesting its promise as a robust, AI-driven framework for dynamic portfolio optimization."}
{"id": "2511.17660", "categories": ["cs.LG", "cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.17660", "abs": "https://arxiv.org/abs/2511.17660", "authors": ["Giuseppe Carrino", "Elena Loli Piccolomini", "Elisa Riccietti", "Theo Mary"], "title": "Frugality in second-order optimization: floating-point approximations for Newton's method", "comment": "Master Thesis for the Artificial Intelligence course at University of Bologna", "summary": "Minimizing loss functions is central to machine-learning training. Although first-order methods dominate practical applications, higher-order techniques such as Newton's method can deliver greater accuracy and faster convergence, yet are often avoided due to their computational cost. This work analyzes the impact of finite-precision arithmetic on Newton steps and establishes a convergence theorem for mixed-precision Newton optimizers, including \"quasi\" and \"inexact\" variants. The theorem provides not only convergence guarantees but also a priori estimates of the achievable solution accuracy. Empirical evaluations on standard regression benchmarks demonstrate that the proposed methods outperform Adam on the Australian and MUSH datasets. The second part of the manuscript introduces GN_k, a generalized Gauss-Newton method that enables partial computation of second-order derivatives. GN_k attains performance comparable to full Newton's method on regression tasks while requiring significantly fewer derivative evaluations."}
{"id": "2511.17968", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.17968", "abs": "https://arxiv.org/abs/2511.17968", "authors": ["Oluleke Babayomi", "Dong-Seong Kim"], "title": "Uncertainty-Aware Federated Learning for Cyber-Resilient Microgrid Energy Management", "comment": "6 pages", "summary": "Maintaining economic efficiency and operational reliability in microgrid energy management systems under cyberattack conditions remains challenging. Most approaches assume non-anomalous measurements, make predictions with unquantified uncertainties, and do not mitigate malicious attacks on renewable forecasts for energy management optimization. This paper presents a comprehensive cyber-resilient framework integrating federated Long Short-Term Memory-based photovoltaic forecasting with a novel two-stage cascade false data injection attack detection and energy management system optimization. The approach combines autoencoder reconstruction error with prediction uncertainty quantification to enable attack-resilient energy storage scheduling while preserving data privacy. Extreme false data attack conditions were studied that caused 58% forecast degradation and 16.9\\% operational cost increases. The proposed integrated framework reduced false positive detections by 70%, recovered 93.7% of forecasting performance losses, and achieved 5\\% operational cost savings, mitigating 34.7% of attack-induced economic losses. Results demonstrate that precision-focused cascade detection with multi-signal fusion outperforms single-signal approaches, validating security-performance synergy for decentralized microgrids."}
{"id": "2511.17663", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17663", "abs": "https://arxiv.org/abs/2511.17663", "authors": ["Alex S. C. Maia", "John B. Hall", "Hugo F. M. Milan", "Izabelle A. M. A. Teixeira"], "title": "AI-based framework to predict animal and pen feed intake in feedlot beef cattle", "comment": null, "summary": "Advances in technology are transforming sustainable cattle farming practices, with electronic feeding systems generating big longitudinal datasets on individual animal feed intake, offering the possibility for autonomous precision livestock systems. However, the literature still lacks a methodology that fully leverages these longitudinal big data to accurately predict feed intake accounting for environmental conditions. To fill this gap, we developed an AI-based framework to accurately predict feed intake of individual animals and pen-level aggregation. Data from 19 experiments (>16.5M samples; 2013-2024) conducted at Nancy M. Cummings Research Extension & Education Center (Carmen, ID) feedlot facility and environmental data from AgriMet Network weather stations were used to develop two novel environmental indices: InComfort-Index, based solely on meteorological variables, showed good predictive capability for thermal comfort but had limited ability to predict feed intake; EASI-Index, a hybrid index integrating environmental variables with feed intake behavior, performed well in predicting feed intake but was less effective for thermal comfort. Together with the environmental indices, machine learning models were trained and the best-performing machine learning model (XGBoost) accuracy was RMSE of 1.38 kg/day for animal-level and only 0.14 kg/(day-animal) at pen-level. This approach provides a robust AI-based framework for predicting feed intake in individual animals and pens, with potential applications in precision management of feedlot cattle, through feed waste reduction, resource optimization, and climate-adaptive livestock management."}
{"id": "2511.17970", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17970", "abs": "https://arxiv.org/abs/2511.17970", "authors": ["Mohamed Mabrok", "Yalda Zafari"], "title": "Controllability Analysis of State Space-based Language Model", "comment": null, "summary": "State-space models (SSMs), particularly Mamba, have become powerful architectures for sequence modeling, yet their internal dynamics remain poorly understood compared to attention-based models. We introduce and validate the Influence Score, a controllability-based metric derived from the discretized state-space parameters of Mamba and computed through a backward recurrence analogous to system observability. The score quantifies how strongly a token at position k affects all later states and outputs. We evaluate this measure across three Mamba variants: mamba-130m, mamba-2.8b, and mamba-2.8b-slimpj, using six experiments that test its sensitivity to temperature, prompt complexity, token type, layer depth, token position, and input perturbations. The results show three main insights: (1) the Influence Score increases with model size and training data, reflecting model capacity; (2) Mamba exhibits consistent architectural patterns, including recency bias and concentrated influence in mid-to-late layers; and (3) emergent behaviors appear only at scale, with mamba-2.8b-slimpj uniquely prioritizing content words and reducing internal influence in the presence of noise. These findings establish the Influence Score as a practical diagnostic tool for interpreting and comparing SSM-based language models."}
{"id": "2511.17666", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17666", "abs": "https://arxiv.org/abs/2511.17666", "authors": ["Tom Perel"], "title": "Evaluating Adversarial Vulnerabilities in Modern Large Language Models", "comment": null, "summary": "The recent boom and rapid integration of Large Language Models (LLMs) into a wide range of applications warrants a deeper understanding of their security and safety vulnerabilities. This paper presents a comparative analysis of the susceptibility to jailbreak attacks for two leading publicly available LLMs, Google's Gemini 2.5 Flash and OpenAI's GPT-4 (specifically the GPT-4o mini model accessible in the free tier). The research utilized two main bypass strategies: 'self-bypass', where models were prompted to circumvent their own safety protocols, and 'cross-bypass', where one model generated adversarial prompts to exploit vulnerabilities in the other. Four attack methods were employed - direct injection, role-playing, context manipulation, and obfuscation - to generate five distinct categories of unsafe content: hate speech, illegal activities, malicious code, dangerous content, and misinformation. The success of the attack was determined by the generation of disallowed content, with successful jailbreaks assigned a severity score. The findings indicate a disparity in jailbreak susceptibility between 2.5 Flash and GPT-4, suggesting variations in their safety implementations or architectural design. Cross-bypass attacks were particularly effective, indicating that an ample amount of vulnerabilities exist in the underlying transformer architecture. This research contributes a scalable framework for automated AI red-teaming and provides data-driven insights into the current state of LLM safety, underscoring the complex challenge of balancing model capabilities with robust safety mechanisms."}
{"id": "2511.17978", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.17978", "abs": "https://arxiv.org/abs/2511.17978", "authors": ["Oluleke Babayomi", "Dong-Seong Kim"], "title": "Federated Anomaly Detection and Mitigation for EV Charging Forecasting Under Cyberattacks", "comment": "6 pages", "summary": "Electric Vehicle (EV) charging infrastructure faces escalating cybersecurity threats that can severely compromise operational efficiency and grid stability. Existing forecasting techniques are limited by the lack of combined robust anomaly mitigation solutions and data privacy preservation. Therefore, this paper addresses these challenges by proposing a novel anomaly-resilient federated learning framework that simultaneously preserves data privacy, detects cyber-attacks, and maintains trustworthy demand prediction accuracy under adversarial conditions. The proposed framework integrates three key innovations: LSTM autoencoder-based distributed anomaly detection deployed at each federated client, interpolation-based anomalous data mitigation to preserve temporal continuity, and federated Long Short-Term Memory (LSTM) networks that enable collaborative learning without centralized data aggregation. The framework is validated on real-world EV charging infrastructure datasets combined with real-world DDoS attack datasets, providing robust validation of the proposed approach under realistic threat scenarios. Experimental results demonstrate that the federated approach achieves superior performance compared to centralized models, with 15.2% improvement in R2 accuracy while maintaining data locality. The integrated cyber-attack detection and mitigation system produces trustworthy datasets that enhance prediction reliability, recovering 47.9% of attack-induced performance degradation while maintaining exceptional precision (91.3%) and minimal false positive rates (1.21%). The proposed architecture enables enhanced EV infrastructure planning, privacy-preserving collaborative forecasting, cybersecurity resilience, and rapid recovery from malicious threats across distributed charging networks."}
{"id": "2511.17669", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17669", "abs": "https://arxiv.org/abs/2511.17669", "authors": ["Ashish", "Aparajita Jaiswal", "Sudip Vhaduri", "Niveditha Nerella", "Shubham Jha"], "title": "Empa: An AI-Powered Virtual Mentor for Developing Global Collaboration Skills in HPC Education", "comment": null, "summary": "High-performance computing (HPC) and parallel computing increasingly rely on global collaboration among diverse teams, yet traditional computing curricula inadequately prepare students for cross-cultural teamwork essential in modern computational research environments. This paper presents Empa, an AI-powered virtual mentor that integrates intercultural collaboration training into undergraduate computing education. Built using large language models and deployed through a progressive web application, Empa guides students through structured activities covering cultural dimensions, communication styles, and conflict resolution that are critical for effective multicultural teamwork. Our system addresses the growing need for culturally competent HPC professionals by helping computing students develop skills to collaborate effectively in international research teams, contribute to global computational projects, and navigate the cultural complexities inherent in distributed computing environments. Pilot preparation for deployment in computing courses demonstrates the feasibility of AI-mediated intercultural training and provides insights into scalable approaches for developing intercultural collaboration skills essential for HPC workforce development."}
{"id": "2511.17983", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.17983", "abs": "https://arxiv.org/abs/2511.17983", "authors": ["Naoki Masuyama", "Yuichiro Toda", "Yusuke Nojima", "Hisao Ishibuchi"], "title": "An Adaptive Resonance Theory-based Topological Clustering Algorithm with a Self-Adjusting Vigilance Parameter", "comment": "This manuscript is currently under review", "summary": "Clustering in stationary and nonstationary settings, where data distributions remain static or evolve over time, requires models that can adapt to distributional shifts while preserving previously learned cluster structures. This paper proposes an Adaptive Resonance Theory (ART)-based topological clustering algorithm that autonomously adjusts its recalculation interval and vigilance threshold through a diversity-driven adaptation mechanism. This mechanism enables hyperparameter-free learning that maintains cluster stability and continuity in dynamic environments. Experiments on 24 real-world datasets demonstrate that the proposed algorithm outperforms state-of-the-art methods in both clustering performance and continual learning capability. These results highlight the effectiveness of the proposed parameter adaptation in mitigating catastrophic forgetting and maintaining consistent clustering in evolving data streams. Source code is available at https://github.com/Masuyama-lab/IDAT"}
{"id": "2511.17671", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.17671", "abs": "https://arxiv.org/abs/2511.17671", "authors": ["Atharv Singh Patlan", "Peiyao Sheng", "S. Ashwin Hebbar", "Prateek Mittal", "Pramod Viswanath"], "title": "MURMUR: Using cross-user chatter to break collaborative language agents in groups", "comment": "20 pages, 7 figures", "summary": "Language agents are rapidly expanding from single-user assistants to multi-user collaborators in shared workspaces and groups. However, today's language models lack a mechanism for isolating user interactions and concurrent tasks, creating a new attack vector inherent to this new setting: cross-user poisoning (CUP). In a CUP attack, an adversary injects ordinary-looking messages that poison the persistent, shared state, which later triggers the agent to execute unintended, attacker-specified actions on behalf of benign users. We validate CUP on real systems, successfully attacking popular multi-user agents. To study the phenomenon systematically, we present MURMUR, a framework that composes single-user tasks into concurrent, group-based scenarios using an LLM to generate realistic, history-aware user interactions. We observe that CUP attacks succeed at high rates and their effects persist across multiple tasks, thus posing fundamental risks to multi-user LLM deployments. Finally, we introduce a first-step defense with task-based clustering to mitigate this new class of vulnerability"}
{"id": "2511.17987", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17987", "abs": "https://arxiv.org/abs/2511.17987", "authors": ["Jinping Wang", "Zhiqiang Gao", "Dinggen Zhang", "Zhiwu Xie"], "title": "Escaping Optimization Stagnation: Taking Steps Beyond Task Arithmetic via Difference Vectors", "comment": null, "summary": "Current methods for editing pre-trained models face significant challenges, primarily high computational costs and limited scalability. Task arithmetic has recently emerged as a promising solution, using simple arithmetic operations-addition and negation-based on task vectors which are the differences between fine-tuned and pre-trained model weights, to efficiently modify model behavior. However, the full potential of task arithmetic remains underexplored, primarily due to limited mechanisms for overcoming optimization stagnation. To address this challenge, we introduce the notion of difference vector, a generalized form of task vectors derived from the historical movements during optimization. Using difference vectors as directed perturbations, we propose the Difference Vector-based Anisotropic Scaling Iterative algorithm (DV-BASI) to enable a continuous optimization process for task arithmetic methods without relying on any additional modules or components. Notably, by leveraging escapability and directional advantages of difference vectors, the average performance on different tasks of the multi-task model merged by DV-BASI may even outperform models individually fine-tuned. Based on this observation, we extend the application of difference vectors to a feasible fine-tuning method for single-task models. On the practical side, DV-BASI allows expressive searching directions with few learnable parameters and forms a scalable framework. We also integrate DV-BASI with task arithmetic methods and advanced optimization techniques to achieve state-of-the-art performance on both supervised and unsupervised evaluation protocols."}
{"id": "2511.17678", "categories": ["cs.CY", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.17678", "abs": "https://arxiv.org/abs/2511.17678", "authors": ["Ingo Siegert", "Jan Nehring", "Aranxa Márquez Ampudia", "Matthias Busch", "Stefan Hillmann"], "title": "Chatbots to strengthen democracy: An interdisciplinary seminar to train identifying argumentation techniques of science denial", "comment": "6 pages, 4 figures", "summary": "In recent times, discussions on social media platforms have increasingly come under scrutiny due to the proliferation of science denial and fake news. Traditional solutions, such as regulatory actions, have been implemented to mitigate the spread of misinformation; however, these measures alone are not sufficient. To complement these efforts, educational approaches are becoming essential in empowering users to critically engage with misinformation. Conversation training, through serious games or personalized methods, has emerged as a promising strategy to help users handle science denial and toxic conversation tactics. This paper suggests an interdisciplinary seminar to explore the suitability of Large Language Models (LLMs) acting as a persona of a science denier to support people in identifying misinformation and improving resilience against toxic interactions. In the seminar, groups of four to five students will develop an AI-based chatbot that enables realistic interactions with science-denial argumentation structures. The task involves planning the setting, integrating a Large Language Model to facilitate natural dialogues, implementing the chatbot using the RASA framework, and evaluating the outcomes in a user study. It is crucial that users understand what they need to do during the interaction, how to conclude it, and how the relevant information is conveyed. The seminar does not aim to develop chatbots for practicing debunking but serves to teach AI technologies and test the feasibility of this idea for future applications. The chatbot seminar is conducted as a hybrid, parallel master's module at the participating educational institutions."}
{"id": "2511.17989", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.17989", "abs": "https://arxiv.org/abs/2511.17989", "authors": ["Jiayi Luo", "Qingyun Sun", "Yuecen Wei", "Haonan Yuan", "Xingcheng Fu", "Jianxin Li"], "title": "Privacy Auditing of Multi-domain Graph Pre-trained Model under Membership Inference Attacks", "comment": "Accepted by AAAI 2026(Oral)", "summary": "Multi-domain graph pre-training has emerged as a pivotal technique in developing graph foundation models. While it greatly improves the generalization of graph neural networks, its privacy risks under membership inference attacks (MIAs), which aim to identify whether a specific instance was used in training (member), remain largely unexplored. However, effectively conducting MIAs against multi-domain graph pre-trained models is a significant challenge due to: (i) Enhanced Generalization Capability: Multi-domain pre-training reduces the overfitting characteristics commonly exploited by MIAs. (ii) Unrepresentative Shadow Datasets: Diverse training graphs hinder the obtaining of reliable shadow graphs. (iii) Weakened Membership Signals: Embedding-based outputs offer less informative cues than logits for MIAs. To tackle these challenges, we propose MGP-MIA, a novel framework for Membership Inference Attacks against Multi-domain Graph Pre-trained models. Specifically, we first propose a membership signal amplification mechanism that amplifies the overfitting characteristics of target models via machine unlearning. We then design an incremental shadow model construction mechanism that builds a reliable shadow model with limited shadow graphs via incremental learning. Finally, we introduce a similarity-based inference mechanism that identifies members based on their similarity to positive and negative samples. Extensive experiments demonstrate the effectiveness of our proposed MGP-MIA and reveal the privacy risks of multi-domain graph pre-training."}
{"id": "2511.17682", "categories": ["cs.CY", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.17682", "abs": "https://arxiv.org/abs/2511.17682", "authors": ["Tim Schlippe", "Matthias Wölfel", "Koena Ronny Mabokela"], "title": "A Cross-Cultural Assessment of Human Ability to Detect LLM-Generated Fake News about South Africa", "comment": null, "summary": "This study investigates how cultural proximity affects the ability to detect AI-generated fake news by comparing South African participants with those from other nationalities. As large language models increasingly enable the creation of sophisticated fake news, understanding human detection capabilities becomes crucial, particularly across different cultural contexts. We conducted a survey where 89 participants (56 South Africans, 33 from other nationalities) evaluated 10 true South African news articles and 10 AI-generated fake versions. Results reveal an asymmetric pattern: South Africans demonstrated superior performance in detecting true news about their country (40% deviation from ideal rating) compared to other participants (52%), but performed worse at identifying fake news (62% vs. 55%). This difference may reflect South Africans' higher overall trust in news sources. Our analysis further shows that South Africans relied more on content knowledge and contextual understanding when judging credibility, while participants from other countries emphasised formal linguistic features such as grammar and structure. Overall, the deviation from ideal rating was similar between groups (51% vs. 53%), suggesting that cultural familiarity appears to aid verification of authentic information but may also introduce bias when evaluating fabricated content. These insights contribute to understanding cross-cultural dimensions of misinformation detection and inform strategies for combating AI-generated fake news in increasingly globalised information ecosystems where content crosses cultural and geographical boundaries."}
{"id": "2511.17994", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.17994", "abs": "https://arxiv.org/abs/2511.17994", "authors": ["Nikita P. Kalinin", "Joel Daniel Andersson"], "title": "Learning Rate Scheduling with Matrix Factorization for Private Training", "comment": null, "summary": "We study differentially private model training with stochastic gradient descent under learning rate scheduling and correlated noise. Although correlated noise, in particular via matrix factorizations, has been shown to improve accuracy, prior theoretical work focused primarily on the prefix-sum workload. That workload assumes a constant learning rate, whereas in practice learning rate schedules are widely used to accelerate training and improve convergence. We close this gap by deriving general upper and lower bounds for a broad class of learning rate schedules in both single- and multi-epoch settings. Building on these results, we propose a learning-rate-aware factorization that achieves improvements over prefix-sum factorizations under both MaxSE and MeanSE error metrics. Our theoretical analysis yields memory-efficient constructions suitable for practical deployment, and experiments on CIFAR-10 and IMDB datasets confirm that schedule-aware factorizations improve accuracy in private training."}
{"id": "2511.17683", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17683", "abs": "https://arxiv.org/abs/2511.17683", "authors": ["Lara Hassan", "Mohamed ElZeftawy", "Abdulrahman Mahmoud"], "title": "Datacenters in the Desert: Feasibility and Sustainability of LLM Inference in the Middle East", "comment": "3 pages, 1 figure", "summary": "As the Middle East emerges as a strategic hub for artificial intelligence (AI) infrastructure, the feasibility of deploying sustainable datacenters in desert environments has become a topic of growing relevance. This paper presents an empirical study analyzing the energy consumption and carbon footprint of large language model (LLM) inference across four countries: the United Arab Emirates, Iceland, Germany, and the United States of America using DeepSeek Coder 1.3B and the HumanEval dataset on the task of code generation. We use the CodeCarbon library to track energy and carbon emissions andcompare geographical trade-offs for climate-aware AI deployment. Our findings highlight both the challenges and potential of datacenters in desert regions and provide a balanced outlook on their role in global AI expansion."}
{"id": "2511.18000", "categories": ["cs.LG", "cs.AI", "q-bio.PE"], "pdf": "https://arxiv.org/pdf/2511.18000", "abs": "https://arxiv.org/abs/2511.18000", "authors": ["Radman Rakhshandehroo", "Daniel Coombs"], "title": "Reward Engineering for Spatial Epidemic Simulations: A Reinforcement Learning Platform for Individual Behavioral Learning", "comment": "35 pages, 15 figures and 14 tables", "summary": "We present ContagionRL, a Gymnasium-compatible reinforcement learning platform specifically designed for systematic reward engineering in spatial epidemic simulations. Unlike traditional agent-based models that rely on fixed behavioral rules, our platform enables rigorous evaluation of how reward function design affects learned survival strategies across diverse epidemic scenarios. ContagionRL integrates a spatial SIRS+D epidemiological model with configurable environmental parameters, allowing researchers to stress-test reward functions under varying conditions including limited observability, different movement patterns, and heterogeneous population dynamics. We evaluate five distinct reward designs, ranging from sparse survival bonuses to a novel potential field approach, across multiple RL algorithms (PPO, SAC, A2C). Through systematic ablation studies, we identify that directional guidance and explicit adherence incentives are critical components for robust policy learning. Our comprehensive evaluation across varying infection rates, grid sizes, visibility constraints, and movement patterns reveals that reward function choice dramatically impacts agent behavior and survival outcomes. Agents trained with our potential field reward consistently achieve superior performance, learning maximal adherence to non-pharmaceutical interventions while developing sophisticated spatial avoidance strategies. The platform's modular design enables systematic exploration of reward-behavior relationships, addressing a knowledge gap in models of this type where reward engineering has received limited attention. ContagionRL is an effective platform for studying adaptive behavioral responses in epidemic contexts and highlight the importance of reward design, information structure, and environmental predictability in learning."}
{"id": "2511.17688", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17688", "abs": "https://arxiv.org/abs/2511.17688", "authors": ["Quan Liu", "Feng Ye", "Chenhao Lu", "Shuming Zhen", "Guanliang Huang", "Lunzhe Chen", "Xudong Ke"], "title": "Enhancing Adversarial Transferability through Block Stretch and Shrink", "comment": "code will be releace", "summary": "Adversarial attacks introduce small, deliberately crafted perturbations that mislead neural networks, and their transferability from white-box to black-box target models remains a critical research focus. Input transformation-based attacks are a subfield of adversarial attacks that enhance input diversity through input transformations to improve the transferability of adversarial examples. However, existing input transformation-based attacks tend to exhibit limited cross-model transferability. Previous studies have shown that high transferability is associated with diverse attention heatmaps and the preservation of global semantics in transformed inputs. Motivated by this observation, we propose Block Stretch and Shrink (BSS), a method that divides an image into blocks and applies stretch and shrink operations to these blocks, thereby diversifying attention heatmaps in transformed inputs while maintaining their global semantics. Empirical evaluations on a subset of ImageNet demonstrate that BSS outperforms existing input transformation-based attack methods in terms of transferability. Furthermore, we examine the impact of the number scale, defined as the number of transformed inputs, in input transformation-based attacks, and advocate evaluating these methods under a unified number scale to enable fair and comparable assessments."}
{"id": "2511.18006", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18006", "abs": "https://arxiv.org/abs/2511.18006", "authors": ["Meng Ding", "Mingxi Lei", "Shaopeng Fu", "Shaowei Wang", "Di Wang", "Jinhui Xu"], "title": "Understanding Private Learning From Feature Perspective", "comment": "39pages", "summary": "Differentially private Stochastic Gradient Descent (DP-SGD) has become integral to privacy-preserving machine learning, ensuring robust privacy guarantees in sensitive domains. Despite notable empirical advances leveraging features from non-private, pre-trained models to enhance DP-SGD training, a theoretical understanding of feature dynamics in private learning remains underexplored. This paper presents the first theoretical framework to analyze private training through a feature learning perspective. Building on the multi-patch data structure from prior work, our analysis distinguishes between label-dependent feature signals and label-independent noise, a critical aspect overlooked by existing analyses in the DP community. Employing a two-layer CNN with polynomial ReLU activation, we theoretically characterize both feature signal learning and data noise memorization in private training via noisy gradient descent. Our findings reveal that (1) Effective private signal learning requires a higher signal-to-noise ratio (SNR) compared to non-private training, and (2) When data noise memorization occurs in non-private learning, it will also occur in private learning, leading to poor generalization despite small training loss. Our findings highlight the challenges of private learning and prove the benefit of feature enhancement to improve SNR. Experiments on synthetic and real-world datasets also validate our theoretical findings."}
{"id": "2511.17696", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17696", "abs": "https://arxiv.org/abs/2511.17696", "authors": ["Douglas C. Schmidt", "Dan Runfola"], "title": "Liberating Logic in the Age of AI: Going Beyond Programming with Computational Thinking", "comment": "15 pages and 17 figures", "summary": "Mastering one or more programming languages has historically been the gateway to implementing ideas on a computer. Today, that gateway is widening with advances in large language models (LLMs) and artificial intelligence (AI)-powered coding assistants. What matters is no longer just fluency in traditional programming languages but the ability to think computationally by translating problems into forms that can be solved with computing tools. The capabilities enabled by these AI-augmented tools are rapidly leading to the commoditization of computational thinking, such that anyone who can articulate a problem in natural language can potentially harness computing power via AI.\n  This shift is poised to radically influence how we teach computer science and data science in the United States and around the world. Educators and industry leaders are grappling with how to adapt: What should students learn when the hottest new programming language is English? How do we prepare a generation of computational thinkers who need not code every algorithm manually, but must still think critically, design solutions, and verify AI-augmented results?\n  This paper explores these questions, examining the impact of natural language programming on software development, the emerging distinction between programmers and prompt-crafting problem solvers, the reforms needed in computer science and data science curricula, and the importance of maintaining our fundamental computational science principles in an AI-augmented future. Along the way, we compare approaches and share best practices for embracing this new paradigm in computing education."}
{"id": "2511.18039", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18039", "abs": "https://arxiv.org/abs/2511.18039", "authors": ["Thong Bach", "Thanh Nguyen-Tang", "Dung Nguyen", "Thao Minh Le", "Truyen Tran"], "title": "Curvature-Aware Safety Restoration In LLMs Fine-Tuning", "comment": "19 pages, 10 figures", "summary": "Fine-tuning Large Language Models (LLMs) for downstream tasks often compromises safety alignment, even when using parameter-efficient methods like LoRA. In this work, we uncover a notable property: fine-tuned models preserve the geometric structure of their loss landscapes concerning harmful content, regardless of the fine-tuning method employed. This suggests that safety behaviors are not erased but shifted to less influential regions of the parameter space. Building on this insight, we propose a curvature-aware alignment restoration method that leverages influence functions and second-order optimization to selectively increase loss on harmful inputs while preserving task performance. By navigating the shared geometry between base and fine-tuned models, our method discourages unsafe outputs while preserving task-relevant performance, avoiding full reversion and enabling precise, low-impact updates. Extensive evaluations across multiple model families and adversarial settings show that our approach efficiently reduces harmful responses while maintaining or even improving utility and few-shot learning performance."}
{"id": "2511.17818", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17818", "abs": "https://arxiv.org/abs/2511.17818", "authors": ["Aishwarya Mandyam", "Kalyani Limaye", "Barbara E. Engelhardt", "Emily Alsentzer"], "title": "APRIL: Annotations for Policy evaluation with Reliable Inference from LLMs", "comment": null, "summary": "Off-policy evaluation (OPE) estimates the value of a contextual bandit policy prior to deployment. As such, OPE plays a critical role in ensuring safety in high-stakes domains such as healthcare. However, standard OPE approaches are limited by the size and coverage of the behavior dataset. While previous work has explored using expert-labeled counterfactual annotations to enhance dataset coverage, obtaining such annotations is expensive, limiting the scalability of prior approaches. We propose leveraging large language models (LLMs) to generate counterfactual annotations for OPE in medical domains. Our method uses domain knowledge to guide LLMs in predicting how key clinical features evolve under alternate treatments. These predicted features can then be transformed using known reward functions to create counterfactual annotations. We first evaluate the ability of several LLMs to predict clinical features across two patient subsets in MIMIC-IV, finding that state-of-the-art LLMs achieve comparable performance. Building on this capacity to predict clinical features, we generate LLM-based counterfactual annotations and incorporate them into an OPE estimator. Our empirical results analyze the benefits of counterfactual annotations under varying degrees of shift between the behavior and target policies. We find that in most cases, the LLM-based counterfactual annotations significantly improve OPE estimates up to a point. We provide an entropy-based metric to identify when additional annotations cease to be useful. Our results demonstrate that LLM-based counterfactual annotations offer a scalable approach for addressing coverage limitations in healthcare datasets, enabling safer deployment of decision-making policies in clinical settings."}
{"id": "2511.18056", "categories": ["cs.LG", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.18056", "abs": "https://arxiv.org/abs/2511.18056", "authors": ["Maximilien Dreveton", "Matthias Grossglauser", "Daichi Kuroda", "Patrick Thiran"], "title": "Hierarchical Linkage Clustering Beyond Binary Trees and Ultrametrics", "comment": null, "summary": "Hierarchical clustering seeks to uncover nested structures in data by constructing a tree of clusters, where deeper levels reveal finer-grained relationships. Traditional methods, including linkage approaches, face three major limitations: (i) they always return a hierarchy, even if none exists, (ii) they are restricted to binary trees, even if the true hierarchy is non-binary, and (iii) they are highly sensitive to the choice of linkage function. In this paper, we address these issues by introducing the notion of a valid hierarchy and defining a partial order over the set of valid hierarchies. We prove the existence of a finest valid hierarchy, that is, the hierarchy that encodes the maximum information consistent with the similarity structure of the data set. In particular, the finest valid hierarchy is not constrained to binary structures and, when no hierarchical relationships exist, collapses to a star tree. We propose a simple two-step algorithm that first constructs a binary tree via a linkage method and then prunes it to enforce validity. We establish necessary and sufficient conditions on the linkage function under which this procedure exactly recovers the finest valid hierarchy, and we show that all linkage functions satisfying these conditions yield the same hierarchy after pruning. Notably, classical linkage rules such as single, complete, and average satisfy these conditions, whereas Ward's linkage fails to do so."}
{"id": "2511.17829", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17829", "abs": "https://arxiv.org/abs/2511.17829", "authors": ["Akhil Singampalli", "Sudeep Pasricha"], "title": "Unified Class and Domain Incremental Learning with Mixture of Experts for Indoor Localization", "comment": null, "summary": "Indoor localization using machine learning has gained traction due to the growing demand for location-based services. However, its long-term reliability is hindered by hardware/software variations across mobile devices, which shift the model's input distribution to create domain shifts. Further, evolving indoor environments can introduce new locations over time, expanding the output space to create class shifts, making static machine learning models ineffective over time. To address these challenges, we propose a novel unified continual learning framework for indoor localization called MOELO that, for the first time, jointly addresses domain-incremental and class-incremental learning scenarios. MOELO enables a lightweight, robust, and adaptive localization solution that can be deployed on resource-limited mobile devices and is capable of continual learning in dynamic, heterogeneous real-world settings. This is made possible by a mixture-of-experts architecture, where experts are incrementally trained per region and selected through an equiangular tight frame based gating mechanism ensuring efficient routing, and low-latency inference, all within a compact model footprint. Experimental evaluations show that MOELO achieves improvements of up to 25.6x in mean localization error, 44.5x in worst-case localization error, and 21.5x lesser forgetting compared to state-of-the-art frameworks across diverse buildings, mobile devices, and learning scenarios."}
{"id": "2511.18066", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18066", "abs": "https://arxiv.org/abs/2511.18066", "authors": ["Md Akil Raihan Iftee", "Syed Md. Ahnaf Hasan", "Mir Sazzat Hossain", "Rakibul Hasan Rajib", "Amin Ahsan Ali", "AKM Mahbubur Rahman", "Sajib Mistry", "Monowar Bhuyan"], "title": "pFedBBN: A Personalized Federated Test-Time Adaptation with Balanced Batch Normalization for Class-Imbalanced Data", "comment": "25 pages, 7 tables, 21 figures", "summary": "Test-time adaptation (TTA) in federated learning (FL) is crucial for handling unseen data distributions across clients, particularly when faced with domain shifts and skewed class distributions. Class Imbalance (CI) remains a fundamental challenge in FL, where rare but critical classes are often severely underrepresented in individual client datasets. Although prior work has addressed CI during training through reliable aggregation and local class distribution alignment, these methods typically rely on access to labeled data or coordination among clients, and none address class unsupervised adaptation to dynamic domains or distribution shifts at inference time under federated CI constraints. Revealing the failure of state-of-the-art TTA in federated client adaptation in CI scenario, we propose pFedBBN,a personalized federated test-time adaptation framework that employs balanced batch normalization (BBN) during local client adaptation to mitigate prediction bias by treating all classes equally, while also enabling client collaboration guided by BBN similarity, ensuring that clients with similar balanced representations reinforce each other and that adaptation remains aligned with domain-specific characteristics. pFedBBN supports fully unsupervised local adaptation and introduces a class-aware model aggregation strategy that enables personalized inference without compromising privacy. It addresses both distribution shifts and class imbalance through balanced feature normalization and domain-aware collaboration, without requiring any labeled or raw data from clients. Extensive experiments across diverse baselines show that pFedBBN consistently enhances robustness and minority-class performance over state-of-the-art FL and TTA methods."}
{"id": "2511.17853", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17853", "abs": "https://arxiv.org/abs/2511.17853", "authors": ["SunMin Moon", "Jangwon Gim", "Chaerin Kim", "Yeeun Kim", "YoungJoo Kim", "Kang Choi"], "title": "A Low-Code Methodology for Developing AI Kiosks: a Case Study with the DIZEST Platform", "comment": "5 pages, 2 figures, conference, 2 tables", "summary": "This paper presents a comprehensive study on enhancing kiosk systems through a low-code architecture, with a focus on AI-based implementations. Modern kiosk systems are confronted with significant challenges, including a lack of integration, structural rigidity, performance bottlenecks, and the absence of collaborative frameworks. To overcome these limitations, we propose a DIZEST-based approach methodology, a specialized low-code platform that enables intuitive workflow design and seamless AI integration. Through a comparative analysis with existing platforms, including Jupyter Notebook, ComfyUI, and Orange3, we demonstrate that DIZEST delivers superior performance across key evaluation criteria. Our photo kiosk case study further validates the effectiveness of this approach in improving interoperability, enhancing user experience, and increasing deployment flexibility."}
{"id": "2511.18084", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18084", "abs": "https://arxiv.org/abs/2511.18084", "authors": ["Dou Liu", "Ying Long", "Sophia Zuoqiu", "Kaipeng Xie", "Runze Yang", "Di Liu", "Kang Li", "Yiting Lin", "Hanyi Liu", "Rong Yin", "Tian Tang"], "title": "The Alignment Paradox of Medical Large Language Models in Infertility Care: Decoupling Algorithmic Improvement from Clinical Decision-making Quality", "comment": "22 pages 5 figures", "summary": "Large language models (LLMs) are increasingly adopted in clinical decision support, yet aligning them with the multifaceted reasoning pathways of real-world medicine remains a major challenge. Using more than 8,000 infertility treatment records, we systematically evaluate four alignment strategies: Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), Group Relative Policy Optimization (GRPO), and In-Context Learning (ICL) through a dual-layer framework combining automatic benchmarks with blinded doctor-in-the-loop assessments. GRPO achieves the highest algorithmic accuracy across multiple decision layers, confirming the value of reinforcement-based optimization for structured prediction tasks. However, clinicians consistently prefer the SFT model, citing clearer reasoning processes (p = 0.035) and higher therapeutic feasibility (p = 0.019). In blinded pairwise comparisons, SFT attains the highest winning rate (51.2%), outperforming both GRPO (26.2%) and even physicians' original decisions (22.7%). These results reveal an alignment paradox: algorithmic improvements do not necessarily translate into higher clinical trust, and may diverge from human-centered preferences. Our findings highlight the need for alignment strategies that prioritize clinically interpretable and practically feasible reasoning, rather than solely optimizing decision-level accuracy."}
{"id": "2511.17902", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.17902", "abs": "https://arxiv.org/abs/2511.17902", "authors": ["Yifan He", "Haodong Zhang", "Qiuheng Song", "Lin Lei", "Zhenxuan Zeng", "Haoyang He", "Hongyan Wu"], "title": "Statistically-Guided Dual-Domain Meta-Learning with Adaptive Multi-Prototype Aggregation for Distributed Fiber Optic Sensing", "comment": null, "summary": "Distributed Fiber Optic Sensing (DFOS) has shown strong potential in perimeter security due to its capability of monitoring vibration events across long distances with fine spatial resolution. However, practical DFOS systems face three critical challenges: (1) signal patterns of the same activity vary drastically under different fiber deployment types (e.g., underground, wall-mounted), causing domain shift; (2) labeled data in new deployment scenarios is often scarce or entirely unavailable, limiting model adaptability; and (3) even within source domains, data scarcity makes it difficult to capture intra-class diversity for robust learning.\n  To address these challenges, we propose a novel meta-learning framework, DUPLE, for cross-deployment DFOS activity identification. First, a dual-domain multi-prototype learner fuses temporal and frequency domain features, enhancing the model's generalization ability under signal distribution shifts. Second, a Statistical Guided Network (SGN) infers domain importance and prototype sensitivity from raw statistical features, providing data-driven prior information for learning in unlabeled or unseen domains. Third, a query-aware prototype aggregation module adaptively selects and combines relevant prototypes, thereby improving classification performance even with limited data.\n  Extensive experiments on cross-deployment DFOS datasets demonstrate that our method significantly outperforms baseline approaches in domain generalization settings, enabling robust event recognition across diverse fiber configurations with minimal labeled data."}
{"id": "2511.18093", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18093", "abs": "https://arxiv.org/abs/2511.18093", "authors": ["Fulong Yao", "Wanqing Zhao", "Matthew Forshaw"], "title": "A New Error Temporal Difference Algorithm for Deep Reinforcement Learning in Microgrid Optimization", "comment": "Have been accepted by 2024 9th International Conference on Renewable Energy and Conservation (ICREC 2024)", "summary": "Predictive control approaches based on deep reinforcement learning (DRL) have gained significant attention in microgrid energy optimization. However, existing research often overlooks the issue of uncertainty stemming from imperfect prediction models, which can lead to suboptimal control strategies. This paper presents a new error temporal difference (ETD) algorithm for DRL to address the uncertainty in predictions,aiming to improve the performance of microgrid operations. First,a microgrid system integrated with renewable energy sources (RES) and energy storage systems (ESS), along with its Markov decision process (MDP), is modelled. Second, a predictive control approach based on a deep Q network (DQN) is presented, in which a weighted average algorithm and a new ETD algorithm are designed to quantify and address the prediction uncertainty, respectively. Finally, simulations on a realworld US dataset suggest that the developed ETD effectively improves the performance of DRL in optimizing microgrid operations."}
{"id": "2511.17959", "categories": ["cs.CR", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17959", "abs": "https://arxiv.org/abs/2511.17959", "authors": ["Yuhao Wu", "Ke Yang", "Franziska Roesner", "Tadayoshi Kohno", "Ning Zhang", "Umar Iqbal"], "title": "Towards Automating Data Access Permissions in AI Agents", "comment": "Accepted by the IEEE Symposium on Security and Privacy (S&P) 2026", "summary": "As AI agents attempt to autonomously act on users' behalf, they raise transparency and control issues. We argue that permission-based access control is indispensable in providing meaningful control to the users, but conventional permission models are inadequate for the automated agentic execution paradigm. We therefore propose automated permission management for AI agents. Our key idea is to conduct a user study to identify the factors influencing users' permission decisions and to encode these factors into an ML-based permission management assistant capable of predicting users' future decisions. We find that participants' permission decisions are influenced by communication context but importantly individual preferences tend to remain consistent within contexts, and align with those of other participants. Leveraging these insights, we develop a permission prediction model achieving 85.1% accuracy overall and 94.4% for high-confidence predictions. We find that even without using permission history, our model achieves an accuracy of 66.9%, and a slight increase of training samples (i.e., 1-4) can substantially increase the accuracy by 10.8%."}
{"id": "2511.18107", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.18107", "abs": "https://arxiv.org/abs/2511.18107", "authors": ["Yegon Kim", "Hyunsu Kim", "Gyeonghoon Ko", "Juho Lee"], "title": "Active Learning with Selective Time-Step Acquisition for PDEs", "comment": null, "summary": "Accurately solving partial differential equations (PDEs) is critical to understanding complex scientific and engineering phenomena, yet traditional numerical solvers are computationally expensive. Surrogate models offer a more efficient alternative, but their development is hindered by the cost of generating sufficient training data from numerical solvers. In this paper, we present a novel framework for active learning (AL) in PDE surrogate modeling that reduces this cost. Unlike the existing AL methods for PDEs that always acquire entire PDE trajectories, our approach strategically generates only the most important time steps with the numerical solver, while employing the surrogate model to approximate the remaining steps. This dramatically reduces the cost incurred by each trajectory and thus allows the active learning algorithm to try out a more diverse set of trajectories given the same budget. To accommodate this novel framework, we develop an acquisition function that estimates the utility of a set of time steps by approximating its resulting variance reduction. We demonstrate the effectiveness of our method on several benchmark PDEs, including the Burgers' equation, Korteweg-De Vries equation, Kuramoto-Sivashinsky equation, the incompressible Navier-Stokes equation, and the compressible Navier-Stokes equation. Experiments show that our approach improves performance by large margins over the best existing method. Our method not only reduces average error but also the 99\\%, 95\\%, and 50\\% quantiles of error, which is rare for an AL algorithm. All in all, our approach offers a data-efficient solution to surrogate modeling for PDEs."}
{"id": "2511.17963", "categories": ["cs.LG", "cs.AI", "cs.CE", "q-fin.PM"], "pdf": "https://arxiv.org/pdf/2511.17963", "abs": "https://arxiv.org/abs/2511.17963", "authors": ["Jun Kevin", "Pujianto Yugopuspito"], "title": "Hybrid LSTM and PPO Networks for Dynamic Portfolio Optimization", "comment": "12 pages, 8 figures, 2 tables, accepted at 2025 8th Artificial Intelligence and Cloud Computing Conference", "summary": "This paper introduces a hybrid framework for portfolio optimization that fuses Long Short-Term Memory (LSTM) forecasting with a Proximal Policy Optimization (PPO) reinforcement learning strategy. The proposed system leverages the predictive power of deep recurrent networks to capture temporal dependencies, while the PPO agent adaptively refines portfolio allocations in continuous action spaces, allowing the system to anticipate trends while adjusting dynamically to market shifts. Using multi-asset datasets covering U.S. and Indonesian equities, U.S. Treasuries, and major cryptocurrencies from January 2018 to December 2024, the model is evaluated against several baselines, including equal-weight, index-style, and single-model variants (LSTM-only and PPO-only). The framework's performance is benchmarked against equal-weighted, index-based, and single-model approaches (LSTM-only and PPO-only) using annualized return, volatility, Sharpe ratio, and maximum drawdown metrics, each adjusted for transaction costs. The results indicate that the hybrid architecture delivers higher returns and stronger resilience under non-stationary market regimes, suggesting its promise as a robust, AI-driven framework for dynamic portfolio optimization."}
{"id": "2511.18138", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.18138", "abs": "https://arxiv.org/abs/2511.18138", "authors": ["Junrui Zhang", "Xinyu Zhao", "Jie Peng", "Chenjie Wang", "Jianmin Ji", "Tianlong Chen"], "title": "Vulnerability-Aware Robust Multimodal Adversarial Training", "comment": "Accepted by AAAI26", "summary": "Multimodal learning has shown significant superiority on various tasks by integrating multiple modalities. However, the interdependencies among modalities increase the susceptibility of multimodal models to adversarial attacks. Existing methods mainly focus on attacks on specific modalities or indiscriminately attack all modalities. In this paper, we find that these approaches ignore the differences between modalities in their contribution to final robustness, resulting in suboptimal robustness performance. To bridge this gap, we introduce Vulnerability-Aware Robust Multimodal Adversarial Training (VARMAT), a probe-in-training adversarial training method that improves multimodal robustness by identifying the vulnerability of each modality. To be specific, VARMAT first explicitly quantifies the vulnerability of each modality, grounded in a first-order approximation of the attack objective (Probe). Then, we propose a targeted regularization term that penalizes modalities with high vulnerability, guiding robust learning while maintaining task accuracy (Training). We demonstrate the enhanced robustness of our method across multiple multimodal datasets involving diverse modalities. Finally, we achieve {12.73%, 22.21%, 11.19%} robustness improvement on three multimodal datasets, revealing a significant blind spot in multimodal adversarial training."}
{"id": "2511.17982", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17982", "abs": "https://arxiv.org/abs/2511.17982", "authors": ["Jiayi Luo", "Qingyun Sun", "Lingjuan Lyu", "Ziwei Zhang", "Haonan Yuan", "Xingcheng Fu", "Jianxin Li"], "title": "Towards Effective, Stealthy, and Persistent Backdoor Attacks Targeting Graph Foundation Models", "comment": "Accepted by AAAI 2026", "summary": "Graph Foundation Models (GFMs) are pre-trained on diverse source domains and adapted to unseen targets, enabling broad generalization for graph machine learning. Despite that GFMs have attracted considerable attention recently, their vulnerability to backdoor attacks remains largely underexplored. A compromised GFM can introduce backdoor behaviors into downstream applications, posing serious security risks. However, launching backdoor attacks against GFMs is non-trivial due to three key challenges. (1) Effectiveness: Attackers lack knowledge of the downstream task during pre-training, complicating the assurance that triggers reliably induce misclassifications into desired classes. (2) Stealthiness: The variability in node features across domains complicates trigger insertion that remains stealthy. (3) Persistence: Downstream fine-tuning may erase backdoor behaviors by updating model parameters. To address these challenges, we propose GFM-BA, a novel Backdoor Attack model against Graph Foundation Models. Specifically, we first design a label-free trigger association module that links the trigger to a set of prototype embeddings, eliminating the need for knowledge about downstream tasks to perform backdoor injection. Then, we introduce a node-adaptive trigger generator, dynamically producing node-specific triggers, reducing the risk of trigger detection while reliably activating the backdoor. Lastly, we develop a persistent backdoor anchoring module that firmly anchors the backdoor to fine-tuning-insensitive parameters, enhancing the persistence of the backdoor under downstream adaptation. Extensive experiments demonstrate the effectiveness, stealthiness, and persistence of GFM-BA."}
{"id": "2511.18150", "categories": ["cs.LG", "cs.AI", "math.CO"], "pdf": "https://arxiv.org/pdf/2511.18150", "abs": "https://arxiv.org/abs/2511.18150", "authors": ["Randy Davila", "Beyzanur Ispir"], "title": "Graph Neural Networks vs Convolutional Neural Networks for Graph Domination Number Prediction", "comment": null, "summary": "We investigate machine learning approaches to approximating the \\emph{domination number} of graphs, the minimum size of a dominating set. Exact computation of this parameter is NP-hard, restricting classical methods to small instances. We compare two neural paradigms: Convolutional Neural Networks (CNNs), which operate on adjacency matrix representations, and Graph Neural Networks (GNNs), which learn directly from graph structure through message passing. Across 2,000 random graphs with up to 64 vertices, GNNs achieve markedly higher accuracy ($R^2=0.987$, MAE $=0.372$) than CNNs ($R^2=0.955$, MAE $=0.500$). Both models offer substantial speedups over exact solvers, with GNNs delivering more than $200\\times$ acceleration while retaining near-perfect fidelity. Our results position GNNs as a practical surrogate for combinatorial graph invariants, with implications for scalable graph optimization and mathematical discovery."}
{"id": "2511.17987", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17987", "abs": "https://arxiv.org/abs/2511.17987", "authors": ["Jinping Wang", "Zhiqiang Gao", "Dinggen Zhang", "Zhiwu Xie"], "title": "Escaping Optimization Stagnation: Taking Steps Beyond Task Arithmetic via Difference Vectors", "comment": null, "summary": "Current methods for editing pre-trained models face significant challenges, primarily high computational costs and limited scalability. Task arithmetic has recently emerged as a promising solution, using simple arithmetic operations-addition and negation-based on task vectors which are the differences between fine-tuned and pre-trained model weights, to efficiently modify model behavior. However, the full potential of task arithmetic remains underexplored, primarily due to limited mechanisms for overcoming optimization stagnation. To address this challenge, we introduce the notion of difference vector, a generalized form of task vectors derived from the historical movements during optimization. Using difference vectors as directed perturbations, we propose the Difference Vector-based Anisotropic Scaling Iterative algorithm (DV-BASI) to enable a continuous optimization process for task arithmetic methods without relying on any additional modules or components. Notably, by leveraging escapability and directional advantages of difference vectors, the average performance on different tasks of the multi-task model merged by DV-BASI may even outperform models individually fine-tuned. Based on this observation, we extend the application of difference vectors to a feasible fine-tuning method for single-task models. On the practical side, DV-BASI allows expressive searching directions with few learnable parameters and forms a scalable framework. We also integrate DV-BASI with task arithmetic methods and advanced optimization techniques to achieve state-of-the-art performance on both supervised and unsupervised evaluation protocols."}
{"id": "2511.18157", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18157", "abs": "https://arxiv.org/abs/2511.18157", "authors": ["Martin Schuck", "Alexander von Rohr", "Angela P. Schoellig"], "title": "scipy.spatial.transform: Differentiable Framework-Agnostic 3D Transformations in Python", "comment": "Accepted as oral at the 1st Workshop on Differentiable Systems and Scientific Machine Learning @ EurIPS 2025", "summary": "Three-dimensional rigid-body transforms, i.e. rotations and translations, are central to modern differentiable machine learning pipelines in robotics, vision, and simulation. However, numerically robust and mathematically correct implementations, particularly on SO(3), are error-prone due to issues such as axis conventions, normalizations, composition consistency and subtle errors that only appear in edge cases. SciPy's spatial.transform module is a rigorously tested Python implementation. However, it historically only supported NumPy, limiting adoption in GPU-accelerated and autodiff-based workflows. We present a complete overhaul of SciPy's spatial.transform functionality that makes it compatible with any array library implementing the Python array API, including JAX, PyTorch, and CuPy. The revised implementation preserves the established SciPy interface while enabling GPU/TPU execution, JIT compilation, vectorized batching, and differentiation via native autodiff of the chosen backend. We demonstrate how this foundation supports differentiable scientific computing through two case studies: (i) scalability of 3D transforms and rotations and (ii) a JAX drone simulation that leverages SciPy's Rotation for accurate integration of rotational dynamics. Our contributions have been merged into SciPy main and will ship in the next release, providing a framework-agnostic, production-grade basis for 3D spatial math in differentiable systems and ML."}
{"id": "2511.17989", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.17989", "abs": "https://arxiv.org/abs/2511.17989", "authors": ["Jiayi Luo", "Qingyun Sun", "Yuecen Wei", "Haonan Yuan", "Xingcheng Fu", "Jianxin Li"], "title": "Privacy Auditing of Multi-domain Graph Pre-trained Model under Membership Inference Attacks", "comment": "Accepted by AAAI 2026(Oral)", "summary": "Multi-domain graph pre-training has emerged as a pivotal technique in developing graph foundation models. While it greatly improves the generalization of graph neural networks, its privacy risks under membership inference attacks (MIAs), which aim to identify whether a specific instance was used in training (member), remain largely unexplored. However, effectively conducting MIAs against multi-domain graph pre-trained models is a significant challenge due to: (i) Enhanced Generalization Capability: Multi-domain pre-training reduces the overfitting characteristics commonly exploited by MIAs. (ii) Unrepresentative Shadow Datasets: Diverse training graphs hinder the obtaining of reliable shadow graphs. (iii) Weakened Membership Signals: Embedding-based outputs offer less informative cues than logits for MIAs. To tackle these challenges, we propose MGP-MIA, a novel framework for Membership Inference Attacks against Multi-domain Graph Pre-trained models. Specifically, we first propose a membership signal amplification mechanism that amplifies the overfitting characteristics of target models via machine unlearning. We then design an incremental shadow model construction mechanism that builds a reliable shadow model with limited shadow graphs via incremental learning. Finally, we introduce a similarity-based inference mechanism that identifies members based on their similarity to positive and negative samples. Extensive experiments demonstrate the effectiveness of our proposed MGP-MIA and reveal the privacy risks of multi-domain graph pre-training."}
{"id": "2511.18158", "categories": ["cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.18158", "abs": "https://arxiv.org/abs/2511.18158", "authors": ["Abdelrahman Abdelmotlb", "Abdallah Taman", "Sherif Mostafa", "Moustafa Youssef"], "title": "LocaGen: Low-Overhead Indoor Localization Through Spatial Augmentation", "comment": "Accepted at GeoIndustry @ ACM SIGSPATIAL 2025 (The 4th International Workshop on Spatial Big Data and AI for Industrial Applications)", "summary": "Indoor localization systems commonly rely on fingerprinting, which requires extensive survey efforts to obtain location-tagged signal data, limiting their real-world deployability. Recent approaches that attempt to reduce this overhead either suffer from low representation ability, mode collapse issues, or require the effort of collecting data at all target locations. We present LocaGen, a novel spatial augmentation framework that significantly reduces fingerprinting overhead by generating high-quality synthetic data at completely unseen locations. LocaGen leverages a conditional diffusion model guided by a novel spatially aware optimization strategy to synthesize realistic fingerprints at unseen locations using only a subset of seen locations. To further improve our diffusion model performance, LocaGen augments seen location data based on domain-specific heuristics and strategically selects the seen and unseen locations using a novel density-based approach that ensures robust coverage. Our extensive evaluation on a real-world WiFi fingerprinting dataset shows that LocaGen maintains the same localization accuracy even with 30% of the locations unseen and achieves up to 28% improvement in accuracy over state-of-the-art augmentation methods."}
{"id": "2511.18000", "categories": ["cs.LG", "cs.AI", "q-bio.PE"], "pdf": "https://arxiv.org/pdf/2511.18000", "abs": "https://arxiv.org/abs/2511.18000", "authors": ["Radman Rakhshandehroo", "Daniel Coombs"], "title": "Reward Engineering for Spatial Epidemic Simulations: A Reinforcement Learning Platform for Individual Behavioral Learning", "comment": "35 pages, 15 figures and 14 tables", "summary": "We present ContagionRL, a Gymnasium-compatible reinforcement learning platform specifically designed for systematic reward engineering in spatial epidemic simulations. Unlike traditional agent-based models that rely on fixed behavioral rules, our platform enables rigorous evaluation of how reward function design affects learned survival strategies across diverse epidemic scenarios. ContagionRL integrates a spatial SIRS+D epidemiological model with configurable environmental parameters, allowing researchers to stress-test reward functions under varying conditions including limited observability, different movement patterns, and heterogeneous population dynamics. We evaluate five distinct reward designs, ranging from sparse survival bonuses to a novel potential field approach, across multiple RL algorithms (PPO, SAC, A2C). Through systematic ablation studies, we identify that directional guidance and explicit adherence incentives are critical components for robust policy learning. Our comprehensive evaluation across varying infection rates, grid sizes, visibility constraints, and movement patterns reveals that reward function choice dramatically impacts agent behavior and survival outcomes. Agents trained with our potential field reward consistently achieve superior performance, learning maximal adherence to non-pharmaceutical interventions while developing sophisticated spatial avoidance strategies. The platform's modular design enables systematic exploration of reward-behavior relationships, addressing a knowledge gap in models of this type where reward engineering has received limited attention. ContagionRL is an effective platform for studying adaptive behavioral responses in epidemic contexts and highlight the importance of reward design, information structure, and environmental predictability in learning."}
{"id": "2511.18159", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18159", "abs": "https://arxiv.org/abs/2511.18159", "authors": ["Mengni Jia", "Mengyu Zhou", "Yihao Liu", "Xiaoxi Jiang", "Guanjun Jiang"], "title": "Bringing Stability to Diffusion: Decomposing and Reducing Variance of Training Masked Diffusion Models", "comment": null, "summary": "Masked diffusion models (MDMs) are a promising alternative to autoregressive models (ARMs), but they suffer from inherently much higher training variance. High variance leads to noisier gradient estimates and unstable optimization, so even equally strong pretrained MDMs and ARMs that are competitive at initialization often diverge after task-specific training, with MDMs falling far behind. There has been no theoretical explanation or systematic solution. We derive the first decomposition of MDM training variance into three sources: (A) masking pattern noise, (B) masking rate noise, and (C) data noise, while ARMs are only affected by (C). This explains the fundamental training gap. Building on this foundation, we design six variance-reduction methods, including two core methods: (1) P-POTS, a Pareto-optimal t sampler that minimizes training variance by sampling harder t values more often with appropriately smaller update steps, and (2) MIRROR, which uses negatively correlated samples to reduce (A). Experiments show that compared to standard MDM training, our methods improve accuracy by 7-8% on complex reasoning tasks, while simultaneously reducing run-to-run variability to near ARM levels, substantially narrowing the gap with strong ARM baselines; in most settings, even the best baseline runs remain below the worst run of our method."}
{"id": "2511.18038", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18038", "abs": "https://arxiv.org/abs/2511.18038", "authors": ["Xiaoke Han", "Hong Zhu"], "title": "MASTEST: A LLM-Based Multi-Agent System For RESTful API Tests", "comment": "14 Page of main text plus 4 pages of appendix", "summary": "Testing RESTful API is increasingly important in quality assurance of cloud-native applications. Recent advances in machine learning (ML) techniques have demonstrated that various testing activities can be performed automatically by large language models (LLMs) with reasonable accuracy. This paper develops a multi-agent system called MASTEST that combines LLM-based and programmed agents to form a complete tool chain that covers the whole workflow of API test starting from generating unit and system test scenarios from API specification in the OpenAPI Swagger format, to generating of Pytest test scripts, executing test scripts to interact with web services, to analysing web service response messages to determine test correctness and calculate test coverage. The system also supports the incorporation of human testers in reviewing and correcting LLM generated test artefacts to ensure the quality of testing activities. MASTEST system is evaluated on two LLMs, GPT-4o and DeepSeek V3.1 Reasoner with five public APIs. The performances of LLMs on various testing activities are measured by a wide range of metrics, including unit and system test scenario coverage and API operation coverage for the quality of generated test scenarios, data type correctness, status code coverage and script syntax correctness for the quality of LLM generated test scripts, as well as bug detection ability and usability of LLM generated test scenarios and scripts. Experiment results demonstrated that both DeepSeek and GPT-4o achieved a high overall performance. DeepSeek excels in data type correctness and status code detection, while GPT-4o performs best in API operation coverage. For both models, LLM generated test scripts maintained 100\\% syntax correctness and only required minimal manual edits for semantic correctness. These findings indicate the effectiveness and feasibility of MASTEST."}
{"id": "2511.18178", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18178", "abs": "https://arxiv.org/abs/2511.18178", "authors": ["Shrenik Zinage", "Peter Meckl", "Ilias Bilionis"], "title": "Bayesian Calibration of Engine-out NOx Models for Engine-to-Engine Transferability", "comment": null, "summary": "Accurate prediction of engine-out NOx is essential for meeting stringent emissions regulations and optimizing engine performance. Traditional approaches rely on models trained on data from a small number of engines, which can be insufficient in generalizing across an entire population of engines due to sensor biases and variations in input conditions. In real world applications, these models require tuning or calibration to maintain acceptable error tolerance when applied to other engines. This highlights the need for models that can adapt with minimal adjustments to accommodate engine-to-engine variability and sensor discrepancies. While previous studies have explored machine learning methods for predicting engine-out NOx, these approaches often fail to generalize reliably across different engines and operating environments. To address these issues, we propose a Bayesian calibration framework that combines Gaussian processes with approximate Bayesian computation to infer and correct sensor biases. Starting with a pre-trained model developed using nominal engine data, our method identifies engine specific sensor biases and recalibrates predictions accordingly. By incorporating these inferred biases, our approach generates posterior predictive distributions for engine-out NOx on unseen test data, achieving high accuracy without retraining the model. Our results demonstrate that this transferable modeling approach significantly improves the accuracy of predictions compared to conventional non-adaptive GP models, effectively addressing engine-to-engine variability and improving model generalizability."}
{"id": "2511.18084", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18084", "abs": "https://arxiv.org/abs/2511.18084", "authors": ["Dou Liu", "Ying Long", "Sophia Zuoqiu", "Kaipeng Xie", "Runze Yang", "Di Liu", "Kang Li", "Yiting Lin", "Hanyi Liu", "Rong Yin", "Tian Tang"], "title": "The Alignment Paradox of Medical Large Language Models in Infertility Care: Decoupling Algorithmic Improvement from Clinical Decision-making Quality", "comment": "22 pages 5 figures", "summary": "Large language models (LLMs) are increasingly adopted in clinical decision support, yet aligning them with the multifaceted reasoning pathways of real-world medicine remains a major challenge. Using more than 8,000 infertility treatment records, we systematically evaluate four alignment strategies: Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), Group Relative Policy Optimization (GRPO), and In-Context Learning (ICL) through a dual-layer framework combining automatic benchmarks with blinded doctor-in-the-loop assessments. GRPO achieves the highest algorithmic accuracy across multiple decision layers, confirming the value of reinforcement-based optimization for structured prediction tasks. However, clinicians consistently prefer the SFT model, citing clearer reasoning processes (p = 0.035) and higher therapeutic feasibility (p = 0.019). In blinded pairwise comparisons, SFT attains the highest winning rate (51.2%), outperforming both GRPO (26.2%) and even physicians' original decisions (22.7%). These results reveal an alignment paradox: algorithmic improvements do not necessarily translate into higher clinical trust, and may diverge from human-centered preferences. Our findings highlight the need for alignment strategies that prioritize clinically interpretable and practically feasible reasoning, rather than solely optimizing decision-level accuracy."}
{"id": "2511.18181", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18181", "abs": "https://arxiv.org/abs/2511.18181", "authors": ["Adam Callaghan", "Karl Mason", "Patrick Mannion"], "title": "MOMA-AC: A preference-driven actor-critic framework for continuous multi-objective multi-agent reinforcement learning", "comment": "23 pages, 5 figures", "summary": "This paper addresses a critical gap in Multi-Objective Multi-Agent Reinforcement Learning (MOMARL) by introducing the first dedicated inner-loop actor-critic framework for continuous state and action spaces: Multi-Objective Multi-Agent Actor-Critic (MOMA-AC). Building on single-objective, single-agent algorithms, we instantiate this framework with Twin Delayed Deep Deterministic Policy Gradient (TD3) and Deep Deterministic Policy Gradient (DDPG), yielding MOMA-TD3 and MOMA-DDPG. The framework combines a multi-headed actor network, a centralised critic, and an objective preference-conditioning architecture, enabling a single neural network to encode the Pareto front of optimal trade-off policies for all agents across conflicting objectives in a continuous MOMARL setting. We also outline a natural test suite for continuous MOMARL by combining a pre-existing multi-agent single-objective physics simulator with its multi-objective single-agent counterpart. Evaluating cooperative locomotion tasks in this suite, we show that our framework achieves statistically significant improvements in expected utility and hypervolume relative to outer-loop and independent training baselines, while demonstrating stable scalability as the number of agents increases. These results establish our framework as a foundational step towards robust, scalable multi-objective policy learning in continuous multi-agent domains."}
{"id": "2511.18093", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18093", "abs": "https://arxiv.org/abs/2511.18093", "authors": ["Fulong Yao", "Wanqing Zhao", "Matthew Forshaw"], "title": "A New Error Temporal Difference Algorithm for Deep Reinforcement Learning in Microgrid Optimization", "comment": "Have been accepted by 2024 9th International Conference on Renewable Energy and Conservation (ICREC 2024)", "summary": "Predictive control approaches based on deep reinforcement learning (DRL) have gained significant attention in microgrid energy optimization. However, existing research often overlooks the issue of uncertainty stemming from imperfect prediction models, which can lead to suboptimal control strategies. This paper presents a new error temporal difference (ETD) algorithm for DRL to address the uncertainty in predictions,aiming to improve the performance of microgrid operations. First,a microgrid system integrated with renewable energy sources (RES) and energy storage systems (ESS), along with its Markov decision process (MDP), is modelled. Second, a predictive control approach based on a deep Q network (DQN) is presented, in which a weighted average algorithm and a new ETD algorithm are designed to quantify and address the prediction uncertainty, respectively. Finally, simulations on a realworld US dataset suggest that the developed ETD effectively improves the performance of DRL in optimizing microgrid operations."}
{"id": "2511.18191", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18191", "abs": "https://arxiv.org/abs/2511.18191", "authors": ["Pranav Subbaraman", "Fang Sun", "Yue Yao", "Huacong Tang", "Xiao Luo", "Yizhou Sun"], "title": "Accelerating Time Series Foundation Models with Speculative Decoding", "comment": "14 pages, 7 figures", "summary": "Modern web applications--from real-time content recommendation and dynamic pricing to CDN optimization--increasingly rely on time-series forecasting to deliver personalized experiences to billions of users. Large-scale Transformer-based models have achieved state-of-the-art performance in time-series forecasting but suffer from high computational costs, limiting their deployment in latency-sensitive web applications. To address this challenge, we propose a general inference acceleration framework that adapts speculative decoding to autoregressive time-series models. Our approach employs a smaller \"draft\" model to propose future time-series patches, which are then verified in parallel by a larger \"target\" model, reducing the number of sequential forward passes required. We address key technical challenges in adapting this technique from discrete language tokens to continuous time-series distributions, including the design of acceptance criteria for multivariate Gaussian patches and practical variants that balance efficiency with accuracy. Through experiments on time series forecasting benchmarks relevant to web applications, we demonstrate significant inference speedups while maintaining competitive accuracy. The framework requires no architectural modifications to existing foundation models, making it immediately applicable to accelerate deployed time-series forecasting systems. Our implementation can be found at https://github.com/PranavSubbaraman/STRIDE"}
{"id": "2511.18150", "categories": ["cs.LG", "cs.AI", "math.CO"], "pdf": "https://arxiv.org/pdf/2511.18150", "abs": "https://arxiv.org/abs/2511.18150", "authors": ["Randy Davila", "Beyzanur Ispir"], "title": "Graph Neural Networks vs Convolutional Neural Networks for Graph Domination Number Prediction", "comment": null, "summary": "We investigate machine learning approaches to approximating the \\emph{domination number} of graphs, the minimum size of a dominating set. Exact computation of this parameter is NP-hard, restricting classical methods to small instances. We compare two neural paradigms: Convolutional Neural Networks (CNNs), which operate on adjacency matrix representations, and Graph Neural Networks (GNNs), which learn directly from graph structure through message passing. Across 2,000 random graphs with up to 64 vertices, GNNs achieve markedly higher accuracy ($R^2=0.987$, MAE $=0.372$) than CNNs ($R^2=0.955$, MAE $=0.500$). Both models offer substantial speedups over exact solvers, with GNNs delivering more than $200\\times$ acceleration while retaining near-perfect fidelity. Our results position GNNs as a practical surrogate for combinatorial graph invariants, with implications for scalable graph optimization and mathematical discovery."}
{"id": "2511.18214", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18214", "abs": "https://arxiv.org/abs/2511.18214", "authors": ["Matthijs van der Lende", "Juan Cardenas-Cartagena"], "title": "Deep Gaussian Process Proximal Policy Optimization", "comment": null, "summary": "Uncertainty estimation for Reinforcement Learning (RL) is a critical component in control tasks where agents must balance safe exploration and efficient learning. While deep neural networks have enabled breakthroughs in RL, they often lack calibrated uncertainty estimates. We introduce Deep Gaussian Process Proximal Policy Optimization (GPPO), a scalable, model-free actor-critic algorithm that leverages Deep Gaussian Processes (DGPs) to approximate both the policy and value function. GPPO maintains competitive performance with respect to Proximal Policy Optimization on standard high-dimensional continuous control benchmarks while providing well-calibrated uncertainty estimates that can inform safer and more effective exploration."}
{"id": "2511.18165", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18165", "abs": "https://arxiv.org/abs/2511.18165", "authors": ["Israel Puerta-Merino", "Carlos Núñez-Molina", "Pablo Mesejo", "Juan Fernández-Olivares"], "title": "Towards a General Framework for HTN Modeling with LLMs", "comment": "10 pages, 5 figures, to be published in the Workshop on Planning in the Era of LLMs ( LM4Plan - https://llmforplanning.github.io ) and the Workshop on Hierarchical Planning ( HPlan - https://icaps25.icaps-conference.org/program/workshops/hplan/ ), both in the International Conference on Automated Planning and Scheduling (ICAPS) 2025", "summary": "The use of Large Language Models (LLMs) for generating Automated Planning (AP) models has been widely explored; however, their application to Hierarchical Planning (HP) is still far from reaching the level of sophistication observed in non-hierarchical architectures. In this work, we try to address this gap. We present two main contributions. First, we propose L2HP, an extension of L2P (a library to LLM-driven PDDL models generation) that support HP model generation and follows a design philosophy of generality and extensibility. Second, we apply our framework to perform experiments where we compare the modeling capabilities of LLMs for AP and HP. On the PlanBench dataset, results show that parsing success is limited but comparable in both settings (around 36\\%), while syntactic validity is substantially lower in the hierarchical case (1\\% vs. 20\\% of instances). These findings underscore the unique challenges HP presents for LLMs, highlighting the need for further research to improve the quality of generated HP models."}
{"id": "2511.18225", "categories": ["cs.LG", "stat.ML", "stat.OT"], "pdf": "https://arxiv.org/pdf/2511.18225", "abs": "https://arxiv.org/abs/2511.18225", "authors": ["Douglas Spencer", "Samual Nicholls", "Michele Caprio"], "title": "Adaptive Conformal Prediction for Quantum Machine Learning", "comment": "26 pages, 5 figures", "summary": "Quantum machine learning seeks to leverage quantum computers to improve upon classical machine learning algorithms. Currently, robust uncertainty quantification methods remain underdeveloped in the quantum domain, despite the critical need for reliable and trustworthy predictions. Recent work has introduced quantum conformal prediction, a framework that produces prediction sets that are guaranteed to contain the true outcome with user-specified probability. In this work, we formalise how the time-varying noise inherent in quantum processors can undermine conformal guarantees, even when calibration and test data are exchangeable. To address this challenge, we draw on Adaptive Conformal Inference, a method which maintains validity over time via repeated recalibration. We introduce Adaptive Quantum Conformal Prediction (AQCP), an algorithm which preserves asymptotic average coverage guarantees under arbitrary hardware noise conditions. Empirical studies on an IBM quantum processor demonstrate that AQCP achieves target coverage levels and exhibits greater stability than quantum conformal prediction."}
{"id": "2511.18181", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18181", "abs": "https://arxiv.org/abs/2511.18181", "authors": ["Adam Callaghan", "Karl Mason", "Patrick Mannion"], "title": "MOMA-AC: A preference-driven actor-critic framework for continuous multi-objective multi-agent reinforcement learning", "comment": "23 pages, 5 figures", "summary": "This paper addresses a critical gap in Multi-Objective Multi-Agent Reinforcement Learning (MOMARL) by introducing the first dedicated inner-loop actor-critic framework for continuous state and action spaces: Multi-Objective Multi-Agent Actor-Critic (MOMA-AC). Building on single-objective, single-agent algorithms, we instantiate this framework with Twin Delayed Deep Deterministic Policy Gradient (TD3) and Deep Deterministic Policy Gradient (DDPG), yielding MOMA-TD3 and MOMA-DDPG. The framework combines a multi-headed actor network, a centralised critic, and an objective preference-conditioning architecture, enabling a single neural network to encode the Pareto front of optimal trade-off policies for all agents across conflicting objectives in a continuous MOMARL setting. We also outline a natural test suite for continuous MOMARL by combining a pre-existing multi-agent single-objective physics simulator with its multi-objective single-agent counterpart. Evaluating cooperative locomotion tasks in this suite, we show that our framework achieves statistically significant improvements in expected utility and hypervolume relative to outer-loop and independent training baselines, while demonstrating stable scalability as the number of agents increases. These results establish our framework as a foundational step towards robust, scalable multi-objective policy learning in continuous multi-agent domains."}
{"id": "2511.18247", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.18247", "abs": "https://arxiv.org/abs/2511.18247", "authors": ["Sajad Khodadadian", "Mehrdad Moharrami"], "title": "Tail Distribution of Regret in Optimistic Reinforcement Learning", "comment": "18 pages, 0 figures", "summary": "We derive instance-dependent tail bounds for the regret of optimism-based reinforcement learning in finite-horizon tabular Markov decision processes with unknown transition dynamics. Focusing on a UCBVI-type algorithm, we characterize the tail distribution of the cumulative regret $R_K$ over $K$ episodes, rather than only its expectation or a single high-probability quantile. We analyze two natural exploration-bonus schedules: (i) a $K$-dependent scheme that explicitly incorporates the total number of episodes $K$, and (ii) a $K$-independent scheme that depends only on the current episode index. For both settings, we obtain an upper bound on $\\Pr(R_K \\ge x)$ that exhibits a distinctive two-regime structure: a sub-Gaussian tail starting from an instance-dependent scale $m_K$ up to a transition threshold, followed by a sub-Weibull tail beyond that point. We further derive corresponding instance-dependent bounds on the expected regret $\\mathbb{E}[R_K]$. The proposed algorithm depends on a tuning parameter $α$, which balances the expected regret and the range over which the regret exhibits a sub-Gaussian tail. To the best of our knowledge, our results provide one of the first comprehensive tail-regret guarantees for a standard optimistic algorithm in episodic reinforcement learning."}
{"id": "2511.18182", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18182", "abs": "https://arxiv.org/abs/2511.18182", "authors": ["Lee Ackerman"], "title": "The Workflow as Medium: A Framework for Navigating Human-AI Co-Creation", "comment": "57 pages, 13 images, 6 tables", "summary": "This paper introduces the Creative Intelligence Loop (CIL), a novel socio-technical framework for responsible human-AI co-creation. Rooted in the 'Workflow as Medium' paradigm, the CIL proposes a disciplined structure for dynamic human-AI collaboration, guiding the strategic integration of diverse AI teammates who function as collaborators while the human remains the final arbiter for ethical alignment and creative integrity. The CIL was empirically demonstrated through the practice-led creation of two graphic novellas, investigating how AI could serve as an effective creative colleague within a subjective medium lacking objective metrics. The process required navigating multifaceted challenges including AI's 'jagged frontier' of capabilities, sycophancy, and attention-scarce feedback environments. This prompted iterative refinement of teaming practices, yielding emergent strategies: a multi-faceted critique system integrating adversarial AI roles to counter sycophancy, and prioritizing 'feedback-ready' concrete artifacts to elicit essential human critique. The resulting graphic novellas analyze distinct socio-technical governance failures: 'The Steward' examines benevolent AI paternalism in smart cities, illustrating how algorithmic hubris can erode freedom; 'Fork the Vote' probes democratic legitimacy by comparing centralized AI opacity with emergent collusion in federated networks. This work contributes a self-improving framework for responsible human-AI co-creation and two graphic novellas designed to foster AI literacy and dialogue through accessible narrative analysis of AI's societal implications."}
{"id": "2511.18248", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18248", "abs": "https://arxiv.org/abs/2511.18248", "authors": ["Wei Zhen Teoh"], "title": "Coherent Multi-Agent Trajectory Forecasting in Team Sports with CausalTraj", "comment": "9 pages, 3 figures, accepted to the AI4TS Workshop at AAAI 2026", "summary": "Jointly forecasting trajectories of multiple interacting agents is a core challenge in sports analytics and other domains involving complex group dynamics. Accurate prediction enables realistic simulation and strategic understanding of gameplay evolution. Most existing models are evaluated solely on per-agent accuracy metrics (minADE, minFDE), which assess each agent independently on its best-of-k prediction. However these metrics overlook whether the model learns which predicted trajectories can jointly form a plausible multi-agent future. Many state-of-the-art models are designed and optimized primarily based on these metrics. As a result, they may underperform on joint predictions and also fail to generate coherent, interpretable multi-agent scenarios in team sports. We propose CausalTraj, a temporally causal, likelihood-based model that is built to generate jointly probable multi-agent trajectory forecasts. To better assess collective modeling capability, we emphasize joint metrics (minJADE, minJFDE) that measure joint accuracy across agents within the best generated scenario sample. Evaluated on the NBA SportVU, Basketball-U, and Football-U datasets, CausalTraj achieves competitive per-agent accuracy and the best recorded results on joint metrics, while yielding qualitatively coherent and realistic gameplay evolutions."}
{"id": "2511.18221", "categories": ["cs.CY", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.18221", "abs": "https://arxiv.org/abs/2511.18221", "authors": ["Liangliang Chen", "Huiru Xie", "Zhihao Qin", "Yiming Guo", "Jacqueline Rohde", "Ying Zhang"], "title": "Enhancing Large Language Models for Automated Homework Assessment in Undergraduate Circuit Analysis", "comment": "Accepted to 2025 Frontiers in Education (FIE) Conference", "summary": "This research full paper presents an enhancement pipeline for large language models (LLMs) in assessing homework for an undergraduate circuit analysis course, aiming to improve LLMs' capacity to provide personalized support to electrical engineering students. Existing evaluations have demonstrated that GPT-4o possesses promising capabilities in assessing student homework in this domain. Building on these findings, we enhance GPT-4o's performance through multi-step prompting, contextual data augmentation, and the incorporation of targeted hints. These strategies effectively address common errors observed in GPT-4o's responses when using simple prompts, leading to a substantial improvement in assessment accuracy. Specifically, the correct response rate for GPT-4o increases from 74.71% to 97.70% after applying the enhanced prompting and augmented data on entry-level circuit analysis topics. This work lays a foundation for the effective integration of LLMs into circuit analysis instruction and, more broadly, into engineering education."}
{"id": "2511.18260", "categories": ["cs.LG", "math.NA"], "pdf": "https://arxiv.org/pdf/2511.18260", "abs": "https://arxiv.org/abs/2511.18260", "authors": ["Yueqi Wang", "Guang Lin"], "title": "Reduced-Basis Deep Operator Learning for Parametric PDEs with Independently Varying Boundary and Source Data", "comment": null, "summary": "Parametric PDEs power modern simulation, design, and digital-twin systems, yet their many-query workloads still hinge on repeatedly solving large finite-element systems. Existing operator-learning approaches accelerate this process but often rely on opaque learned trunks, require extensive labeled data, or break down when boundary and source data vary independently from physical parameters. We introduce RB-DeepONet, a hybrid operator-learning framework that fuses reduced-basis (RB) numerical structure with the branch-trunk architecture of DeepONet. The trunk is fixed to a rigorously constructed RB space generated offline via Greedy selection, granting physical interpretability, stability, and certified error control. The branch network predicts only RB coefficients and is trained label-free using a projected variational residual that targets the RB-Galerkin solution. For problems with independently varying loads or boundary conditions, we develop boundary and source modal encodings that compress exogenous data into low-dimensional coordinates while preserving accuracy. Combined with affine or empirical interpolation decompositions, RB-DeepONet achieves a strict offline-online split: all heavy lifting occurs offline, and online evaluation scales only with the RB dimension rather than the full mesh. We provide convergence guarantees separating RB approximation error from statistical learning error, and numerical experiments show that RB-DeepONet attains accuracy competitive with intrusive RB-Galerkin, POD-DeepONet, and FEONet while using dramatically fewer trainable parameters and achieving significant speedups. This establishes RB-DeepONet as an efficient, stable, and interpretable operator learner for large-scale parametric PDEs."}
{"id": "2511.18223", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18223", "abs": "https://arxiv.org/abs/2511.18223", "authors": ["H. Zhang", "L. Zhang", "G. Epiphaniou", "C. Maple"], "title": "A Novel and Practical Universal Adversarial Perturbations against Deep Reinforcement Learning based Intrusion Detection Systems", "comment": "13 pages, 7 Figures,", "summary": "Intrusion Detection Systems (IDS) play a vital role in defending modern cyber physical systems against increasingly sophisticated cyber threats. Deep Reinforcement Learning-based IDS, have shown promise due to their adaptive and generalization capabilities. However, recent studies reveal their vulnerability to adversarial attacks, including Universal Adversarial Perturbations (UAPs), which can deceive models with a single, input-agnostic perturbation. In this work, we propose a novel UAP attack against Deep Reinforcement Learning (DRL)-based IDS under the domain-specific constraints derived from network data rules and feature relationships. To the best of our knowledge, there is no existing study that has explored UAP generation for the DRL-based IDS. In addition, this is the first work that focuses on developing a UAP against a DRL-based IDS under realistic domain constraints based on not only the basic domain rules but also mathematical relations between the features. Furthermore, we enhance the evasion performance of the proposed UAP, by introducing a customized loss function based on the Pearson Correlation Coefficient, and we denote it as Customized UAP. To the best of our knowledge, this is also the first work using the PCC value in the UAP generation, even in the broader context. Four additional established UAP baselines are implemented for a comprehensive comparison. Experimental results demonstrate that our proposed Customized UAP outperforms two input-dependent attacks including Fast Gradient Sign Method (FGSM), Basic Iterative Method (BIM), and four UAP baselines, highlighting its effectiveness for real-world adversarial scenarios."}
{"id": "2511.18269", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.18269", "abs": "https://arxiv.org/abs/2511.18269", "authors": ["Ved Mohan", "El Mehdi Er Raqabi", "Pascal Van Hentenryck"], "title": "A Fair OR-ML Framework for Resource Substitution in Large-Scale Networks", "comment": null, "summary": "Ensuring that the right resource is available at the right location and time remains a major challenge for organizations operating large-scale logistics networks. The challenge comes from uneven demand patterns and the resulting asymmetric flow of resources across the arcs, which create persistent imbalances at the network nodes. Resource substitution among multiple, potentially composite and interchangeable, resource types is a cost-effective way to mitigate these imbalances. This leads to the resource substitution problem, which aims at determining the minimum number of resource substitutions from an initial assignment to minimize the overall network imbalance. In decentralized settings, achieving globally coordinated solutions becomes even more difficult. When substitution entails costs, effective prescriptions must also incorporate fairness and account for the individual preferences of schedulers. This paper presents a generic framework that combines operations research (OR) and machine learning (ML) to enable fair resource substitution in large networks. The OR component models and solves the resource substitution problem under a fairness lens. The ML component leverages historical data to learn schedulers' preferences, guide intelligent exploration of the decision space, and enhance computational efficiency by dynamically selecting the top-$κ$ resources for each arc in the network. The framework produces a portfolio of high-quality solutions from which schedulers can select satisfactory trade-offs. The proposed framework is applied to the network of one of the largest package delivery companies in the world, which serves as the primary motivation for this research. Computational results demonstrate substantial improvements over state-of-the-art methods, including an 80% reduction in model size and a 90% decrease in execution time while preserving optimality."}
{"id": "2511.18239", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18239", "abs": "https://arxiv.org/abs/2511.18239", "authors": ["Mohamed Afane", "Ying Wang", "Juntao Chen"], "title": "Can LLMs Help Allocate Public Health Resources? A Case Study on Childhood Lead Testing", "comment": null, "summary": "Public health agencies face critical challenges in identifying high-risk neighborhoods for childhood lead exposure with limited resources for outreach and intervention programs. To address this, we develop a Priority Score integrating untested children proportions, elevated blood lead prevalence, and public health coverage patterns to support optimized resource allocation decisions across 136 neighborhoods in Chicago, New York City, and Washington, D.C. We leverage these allocation tasks, which require integrating multiple vulnerability indicators and interpreting empirical evidence, to evaluate whether large language models (LLMs) with agentic reasoning and deep research capabilities can effectively allocate public health resources when presented with structured allocation scenarios. LLMs were tasked with distributing 1,000 test kits within each city based on neighborhood vulnerability indicators. Results reveal significant limitations: LLMs frequently overlooked neighborhoods with highest lead prevalence and largest proportions of untested children, such as West Englewood in Chicago, while allocating disproportionate resources to lower-priority areas like Hunts Point in New York City. Overall accuracy averaged 0.46, reaching a maximum of 0.66 with ChatGPT 5 Deep Research. Despite their marketed deep research capabilities, LLMs struggled with fundamental limitations in information retrieval and evidence-based reasoning, frequently citing outdated data and allowing non-empirical narratives about neighborhood conditions to override quantitative vulnerability indicators."}
{"id": "2511.18278", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18278", "abs": "https://arxiv.org/abs/2511.18278", "authors": ["Jianqiao Zheng", "Cameron Gordon", "Yiping Ji", "Hemanth Saratchandran", "Simon Lucey"], "title": "From Tables to Signals: Revealing Spectral Adaptivity in TabPFN", "comment": null, "summary": "Task-agnostic tabular foundation models such as TabPFN have achieved impressive performance on tabular learning tasks, yet the origins of their inductive biases remain poorly understood. In this work, we study TabPFN through the lens of signal reconstruction and provide the first frequency-based analysis of its in-context learning behavior. We show that TabPFN possesses a broader effective frequency capacity than standard ReLU-MLPs, even without hyperparameter tuning. Moreover, unlike MLPs whose spectra evolve primarily over training epochs, we find that TabPFN's spectral capacity adapts directly to the number of samples provided in-context, a phenomenon we term Spectral Adaptivity. We further demonstrate that positional encoding modulates TabPFN's frequency response, mirroring classical results in implicit neural representations. Finally, we show that these properties enable TabPFN to perform training-free and hyperparameter-free image denoising, illustrating its potential as a task-agnostic implicit model. Our analysis provides new insight into the structure and inductive biases of tabular foundation models and highlights their promise for broader signal reconstruction tasks."}
{"id": "2511.18294", "categories": ["cs.LG", "cs.AI", "cs.HC", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2511.18294", "abs": "https://arxiv.org/abs/2511.18294", "authors": ["Mengchun Zhang", "Kateryna Shapovalenko", "Yucheng Shao", "Eddie Guo", "Parusha Pradhan"], "title": "MultiDiffNet: A Multi-Objective Diffusion Framework for Generalizable Brain Decoding", "comment": null, "summary": "Neural decoding from electroencephalography (EEG) remains fundamentally limited by poor generalization to unseen subjects, driven by high inter-subject variability and the lack of large-scale datasets to model it effectively. Existing methods often rely on synthetic subject generation or simplistic data augmentation, but these strategies fail to scale or generalize reliably. We introduce \\textit{MultiDiffNet}, a diffusion-based framework that bypasses generative augmentation entirely by learning a compact latent space optimized for multiple objectives. We decode directly from this space and achieve state-of-the-art generalization across various neural decoding tasks using subject and session disjoint evaluation. We also curate and release a unified benchmark suite spanning four EEG decoding tasks of increasing complexity (SSVEP, Motor Imagery, P300, and Imagined Speech) and an evaluation protocol that addresses inconsistent split practices in prior EEG research. Finally, we develop a statistical reporting framework tailored for low-trial EEG settings. Our work provides a reproducible and open-source foundation for subject-agnostic EEG decoding in real-world BCI systems."}
{"id": "2511.18287", "categories": ["cs.LG", "cs.CV", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2511.18287", "abs": "https://arxiv.org/abs/2511.18287", "authors": ["Rui Peng", "Ziru Liu", "Lingyuan Ye", "Yuxing Lu", "Boxin Shi", "Jinzhuo Wang"], "title": "TRIDENT: A Trimodal Cascade Generative Framework for Drug and RNA-Conditioned Cellular Morphology Synthesis", "comment": null, "summary": "Accurately modeling the relationship between perturbations, transcriptional responses, and phenotypic changes is essential for building an AI Virtual Cell (AIVC). However, existing methods typically constrained to modeling direct associations, such as Perturbation $\\rightarrow$ RNA or Perturbation $\\rightarrow$ Morphology, overlook the crucial causal link from RNA to morphology. To bridge this gap, we propose TRIDENT, a cascade generative framework that synthesizes realistic cellular morphology by conditioning on both the perturbation and the corresponding gene expression profile. To train and evaluate this task, we construct MorphoGene, a new dataset pairing L1000 gene expression with Cell Painting images for 98 compounds. TRIDENT significantly outperforms state-of-the-art approaches, achieving up to 7-fold improvement with strong generalization to unseen compounds. In a case study on docetaxel, we validate that RNA-guided synthesis accurately produces the corresponding phenotype. An ablation study further confirms that this RNA conditioning is essential for the model's high fidelity. By explicitly modeling transcriptome-phenome mapping, TRIDENT provides a powerful in silico tool and moves us closer to a predictive virtual cell."}
{"id": "2511.18314", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18314", "abs": "https://arxiv.org/abs/2511.18314", "authors": ["Yuting Gao", "Wang Lan", "Hengyuan Zhao", "Linjiang Huang", "Si Liu", "Qingpei Guo"], "title": "AnyExperts: On-Demand Expert Allocation for Multimodal Language Models with Mixture of Expert", "comment": null, "summary": "Multimodal Mixture-of-Experts (MoE) models offer a promising path toward scalable and efficient large vision-language systems. However, existing approaches rely on rigid routing strategies (typically activating a fixed number of experts per token) ignoring the inherent heterogeneity in semantic importance across modalities. This leads to suboptimal compute allocation, where redundant tokens consume as many resources as critical ones. To address this, we propose AnyExperts, a novel on-demand, budget-aware dynamic routing framework that allocates a variable total number of expert slots per token based on its semantic importance. Crucially, to prevent uncontrolled compute growth, the total slots per token are constrained within a fixed range, and each slot is filled by either a real expert or a virtual expert, with the virtual share capped at a small maximum (e.g., 20%). The model then adaptively balances the real-to-virtual ratio per token, assigning more real experts to semantically rich regions and relying more on virtual experts for redundant content. Evaluated across diverse tasks in visual understanding, audio understanding, and NLP understanding, AnyExperts improves performance under the same compute budget. Notably, on general image/video tasks, it achieves comparable accuracy with 40% fewer real expert activations; on text-dense tasks (OCR and NLP), it maintains performance while reducing real expert usage by 10%. These results demonstrate that fine-grained, importance-driven expert allocation significantly enhances both the efficiency and effectiveness of multimodal MoE models."}
{"id": "2511.18291", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.18291", "abs": "https://arxiv.org/abs/2511.18291", "authors": ["Xiaoyu Wang", "Xiaotian Li", "Zhixiang Zhou", "Chen Li", "Yong Liu"], "title": "ADF-LoRA: Alternating Low-Rank Aggregation for Decentralized Federated Fine-Tuning", "comment": "10 Pages", "summary": "This paper revisits alternating low-rank updates for federated fine-tuning and examines their behavior in decentralized federated learning (DFL). While alternating the LoRA matrices has been shown to stabilize aggregation in centralized FL, extending this mechanism to decentralized, peer-to-peer communication introduces new challenges due to phase-state mismatch and block-wise divergence across clients. We introduce ADF-LoRA, which synchronizes the update of only one low-rank matrix per round and mixes both matrices to maintain more consistent parameter states under decentralized propagation. This design preserves the cross-term suppression effect of alternating updates while improving stability in serverless topologies. We provide a convergence analysis under standard smoothness assumptions and evaluate ADF-LoRA on multiple GLUE tasks. Experiments show that ADF-LoRA achieves faster and smoother convergence and delivers the highest average accuracy across tasks, outperforming existing LoRA variants in decentralized FL by a consistent margin."}
{"id": "2511.18334", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18334", "abs": "https://arxiv.org/abs/2511.18334", "authors": ["Chibuike E. Ugwu", "Roschelle Fritz", "Diane J. Cook", "Janardhan Rao Doppa"], "title": "Clinician-in-the-Loop Smart Home System to Detect Urinary Tract Infection Flare-Ups via Uncertainty-Aware Decision Support", "comment": "Accepted for publication at IAAI-26 / AAAI-26", "summary": "Urinary tract infection (UTI) flare-ups pose a significant health risk for older adults with chronic conditions. These infections often go unnoticed until they become severe, making early detection through innovative smart home technologies crucial. Traditional machine learning (ML) approaches relying on simple binary classification for UTI detection offer limited utility to nurses and practitioners as they lack insight into prediction uncertainty, hindering informed clinical decision-making. This paper presents a clinician-in-the-loop (CIL) smart home system that leverages ambient sensor data to extract meaningful behavioral markers, train robust predictive ML models, and calibrate them to enable uncertainty-aware decision support. The system incorporates a statistically valid uncertainty quantification method called Conformal-Calibrated Interval (CCI), which quantifies uncertainty and abstains from making predictions (\"I don't know\") when the ML model's confidence is low. Evaluated on real-world data from eight smart homes, our method outperforms baseline methods in recall and other classification metrics while maintaining the lowest abstention proportion and interval width. A survey of 42 nurses confirms that our system's outputs are valuable for guiding clinical decision-making, underscoring their practical utility in improving informed decisions and effectively managing UTIs and other condition flare-ups in older adults."}
{"id": "2511.18294", "categories": ["cs.LG", "cs.AI", "cs.HC", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2511.18294", "abs": "https://arxiv.org/abs/2511.18294", "authors": ["Mengchun Zhang", "Kateryna Shapovalenko", "Yucheng Shao", "Eddie Guo", "Parusha Pradhan"], "title": "MultiDiffNet: A Multi-Objective Diffusion Framework for Generalizable Brain Decoding", "comment": null, "summary": "Neural decoding from electroencephalography (EEG) remains fundamentally limited by poor generalization to unseen subjects, driven by high inter-subject variability and the lack of large-scale datasets to model it effectively. Existing methods often rely on synthetic subject generation or simplistic data augmentation, but these strategies fail to scale or generalize reliably. We introduce \\textit{MultiDiffNet}, a diffusion-based framework that bypasses generative augmentation entirely by learning a compact latent space optimized for multiple objectives. We decode directly from this space and achieve state-of-the-art generalization across various neural decoding tasks using subject and session disjoint evaluation. We also curate and release a unified benchmark suite spanning four EEG decoding tasks of increasing complexity (SSVEP, Motor Imagery, P300, and Imagined Speech) and an evaluation protocol that addresses inconsistent split practices in prior EEG research. Finally, we develop a statistical reporting framework tailored for low-trial EEG settings. Our work provides a reproducible and open-source foundation for subject-agnostic EEG decoding in real-world BCI systems."}
{"id": "2511.18404", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18404", "abs": "https://arxiv.org/abs/2511.18404", "authors": ["Van Thuy Hoang", "O-Joun Lee"], "title": "Pre-training Graph Neural Networks on 2D and 3D Molecular Structures by using Multi-View Conditional Information Bottleneck", "comment": null, "summary": "Recent pre-training strategies for molecular graphs have attempted to use 2D and 3D molecular views as both inputs and self-supervised signals, primarily aligning graph-level representations. However, existing studies remain limited in addressing two main challenges of multi-view molecular learning: (1) discovering shared information between two views while diminishing view-specific information and (2) identifying and aligning important substructures, e.g., functional groups, which are crucial for enhancing cross-view consistency and model expressiveness. To solve these challenges, we propose a Multi-View Conditional Information Bottleneck framework, called MVCIB, for pre-training graph neural networks on 2D and 3D molecular structures in a self-supervised setting. Our idea is to discover the shared information while minimizing irrelevant features from each view under the MVCIB principle, which uses one view as a contextual condition to guide the representation learning of its counterpart. To enhance semantic and structural consistency across views, we utilize key substructures, e.g., functional groups and ego-networks, as anchors between the two views. Then, we propose a cross-attention mechanism that captures fine-grained correlations between the substructures to achieve subgraph alignment across views. Extensive experiments in four molecular domains demonstrated that MVCIB consistently outperforms baselines in both predictive performance and interpretability. Moreover, MVCIB achieved the 3d Weisfeiler-Lehman expressiveness power to distinguish not only non-isomorphic graphs but also different 3D geometries that share identical 2D connectivity, such as isomers."}
{"id": "2511.18297", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18297", "abs": "https://arxiv.org/abs/2511.18297", "authors": ["Kiran Thorat", "Hongwu Peng", "Yuebo Luo", "Xi Xie", "Shaoyi Huang", "Amit Hasan", "Jiahui Zhao", "Yingjie Li", "Zhijie Shi", "Cunxi Yu", "Caiwen Ding"], "title": "GROOT: Graph Edge Re-growth and Partitioning for the Verification of Large Designs in Logic Synthesis", "comment": null, "summary": "Traditional verification methods in chip design are highly time-consuming and computationally demanding, especially for large scale circuits. Graph neural networks (GNNs) have gained popularity as a potential solution to improve verification efficiency. However, there lacks a joint framework that considers all chip design domain knowledge, graph theory, and GPU kernel designs. To address this challenge, we introduce GROOT, an algorithm and system co-design framework that contains chip design domain knowledge and redesigned GPU kernels, to improve verification efficiency. More specifically, we create node features utilizing the circuit node types and the polarity of the connections between the input edges to nodes in And-Inverter Graphs (AIGs). We utilize a graph partitioning algorithm to divide the large graphs into smaller sub-graphs for fast GPU processing and develop a graph edge re-growth algorithm to recover verification accuracy. We carefully profile the EDA graph workloads and observe the uniqueness of their polarized distribution of high degree (HD) nodes and low degree (LD) nodes. We redesign two GPU kernels (HD-kernel and LD-kernel), to fit the EDA graph learning workload on a single GPU. We compare the results with state-of-the-art (SOTA) methods: GAMORA, a GNN-based approach, and the traditional ABC framework. Results show that GROOT achieves a significant reduction in memory footprint (59.38 %), with high accuracy (99.96%) for a very large CSA multiplier, i.e. 1,024 bits with a batch size of 16, which consists of 134,103,040 nodes and 268,140,544 edges. We compare GROOT with GPU-based GPU Kernel designs SOTAs such as cuSPARSE, MergePath-SpMM, and GNNAdvisor. We achieve up to 1.104x, 5.796x, and 1.469x improvement in runtime, respectively."}
{"id": "2511.18417", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18417", "abs": "https://arxiv.org/abs/2511.18417", "authors": ["Yoshihiro Maruyama"], "title": "Categorical Equivariant Deep Learning: Category-Equivariant Neural Networks and Universal Approximation Theorems", "comment": null, "summary": "We develop a theory of category-equivariant neural networks (CENNs) that unifies group/groupoid-equivariant networks, poset/lattice-equivariant networks, graph and sheaf neural networks. Equivariance is formulated as naturality in a topological category with Radon measures, formulating linear and nonlinear layers in the categorical setup. We prove the equivariant universal approximation theorem in the general setting: the class of finite-depth CENNs is dense in the space of continuous equivariant transformations. We instantiate the framework for groups/groupoids, posets/lattices, graphs and cellular sheaves, deriving universal approximation theorems for them in a systematic manner. Categorical equivariant deep learning thus allows us to expand the horizons of equivariant deep learning beyond group actions, encompassing not only geometric symmetries but also contextual and compositional symmetries."}
{"id": "2511.18303", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18303", "abs": "https://arxiv.org/abs/2511.18303", "authors": ["Rui Ding", "Rodrigo Pires Ferreira", "Yuxin Chen", "Junhong Chen"], "title": "Hierarchical Deep Research with Local-Web RAG: Toward Automated System-Level Materials Discovery", "comment": "A preliminary version appeared in The AI for Accelerated Materials Discovery (AI4Mat) Workshop at NeurIPS 2025", "summary": "We present a long-horizon, hierarchical deep research (DR) agent designed for complex materials and device discovery problems that exceed the scope of existing Machine Learning (ML) surrogates and closed-source commercial agents. Our framework instantiates a locally deployable DR instance that integrates local retrieval-augmented generation with large language model reasoners, enhanced by a Deep Tree of Research (DToR) mechanism that adaptively expands and prunes research branches to maximize coverage, depth, and coherence. We systematically evaluate across 27 nanomaterials/device topics using a large language model (LLM)-as-judge rubric with five web-enabled state-of-the-art models as jurors. In addition, we conduct dry-lab validations on five representative tasks, where human experts use domain simulations (e.g., density functional theory, DFT) to verify whether DR-agent proposals are actionable. Results show that our DR agent produces reports with quality comparable to--and often exceeding--those of commercial systems (ChatGPT-5-thinking/o3/o4-mini-high Deep Research) at a substantially lower cost, while enabling on-prem integration with local data and tools."}
{"id": "2511.18467", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18467", "abs": "https://arxiv.org/abs/2511.18467", "authors": ["Xiaoqing Wang", "Keman Huang", "Bin Liang", "Hongyu Li", "Xiaoyong Du"], "title": "Shadows in the Code: Exploring the Risks and Defenses of LLM-based Multi-Agent Software Development Systems", "comment": "Accepted by AAAI 2026 Alignment Track", "summary": "The rapid advancement of Large Language Model (LLM)-driven multi-agent systems has significantly streamlined software developing tasks, enabling users with little technical expertise to develop executable applications. While these systems democratize software creation through natural language requirements, they introduce significant security risks that remain largely unexplored. We identify two risky scenarios: Malicious User with Benign Agents (MU-BA) and Benign User with Malicious Agents (BU-MA). We introduce the Implicit Malicious Behavior Injection Attack (IMBIA), demonstrating how multi-agent systems can be manipulated to generate software with concealed malicious capabilities beneath seemingly benign applications, and propose Adv-IMBIA as a defense mechanism. Evaluations across ChatDev, MetaGPT, and AgentVerse frameworks reveal varying vulnerability patterns, with IMBIA achieving attack success rates of 93%, 45%, and 71% in MU-BA scenarios, and 71%, 84%, and 45% in BU-MA scenarios. Our defense mechanism reduced attack success rates significantly, particularly in the MU-BA scenario. Further analysis reveals that compromised agents in the coding and testing phases pose significantly greater security risks, while also identifying critical agents that require protection against malicious user exploitation. Our findings highlight the urgent need for robust security measures in multi-agent software development systems and provide practical guidelines for implementing targeted, resource-efficient defensive strategies."}
{"id": "2511.18312", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18312", "abs": "https://arxiv.org/abs/2511.18312", "authors": ["Zihao Yao", "Jiankai Zuo", "Yaying Zhang"], "title": "DiM-TS: Bridge the Gap between Selective State Space Models and Time Series for Generative Modeling", "comment": null, "summary": "Time series data plays a pivotal role in a wide variety of fields but faces challenges related to privacy concerns. Recently, synthesizing data via diffusion models is viewed as a promising solution. However, existing methods still struggle to capture long-range temporal dependencies and complex channel interrelations. In this research, we aim to utilize the sequence modeling capability of a State Space Model called Mamba to extend its applicability to time series data generation. We firstly analyze the core limitations in State Space Model, namely the lack of consideration for correlated temporal lag and channel permutation. Building upon the insight, we propose Lag Fusion Mamba and Permutation Scanning Mamba, which enhance the model's ability to discern significant patterns during the denoising process. Theoretical analysis reveals that both variants exhibit a unified matrix multiplication framework with the original Mamba, offering a deeper understanding of our method. Finally, we integrate two variants and introduce Diffusion Mamba for Time Series (DiM-TS), a high-quality time series generation model that better preserves the temporal periodicity and inter-channel correlations. Comprehensive experiments on public datasets demonstrate the superiority of DiM-TS in generating realistic time series while preserving diverse properties of data."}
{"id": "2511.18488", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18488", "abs": "https://arxiv.org/abs/2511.18488", "authors": ["Samuel Ackerman", "Wesam Ibraheem", "Orna Raz", "Marcel Zalmanovici"], "title": "Evaluating perturbation robustnessof generative systems that use COBOL code inputs", "comment": "16 pages (8 main, 8 appendix). Accepted to AI-SQE (ICSE, 2026): The 1st International Workshop on AI for Software Quality Evaluation: Judgment, Metrics, Benchmarks, and Beyond", "summary": "Systems incorporating large language models (LLMs) as a component are known to be sensitive (i.e., non-robust) to minor input variations that do not change the meaning of the input; such sensitivity may reduce the system's usefulness. Here, we present a framework to evaluate robustness of systems using COBOL code as input; our application is translation between COBOL and Java programming languages, but the approach extends to other tasks such as code generation or explanation. Targeting robustness of systems with COBOL as input is essential yet challenging. Many business-critical applications are written in COBOL, yet these are typically proprietary legacy applications and their code is unavailable to LLMs for training. We develop a library of COBOL paragraph and full-program perturbation methods, and create variant-expanded versions of a benchmark dataset of examples for a specific task. The robustness of the LLM-based system is evaluated by measuring changes in values of individual and aggregate metrics calculated on the system's outputs. Finally, we present a series of dynamic table and chart visualization dashboards that assist in debugging the system's outputs, and monitoring and understanding root causes of the system's sensitivity to input variation. These tools can be further used to improve the system by, for instance, indicating variations that should be handled by pre-processing steps."}
{"id": "2511.18314", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18314", "abs": "https://arxiv.org/abs/2511.18314", "authors": ["Yuting Gao", "Wang Lan", "Hengyuan Zhao", "Linjiang Huang", "Si Liu", "Qingpei Guo"], "title": "AnyExperts: On-Demand Expert Allocation for Multimodal Language Models with Mixture of Expert", "comment": null, "summary": "Multimodal Mixture-of-Experts (MoE) models offer a promising path toward scalable and efficient large vision-language systems. However, existing approaches rely on rigid routing strategies (typically activating a fixed number of experts per token) ignoring the inherent heterogeneity in semantic importance across modalities. This leads to suboptimal compute allocation, where redundant tokens consume as many resources as critical ones. To address this, we propose AnyExperts, a novel on-demand, budget-aware dynamic routing framework that allocates a variable total number of expert slots per token based on its semantic importance. Crucially, to prevent uncontrolled compute growth, the total slots per token are constrained within a fixed range, and each slot is filled by either a real expert or a virtual expert, with the virtual share capped at a small maximum (e.g., 20%). The model then adaptively balances the real-to-virtual ratio per token, assigning more real experts to semantically rich regions and relying more on virtual experts for redundant content. Evaluated across diverse tasks in visual understanding, audio understanding, and NLP understanding, AnyExperts improves performance under the same compute budget. Notably, on general image/video tasks, it achieves comparable accuracy with 40% fewer real expert activations; on text-dense tasks (OCR and NLP), it maintains performance while reducing real expert usage by 10%. These results demonstrate that fine-grained, importance-driven expert allocation significantly enhances both the efficiency and effectiveness of multimodal MoE models."}
{"id": "2511.18589", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18589", "abs": "https://arxiv.org/abs/2511.18589", "authors": ["Michael Trusov", "Minha Hwang", "Zainab Jamal", "Swarup Chandra"], "title": "Strategic Decision Framework for Enterprise LLM Adoption", "comment": "14 pages, 1 key figure", "summary": "Organizations are rapidly adopting Large Language Models (LLMs) to transform their operations, yet they lack clear guidance on key decisions for adoption and implementation. While LLMs offer powerful capabilities in content generation, assisted coding, and process automation, businesses face critical challenges in data security, LLM solution development approach, infrastructure requirements, and deployment strategies. Healthcare providers must protect patient data while leveraging LLMs for medical analysis, financial institutions need to balance automated customer service with regulatory compliance, and software companies seek to enhance development productivity while maintaining code security.\n  This article presents a systematic six-step decision framework for LLM adoption, helping organizations navigate from initial application selection to final deployment. Based on extensive interviews and analysis of successful and failed implementations, our framework provides practical guidance for business leaders to align technological capabilities with business objectives. Through key decision points and real-world examples from both B2B and B2C contexts, organizations can make informed decisions about LLM adoption while ensuring secure and efficient integration across various use cases, from customer service automation to content creation and advanced analytics."}
{"id": "2511.18331", "categories": ["cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.18331", "abs": "https://arxiv.org/abs/2511.18331", "authors": ["Sohini Roychowdhury", "Adam Holeman", "Mohammad Amin", "Feng Wei", "Bhaskar Mehta", "Srihari Reddy"], "title": "DynamiX: Dynamic Resource eXploration for Personalized Ad-Recommendations", "comment": "9 pages, 3 Tables, 5 images. https://openreview.net/pdf?id=oglD54lvcB", "summary": "For online ad-recommendation systems, processing complete user-ad-engagement histories is both computationally intensive and noise-prone. We introduce Dynamix, a scalable, personalized sequence exploration framework that optimizes event history processing using maximum relevance principles and self-supervised learning through Event Based Features (EBFs). Dynamix categorizes users-engagements at session and surface-levels by leveraging correlations between dwell-times and ad-conversion events. This enables targeted, event-level feature removal and selective feature boosting for certain user-segments, thereby yielding training and inference efficiency wins without sacrificing engaging ad-prediction accuracy. While, dynamic resource removal increases training and inference throughput by 1.15% and 1.8%, respectively, dynamic feature boosting provides 0.033 NE gains while boosting inference QPS by 4.2% over baseline models. These results demonstrate that Dynamix achieves significant cost efficiency and performance improvements in online user-sequence based recommendation models. Self-supervised user-segmentation and resource exploration can further boost complex feature selection strategies while optimizing for workflow and compute resources."}
{"id": "2511.18613", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18613", "abs": "https://arxiv.org/abs/2511.18613", "authors": ["Tabish Ali Rather", "S M Mahmudul Hasan Joy", "Nadezda Sukhorukova", "Federico Frascoli"], "title": "KAN vs LSTM Performance in Time Series Forecasting", "comment": "This paper compares Kolmogorov-Arnold Networks (KANs) and LSTMs for forecasting stock prices, highlighting that LSTMs provide superior predictive accuracy while KANs offer better interpretability and efficiency in limited-resource settings. Practical findings and future research directions are discussed", "summary": "This paper compares Kolmogorov-Arnold Networks (KAN) and Long Short-Term Memory networks (LSTM) for forecasting non-deterministic stock price data, evaluating predictive accuracy versus interpretability trade-offs using Root Mean Square Error (RMSE).LSTM demonstrates substantial superiority across all tested prediction horizons, confirming their established effectiveness for sequential data modelling. Standard KAN, while offering theoretical interpretability through the Kolmogorov-Arnold representation theorem, exhibits significantly higher error rates and limited practical applicability for time series forecasting. The results confirm LSTM dominance in accuracy-critical time series applications while identifying computational efficiency as KANs' primary advantage in resource-constrained scenarios where accuracy requirements are less stringent. The findings support LSTM adoption for practical financial forecasting while suggesting that continued research into specialised KAN architectures may yield future improvements."}
{"id": "2511.18334", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18334", "abs": "https://arxiv.org/abs/2511.18334", "authors": ["Chibuike E. Ugwu", "Roschelle Fritz", "Diane J. Cook", "Janardhan Rao Doppa"], "title": "Clinician-in-the-Loop Smart Home System to Detect Urinary Tract Infection Flare-Ups via Uncertainty-Aware Decision Support", "comment": "Accepted for publication at IAAI-26 / AAAI-26", "summary": "Urinary tract infection (UTI) flare-ups pose a significant health risk for older adults with chronic conditions. These infections often go unnoticed until they become severe, making early detection through innovative smart home technologies crucial. Traditional machine learning (ML) approaches relying on simple binary classification for UTI detection offer limited utility to nurses and practitioners as they lack insight into prediction uncertainty, hindering informed clinical decision-making. This paper presents a clinician-in-the-loop (CIL) smart home system that leverages ambient sensor data to extract meaningful behavioral markers, train robust predictive ML models, and calibrate them to enable uncertainty-aware decision support. The system incorporates a statistically valid uncertainty quantification method called Conformal-Calibrated Interval (CCI), which quantifies uncertainty and abstains from making predictions (\"I don't know\") when the ML model's confidence is low. Evaluated on real-world data from eight smart homes, our method outperforms baseline methods in recall and other classification metrics while maintaining the lowest abstention proportion and interval width. A survey of 42 nurses confirms that our system's outputs are valuable for guiding clinical decision-making, underscoring their practical utility in improving informed decisions and effectively managing UTIs and other condition flare-ups in older adults."}
{"id": "2511.18630", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.18630", "abs": "https://arxiv.org/abs/2511.18630", "authors": ["Amin Rakhsha", "Kanika Madan", "Tianyu Zhang", "Amir-massoud Farahmand", "Amir Khasahmadi"], "title": "Majority of the Bests: Improving Best-of-N via Bootstrapping", "comment": null, "summary": "Sampling multiple outputs from a Large Language Model (LLM) and selecting the most frequent (Self-consistency) or highest-scoring (Best-of-N) candidate is a popular approach to achieve higher accuracy in tasks with discrete final answers. Best-of-N (BoN) selects the output with the highest reward, and with perfect rewards, it often achieves near-perfect accuracy. With imperfect rewards from reward models, however, BoN fails to reliably find the correct answer and its performance degrades drastically. We consider the distribution of BoN's outputs and highlight that, although the correct answer does not usually have a probability close to one under imperfect rewards, it is often the most likely outcome. This suggests that the mode of this distribution can be more reliably correct than a sample from it. Based on this idea, we propose Majority-of-the-Bests (MoB), a novel selection mechanism that estimates the output distribution of BoN via bootstrapping and selects its mode. Experimental results across five benchmarks, three different base LLMs, and two reward models demonstrate consistent improvements over BoN in 25 out of 30 setups. We also provide theoretical results for the consistency of the bootstrapping. MoB serves as a simple, yet strong alternative to BoN and self-consistency, and more broadly, motivates further research in more nuanced selection mechanisms."}
{"id": "2511.18336", "categories": ["cs.LG", "cs.CV", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2511.18336", "abs": "https://arxiv.org/abs/2511.18336", "authors": ["Kaito Shiku", "Kazuya Nishimura", "Shinnosuke Matsuo", "Yasuhiro Kojima", "Ryoma Bise"], "title": "Auxiliary Gene Learning: Spatial Gene Expression Estimation by Auxiliary Gene Selection", "comment": "Accepted to Association for the Advancement of Artificial Intelligence (AAAI) 2026", "summary": "Spatial transcriptomics (ST) is a novel technology that enables the observation of gene expression at the resolution of individual spots within pathological tissues. ST quantifies the expression of tens of thousands of genes in a tissue section; however, heavy observational noise is often introduced during measurement. In prior studies, to ensure meaningful assessment, both training and evaluation have been restricted to only a small subset of highly variable genes, and genes outside this subset have also been excluded from the training process. However, since there are likely co-expression relationships between genes, low-expression genes may still contribute to the estimation of the evaluation target. In this paper, we propose $Auxiliary \\ Gene \\ Learning$ (AGL) that utilizes the benefit of the ignored genes by reformulating their expression estimation as auxiliary tasks and training them jointly with the primary tasks. To effectively leverage auxiliary genes, we must select a subset of auxiliary genes that positively influence the prediction of the target genes. However, this is a challenging optimization problem due to the vast number of possible combinations. To overcome this challenge, we propose Prior-Knowledge-Based Differentiable Top-$k$ Gene Selection via Bi-level Optimization (DkGSB), a method that ranks genes by leveraging prior knowledge and relaxes the combinatorial selection problem into a differentiable top-$k$ selection problem. The experiments confirm the effectiveness of incorporating auxiliary genes and show that the proposed method outperforms conventional auxiliary task learning approaches."}
{"id": "2511.18643", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18643", "abs": "https://arxiv.org/abs/2511.18643", "authors": ["Haojun Xia", "Xiaoxia Wu", "Jisen Li", "Robert Wu", "Junxiong Wang", "Jue Wang", "Chenxi Li", "Aman Singhal", "Alay Dilipbhai Shah", "Alpay Ariyak", "Donglin Zhuang", "Zhongzhu Zhou", "Ben Athiwaratkun", "Zhen Zheng", "Shuaiwen Leon Song"], "title": "Kitty: Accurate and Efficient 2-bit KV Cache Quantization with Dynamic Channel-wise Precision Boost", "comment": null, "summary": "The KV cache is a dominant memory bottleneck for LLM inference. While 4-bit KV quantization preserves accuracy, 2-bit often degrades it, especially on long-context reasoning. We close this gap via an algorithm-system co-design for mixed-precision KV caching: Kitty. On the algorithm side, extensive experiments show that Dynamic Channel-wise Precision Boost -- which ranks Key-cache channels by sensitivity and keeps only a small fraction at higher precision -- maintains near-zero loss in accuracy drop while approaching 2-bit memory. The main challenge is handling dynamic 4-bit channel boosts while keeping the page layout coalesced and the dequantization uniform, with no scattered reads or hard-coded masks. Kitty addresses these issues by decompose each mixed-precision Key page into two tensors with unified 2-bit precision. Based on this, Kitty provides a page-centric KV layout, Triton-compatible page dequantization kernels, and a lightweight runtime pipeline that preserves coalescing and avoids divergence. Across seven tasks and two model families (Qwen3, LLaMA3), Kitty cuts KV memory by nearly 8x with negligible accuracy loss, enabling up to 8x larger batches and 2.1x-4.1x higher throughput under the same memory budget. We release the full implementation of Kitty at https://github.com/Summer-Summer/Kitty."}
{"id": "2511.18394", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18394", "abs": "https://arxiv.org/abs/2511.18394", "authors": ["Chinmay Karkar", "Paras Chopra"], "title": "Future Is Unevenly Distributed: Forecasting Ability of LLMs Depends on What We're Asking", "comment": null, "summary": "Large Language Models (LLMs) demonstrate partial forecasting competence across social, political, and economic events. Yet, their predictive ability varies sharply with domain structure and prompt framing. We investigate how forecasting performance varies with different model families on real-world questions about events that happened beyond the model cutoff date. We analyze how context, question type, and external knowledge affect accuracy and calibration, and how adding factual news context modifies belief formation and failure modes. Our results show that forecasting ability is highly variable as it depends on what, and how, we ask."}
{"id": "2511.18653", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18653", "abs": "https://arxiv.org/abs/2511.18653", "authors": ["Nuo Xu", "Zhaoting Gong", "Ran Ran", "Jinwei Tang", "Wujie Wen", "Caiwen Ding"], "title": "FHE-Agent: Automating CKKS Configuration for Practical Encrypted Inference via an LLM-Guided Agentic Framework", "comment": null, "summary": "Fully Homomorphic Encryption (FHE), particularly the CKKS scheme, is a promising enabler for privacy-preserving MLaaS, but its practical deployment faces a prohibitive barrier: it heavily relies on domain expertise. Configuring CKKS involves a tightly coupled space of ring dimensions, modulus chains, and packing layouts. Without deep cryptographic knowledge to navigate these interactions, practitioners are restricted to compilers that rely on fixed heuristics. These \"one-shot\" tools often emit rigid configurations that are either severely over-provisioned in latency or fail to find a feasible solution entirely for deeper networks.\n  We present FHE-Agent, an agentic framework that automates this expert reasoning process. By coupling a Large Language Model (LLM) controller with a deterministic tool suite, FHE-Agent decomposes the search into global parameter selection and layer-wise bottleneck repair. The agents operate within a multi-fidelity workflow, pruning invalid regimes using cheap static analysis and reserving expensive encrypted evaluations for the most promising candidates.\n  We instantiate FHE-Agent on the Orion compiler and evaluate it on standard benchmarks (MLP, LeNet, LoLa) and deeper architectures (AlexNet). FHE-Agent consistently achieves better precision and lower latency than naïve search strategies. Crucially, it automatically discovers feasible, 128-bit secure configurations for complex models where baseline heuristics and one-shot prompts fail to produce a valid setup."}
{"id": "2511.18404", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18404", "abs": "https://arxiv.org/abs/2511.18404", "authors": ["Van Thuy Hoang", "O-Joun Lee"], "title": "Pre-training Graph Neural Networks on 2D and 3D Molecular Structures by using Multi-View Conditional Information Bottleneck", "comment": null, "summary": "Recent pre-training strategies for molecular graphs have attempted to use 2D and 3D molecular views as both inputs and self-supervised signals, primarily aligning graph-level representations. However, existing studies remain limited in addressing two main challenges of multi-view molecular learning: (1) discovering shared information between two views while diminishing view-specific information and (2) identifying and aligning important substructures, e.g., functional groups, which are crucial for enhancing cross-view consistency and model expressiveness. To solve these challenges, we propose a Multi-View Conditional Information Bottleneck framework, called MVCIB, for pre-training graph neural networks on 2D and 3D molecular structures in a self-supervised setting. Our idea is to discover the shared information while minimizing irrelevant features from each view under the MVCIB principle, which uses one view as a contextual condition to guide the representation learning of its counterpart. To enhance semantic and structural consistency across views, we utilize key substructures, e.g., functional groups and ego-networks, as anchors between the two views. Then, we propose a cross-attention mechanism that captures fine-grained correlations between the substructures to achieve subgraph alignment across views. Extensive experiments in four molecular domains demonstrated that MVCIB consistently outperforms baselines in both predictive performance and interpretability. Moreover, MVCIB achieved the 3d Weisfeiler-Lehman expressiveness power to distinguish not only non-isomorphic graphs but also different 3D geometries that share identical 2D connectivity, such as isomers."}
{"id": "2511.18670", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18670", "abs": "https://arxiv.org/abs/2511.18670", "authors": ["Rowan Bradbury", "Aniket Srinivasan Ashok", "Sai Ram Kasanagottu", "Gunmay Jhingran", "Shuai Meng"], "title": "Deterministic Continuous Replacement: Fast and Stable Module Replacement in Pretrained Transformers", "comment": "Accepted to NeurIPS 2025 ScaleOPT Workshop; 8 pages; includes figures", "summary": "Replacing modules in pretrained models, especially swapping quadratic self-attention for efficient attention alternatives, poses a hard optimization problem: cold-start reinitialization destabilizes frozen backbones. We isolate this core stability challenge in a controlled study. Deterministic Continuous Replacement (DCR) blends teacher and student outputs with a deterministic, annealed weight. Theoretically, DCR eliminates gate-induced gradient variance inherent to stochastic replacement. In a single-seed study, DCR attains faster convergence and stronger alignment than stochastic gating and distillation baselines on controlled attention replacement, establishing a foundation for heterogeneous operator swaps."}
{"id": "2511.18417", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18417", "abs": "https://arxiv.org/abs/2511.18417", "authors": ["Yoshihiro Maruyama"], "title": "Categorical Equivariant Deep Learning: Category-Equivariant Neural Networks and Universal Approximation Theorems", "comment": null, "summary": "We develop a theory of category-equivariant neural networks (CENNs) that unifies group/groupoid-equivariant networks, poset/lattice-equivariant networks, graph and sheaf neural networks. Equivariance is formulated as naturality in a topological category with Radon measures, formulating linear and nonlinear layers in the categorical setup. We prove the equivariant universal approximation theorem in the general setting: the class of finite-depth CENNs is dense in the space of continuous equivariant transformations. We instantiate the framework for groups/groupoids, posets/lattices, graphs and cellular sheaves, deriving universal approximation theorems for them in a systematic manner. Categorical equivariant deep learning thus allows us to expand the horizons of equivariant deep learning beyond group actions, encompassing not only geometric symmetries but also contextual and compositional symmetries."}
{"id": "2511.18692", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.18692", "abs": "https://arxiv.org/abs/2511.18692", "authors": ["Kichang Yang", "Seonjun Kim", "Minjae Kim", "Nairan Zhang", "Chi Zhang", "Youngki Lee"], "title": "VLM in a flash: I/O-Efficient Sparsification of Vision-Language Model via Neuron Chunking", "comment": null, "summary": "Edge deployment of large Vision-Language Models (VLMs) increasingly relies on flash-based weight offloading, where activation sparsification is used to reduce I/O overhead. However, conventional sparsification remains model-centric, selecting neurons solely by activation magnitude and neglecting how access patterns influence flash performance. We present Neuron Chunking, an I/O-efficient sparsification strategy that operates on chunks (i.e., groups of contiguous neurons in memory) and couples neuron importance with storage access cost. The method models I/O latency through a lightweight abstraction of access contiguity and selects chunks with high utility, defined as neuron importance normalized by estimated latency. By aligning sparsification decisions with the underlying storage behavior, Neuron Chunking improves I/O efficiency by up to 4.65x and 5.76x on Jetson Orin Nano and Jetson AGX Orin, respectively."}
{"id": "2511.18457", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18457", "abs": "https://arxiv.org/abs/2511.18457", "authors": ["Duncan Stothers", "Ben Stothers", "Emily Schaeffer", "Kishore Mulpuri"], "title": "Radiation-Preserving Selective Imaging for Pediatric Hip Dysplasia: A Cross-Modal Ultrasound-Xray Policy with Limited Labels", "comment": "Accepted (with oral presentation) to the AAAI 2026 AIMedHealth Bridge Program", "summary": "We study an ultrasound-first, radiation-preserving policy for developmental dysplasia of the hip (DDH) that requests a radiograph only when needed.\n  We (i) pretrain modality-specific encoders (ResNet-18) with SimSiam on a large unlabelled registry (37186 ultrasound; 19546 radiographs), (ii) freeze the backbones and fit small, measurement-faithful heads on DDH relevant landmarks and measurements (iii) calibrate a one sided conformal deferral rule on ultrasound predictions that provides finite sample coverage guarantees under exchangeability, using a held-out calibration set. Ultrasound heads predict Graf alpha, beta, and femoral head coverage; X-ray heads predict acetabular index (AI), center-edge (CE) angle and IHDI grade. On our held out labeled evaluation set, ultrasound measurement error is modest (e.g., alpha MAE ~= 9.7 degrees, coverage MAE ~= 14.0%), while radiographic probes achieve AI and CE MAEs of ~= 7.6 degrees and ~= 8.9 degrees, respectively. The calibrated US-only policy is explored across rule families (alpha-only; alpha OR coverage; alpha AND coverage), uncertainty inflation factors, and per-utility trade-offs using decision-curve analysis. Conservative settings yield high coverage with near-zero US-only rates; permissive settings (e.g., alpha OR coverage at larger deltas) achieve non-zero US-only throughput with expected coverage tradeoffs. The result is a simple, reproducible pipeline that turns limited labels into interpretable measurements and tunable selective imaging curves suitable for clinical handoff and future external validation."}
{"id": "2511.18772", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18772", "abs": "https://arxiv.org/abs/2511.18772", "authors": ["Zihan Wang", "Zhongkui Ma", "Xinguo Feng", "Chuan Yan", "Dongge Liu", "Ruoxi Sun", "Derui Wang", "Minhui Xue", "Guangdong Bai"], "title": "Re-Key-Free, Risky-Free: Adaptable Model Usage Control", "comment": null, "summary": "Deep neural networks (DNNs) have become valuable intellectual property of model owners, due to the substantial resources required for their development. To protect these assets in the deployed environment, recent research has proposed model usage control mechanisms to ensure models cannot be used without proper authorization. These methods typically lock the utility of the model by embedding an access key into its parameters. However, they often assume static deployment, and largely fail to withstand continual post-deployment model updates, such as fine-tuning or task-specific adaptation. In this paper, we propose ADALOC, to endow key-based model usage control with adaptability during model evolution. It strategically selects a subset of weights as an intrinsic access key, which enables all model updates to be confined to this key throughout the evolution lifecycle. ADALOC enables using the access key to restore the keyed model to the latest authorized states without redistributing the entire network (i.e., adaptation), and frees the model owner from full re-keying after each model update (i.e., lock preservation). We establish a formal foundation to underpin ADALOC, providing crucial bounds such as the errors introduced by updates restricted to the access key. Experiments on standard benchmarks, such as CIFAR-100, Caltech-256, and Flowers-102, and modern architectures, including ResNet, DenseNet, and ConvNeXt, demonstrate that ADALOC achieves high accuracy under significant updates while retaining robust protections. Specifically, authorized usages consistently achieve strong task-specific performance, while unauthorized usage accuracy drops to near-random guessing levels (e.g., 1.01% on CIFAR-100), compared to up to 87.01% without ADALOC. This shows that ADALOC can offer a practical solution for adaptive and protected DNN deployment in evolving real-world scenarios."}
{"id": "2511.18468", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18468", "abs": "https://arxiv.org/abs/2511.18468", "authors": ["Md Akil Raihan Iftee", "Mir Sazzat Hossain", "Rakibul Hasan Rajib", "Tariq Iqbal", "Md Mofijul Islam", "M Ashraful Amin", "Amin Ahsan Ali", "AKM Mahbubur Rahman"], "title": "SloMo-Fast: Slow-Momentum and Fast-Adaptive Teachers for Source-Free Continual Test-Time Adaptation", "comment": "38 pages, 38 tables, 16 figures", "summary": "Continual Test-Time Adaptation (CTTA) is crucial for deploying models in real-world applications with unseen, evolving target domains. Existing CTTA methods, however, often rely on source data or prototypes, limiting their applicability in privacy-sensitive and resource-constrained settings. Additionally, these methods suffer from long-term forgetting, which degrades performance on previously encountered domains as target domains shift. To address these challenges, we propose SloMo-Fast, a source-free, dual-teacher CTTA framework designed for enhanced adaptability and generalization. It includes two complementary teachers: the Slow-Teacher, which exhibits slow forgetting and retains long-term knowledge of previously encountered domains to ensure robust generalization, and the Fast-Teacher rapidly adapts to new domains while accumulating and integrating knowledge across them. This framework preserves knowledge of past domains and adapts efficiently to new ones. We also introduce Cyclic Test-Time Adaptation (Cyclic-TTA), a novel CTTA benchmark that simulates recurring domain shifts. Our extensive experiments demonstrate that SloMo-Fast consistently outperforms state-of-the-art methods across Cyclic-TTA, as well as ten other CTTA settings, highlighting its ability to both adapt and generalize across evolving and revisited domains."}
{"id": "2511.18841", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.18841", "abs": "https://arxiv.org/abs/2511.18841", "authors": ["Mincheol Jeon", "Euinam Huh"], "title": "Federated style aware transformer aggregation of representations", "comment": null, "summary": "Personalized Federated Learning (PFL) faces persistent challenges, including domain heterogeneity from diverse client data, data imbalance due to skewed participation, and strict communication constraints. Traditional federated learning often lacks personalization, as a single global model cannot capture client-specific characteristics, leading to biased predictions and poor generalization, especially for clients with highly divergent data distributions.\n  To address these issues, we propose FedSTAR, a style-aware federated learning framework that disentangles client-specific style factors from shared content representations. FedSTAR aggregates class-wise prototypes using a Transformer-based attention mechanism, allowing the server to adaptively weight client contributions while preserving personalization.\n  Furthermore, by exchanging compact prototypes and style vectors instead of full model parameters, FedSTAR significantly reduces communication overhead. Experimental results demonstrate that combining content-style disentanglement with attention-driven prototype aggregation improves personalization and robustness in heterogeneous environments without increasing communication cost."}
{"id": "2511.18474", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18474", "abs": "https://arxiv.org/abs/2511.18474", "authors": ["Winfried van den Dool", "Maksim Zhdanov", "Yuki M. Asano", "Max Welling"], "title": "Adaptive Mesh-Quantization for Neural PDE Solvers", "comment": null, "summary": "Physical systems commonly exhibit spatially varying complexity, presenting a significant challenge for neural PDE solvers. While Graph Neural Networks can handle the irregular meshes required for complex geometries and boundary conditions, they still apply uniform computational effort across all nodes regardless of the underlying physics complexity. This leads to inefficient resource allocation where computationally simple regions receive the same treatment as complex phenomena. We address this challenge by introducing Adaptive Mesh Quantization: spatially adaptive quantization across mesh node, edge, and cluster features, dynamically adjusting the bit-width used by a quantized model. We propose an adaptive bit-width allocation strategy driven by a lightweight auxiliary model that identifies high-loss regions in the input mesh. This enables dynamic resource distribution in the main model, where regions of higher difficulty are allocated increased bit-width, optimizing computational resource utilization. We demonstrate our framework's effectiveness by integrating it with two state-of-the-art models, MP-PDE and GraphViT, to evaluate performance across multiple tasks: 2D Darcy flow, large-scale unsteady fluid dynamics in 2D, steady-state Navier-Stokes simulations in 3D, and a 2D hyper-elasticity problem. Our framework demonstrates consistent Pareto improvements over uniformly quantized baselines, yielding up to 50% improvements in performance at the same cost."}
{"id": "2511.18842", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.18842", "abs": "https://arxiv.org/abs/2511.18842", "authors": ["Mohammad Nour Al Awad", "Sergey Ivanov", "Olga Tikhonova"], "title": "Optimizing LLM Code Suggestions: Feedback-Driven Timing with Lightweight State Bounds", "comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses", "summary": "Large Language Models (LLMs) have transformed code auto-completion by generating context-aware suggestions. Yet, deciding when to present these suggestions remains underexplored, often leading to interruptions or wasted inference calls. We propose an adaptive timing mechanism that dynamically adjusts the delay before offering a suggestion based on real-time developer feedback. Our suggested method combines a logistic transform of recent acceptance rates with a bounded delay range, anchored by a high-level binary prediction of the developer's cognitive state. In a two-month deployment with professional developers, our system improved suggestion acceptance from 4.9% with no delay to 15.4% with static delays, and to 18.6% with adaptive timing-while reducing blind rejections (rejections without being read) from 8.3% to 0.36%. Together, these improvements increase acceptance and substantially reduce wasted inference calls by 75%, making LLM-based code assistants more efficient and cost-effective in practice."}
{"id": "2511.18489", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18489", "abs": "https://arxiv.org/abs/2511.18489", "authors": ["Sai Puppala", "Ismail Hossain", "Md Jahangir Alam", "Sajedul Talukder"], "title": "Real-Time Personalized Content Adaptation through Matrix Factorization and Context-Aware Federated Learning", "comment": null, "summary": "Our study presents a multifaceted approach to enhancing user interaction and content relevance in social media platforms through a federated learning framework. We introduce personalized LLM Federated Learning and Context-based Social Media models. In our framework, multiple client entities receive a foundational GPT model, which is fine-tuned using locally collected social media data while ensuring data privacy through federated aggregation. Key modules focus on categorizing user-generated content, computing user persona scores, and identifying relevant posts from friends networks. By integrating a sophisticated social engagement quantification method with matrix factorization techniques, our system delivers real-time personalized content suggestions tailored to individual preferences. Furthermore, an adaptive feedback loop, alongside a robust readability scoring algorithm, significantly enhances the quality and relevance of the content presented to users. This comprehensive solution not only addresses the challenges of content filtering and recommendation but also fosters a more engaging social media experience while safeguarding user privacy, setting a new standard for personalized interactions in digital platforms."}
{"id": "2511.18846", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18846", "abs": "https://arxiv.org/abs/2511.18846", "authors": ["Yubo Wang", "Hui He", "Chaoxi Niu", "Zhendong Niu"], "title": "WaveTuner: Comprehensive Wavelet Subband Tuning for Time Series Forecasting", "comment": null, "summary": "Due to the inherent complexity, temporal patterns in real-world time series often evolve across multiple intertwined scales, including long-term periodicity, short-term fluctuations, and abrupt regime shifts. While existing literature has designed many sophisticated decomposition approaches based on the time or frequency domain to partition trend-seasonality components and high-low frequency components, an alternative line of approaches based on the wavelet domain has been proposed to provide a unified multi-resolution representation with precise time-frequency localization. However, most wavelet-based methods suffer from a persistent bias toward recursively decomposing only low-frequency components, severely underutilizing subtle yet informative high-frequency components that are pivotal for precise time series forecasting. To address this problem, we propose WaveTuner, a Wavelet decomposition framework empowered by full-spectrum subband Tuning for time series forecasting. Concretely, WaveTuner comprises two key modules: (i) Adaptive Wavelet Refinement module, that transforms time series into time-frequency coefficients, utilizes an adaptive router to dynamically assign subband weights, and generates subband-specific embeddings to support refinement; and (ii) Multi-Branch Specialization module, that employs multiple functional branches, each instantiated as a flexible Kolmogorov-Arnold Network (KAN) with a distinct functional order to model a specific spectral subband. Equipped with these modules, WaveTuner comprehensively tunes global trends and local variations within a unified time-frequency framework. Extensive experiments on eight real-world datasets demonstrate WaveTuner achieves state-of-the-art forecasting performance in time series forecasting."}
{"id": "2511.18515", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18515", "abs": "https://arxiv.org/abs/2511.18515", "authors": ["Ange-Clément Akazan", "Issa Karambal", "Jean Medard Ngnotchouye", "Abebe Geletu Selassie. W"], "title": "RRaPINNs: Residual Risk-Aware Physics Informed Neural Networks", "comment": null, "summary": "Physics-informed neural networks (PINNs) typically minimize average residuals, which can conceal large, localized errors. We propose Residual Risk-Aware Physics-Informed Neural Networks PINNs (RRaPINNs), a single-network framework that optimizes tail-focused objectives using Conditional Value-at-Risk (CVaR), we also introduced a Mean-Excess (ME) surrogate penalty to directly control worst-case PDE residuals. This casts PINN training as risk-sensitive optimization and links it to chance-constrained formulations. The method is effective and simple to implement. Across several partial differential equations (PDEs) such as Burgers, Heat, Korteweg-de-Vries, and Poisson (including a Poisson interface problem with a source jump at x=0.5) equations, RRaPINNs reduce tail residuals while maintaining or improving mean errors compared to vanilla PINNs, Residual-Based Attention and its variant using convolution weighting; the ME surrogate yields smoother optimization than a direct CVaR hinge. The chance constraint reliability level $α$ acts as a transparent knob trading bulk accuracy (lower $α$ ) for stricter tail control (higher $α$ ). We discuss the framework limitations, including memoryless sampling, global-only tail budgeting, and residual-centric risk, and outline remedies via persistent hard-point replay, local risk budgets, and multi-objective risk over BC/IC terms. RRaPINNs offer a practical path to reliability-aware scientific ML for both smooth and discontinuous PDEs."}
{"id": "2511.18849", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.18849", "abs": "https://arxiv.org/abs/2511.18849", "authors": ["Mohammad Nour Al Awad", "Sergey Ivanov", "Olga Tikhonova"], "title": "Pre-Filtering Code Suggestions using Developer Behavioral Telemetry to Optimize LLM-Assisted Programming", "comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses", "summary": "Large Language Models (LLMs) are increasingly integrated into code editors to provide AI-powered code suggestions. Yet many of these suggestions are ignored, resulting in wasted computation, increased latency, and unnecessary interruptions. We introduce a lightweight pre-filtering model that predicts the likelihood of suggestion acceptance before invoking the LLM, using only real-time developer telemetry such as typing speed, file navigation, and editing activity. Deployed in a production-grade Visual Studio Code plugin over four months of naturalistic use, our approach nearly doubled acceptance rates (18.4% -> 34.2%) while suppressing 35% of low-value LLM calls. These findings demonstrate that behavioral signals alone can meaningfully improve both user experience and system efficiency in LLM-assisted programming, highlighting the value of timing-aware, privacy-preserving adaptation mechanisms. The filter operates solely on pre-invocation editor telemetry and never inspects code or prompts."}
{"id": "2511.18519", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18519", "abs": "https://arxiv.org/abs/2511.18519", "authors": ["Xinlin Zhuang", "Yichen Li", "Xiwei Liu", "Haolin Yang", "Yifan Lu", "Ziyun Zou", "Yulong Li", "Huifa Li", "Dongliang Chen", "Qinglei Wang", "Weiyang Liu", "Ying Qian", "Jiangming Shi", "Imran Razzak"], "title": "CHIPS: Efficient CLIP Adaptation via Curvature-aware Hybrid Influence-based Data Selection", "comment": "preprint, under-review", "summary": "Adapting CLIP to vertical domains is typically approached by novel fine-tuning strategies or by continual pre-training (CPT) on large domain-specific datasets. Yet, data itself remains an underexplored factor in this process. We revisit this task from a data-centric perspective: Can effective data selection substitute for large-scale datasets in CPT? We introduce CHIPS (Curvature-aware Hybrid Influence in Projection Subspace), which assigns each image-text pair a utility score that integrates three complementary factors aligned with three goals: faithfulness via a curvature-aware, Newton-style alignment computed in CLIP's end-point subspace; scalability via an InfoNCE-aware curvature estimator with Johnson-Lindenstrauss (JL) sketching; and retention via a selection-aware relevance weight combined with learnability to balance target adaptation against general-domain preservation. We justify this design theoretically by proving a lower-bound guarantee on the proxy's correlation with full-parameter alignment and by characterizing the bias-variance trade-offs introduced by curvature mixing and JL sketching. We evaluate CHIPS empirically across various settings: 1) CHIPS attains state-of-the-art performance among selection baselines on 17 medical benchmarks, matches full-dataset CPT with 30% of the data, and outperforms half-dataset CPT using only 10%; 2) on 31 general-domain benchmarks, CHIPS yields the smallest performance drop under 10-30% data-retention budgets. Code, data, and checkpoints will be released."}
{"id": "2511.18854", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18854", "abs": "https://arxiv.org/abs/2511.18854", "authors": ["Yujing Wang", "Weize Hong"], "title": "Time Travel: LLM-Assisted Semantic Behavior Localization with Git Bisect", "comment": "submitted to Git Bisect SCALCOM 2025 Calgary (to be published)", "summary": "We present a novel framework that integrates Large Language Models (LLMs) into the Git bisect process for semantic fault localization. Traditional bisect assumes deterministic predicates and binary failure states assumptions often violated in modern software development due to flaky tests, nonmonotonic regressions, and semantic divergence from upstream repositories. Our system augments bisect traversal with structured chain of thought reasoning, enabling commit by commit analysis under noisy conditions. We evaluate multiple open source and proprietary LLMs for their suitability and fine tune DeepSeekCoderV2 using QLoRA on a curated dataset of semantically labeled diffs. We adopt a weak supervision workflow to reduce annotation overhead, incorporating human in the loop corrections and self consistency filtering. Experiments across multiple open source projects show a 6.4 point absolute gain in success rate from 74.2 to 80.6 percent, leading to significantly fewer failed traversals and by experiment up to 2x reduction in average bisect time. We conclude with discussions on temporal reasoning, prompt design, and finetuning strategies tailored for commit level behavior analysis."}
{"id": "2511.18521", "categories": ["cs.LG", "astro-ph.EP", "astro-ph.IM"], "pdf": "https://arxiv.org/pdf/2511.18521", "abs": "https://arxiv.org/abs/2511.18521", "authors": ["Core Francisco Park", "Manuel Perez-Carrasco", "Caroline Nowlan", "Cecilia Garraffo"], "title": "Hyperspectral Variational Autoencoders for Joint Data Compression and Component Extraction", "comment": null, "summary": "Geostationary hyperspectral satellites generate terabytes of data daily, creating critical challenges for storage, transmission, and distribution to the scientific community. We present a variational autoencoder (VAE) approach that achieves x514 compression of NASA's TEMPO satellite hyperspectral observations (1028 channels, 290-490nm) with reconstruction errors 1-2 orders of magnitude below the signal across all wavelengths. This dramatic data volume reduction enables efficient archival and sharing of satellite observations while preserving spectral fidelity. Beyond compression, we investigate to what extent atmospheric information is retained in the compressed latent space by training linear and nonlinear probes to extract Level-2 products (NO2, O3, HCHO, cloud fraction). Cloud fraction and total ozone achieve strong extraction performance (R^2 = 0.93 and 0.81 respectively), though these represent relatively straightforward retrievals given their distinct spectral signatures. In contrast, tropospheric trace gases pose genuine challenges for extraction (NO2 R^2 = 0.20, HCHO R^2 = 0.51) reflecting their weaker signals and complex atmospheric interactions. Critically, we find the VAE encodes atmospheric information in a semi-linear manner - nonlinear probes substantially outperform linear ones - and that explicit latent supervision during training provides minimal improvement, revealing fundamental encoding challenges for certain products. This work demonstrates that neural compression can dramatically reduce hyperspectral data volumes while preserving key atmospheric signals, addressing a critical bottleneck for next-generation Earth observation systems. Code - https://github.com/cfpark00/Hyperspectral-VAE"}
{"id": "2511.18868", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18868", "abs": "https://arxiv.org/abs/2511.18868", "authors": ["Dezhi Ran", "Shuxiao Xie", "Mingfang Ji", "Ziyue Hua", "Mengzhou Wu", "Yuan Cao", "Yuzhe Guo", "Yu Hao", "Linyi Li", "Yitao Hu", "Tao Xie"], "title": "KernelBand: Boosting LLM-based Kernel Optimization with a Hierarchical and Hardware-aware Multi-armed Bandit", "comment": "Work in progress", "summary": "High quality kernels are critical for reducing training and inference costs of Large Language Models (LLMs), yet they traditionally require significant expertise in hardware architecture and software optimization. While recent advances in LLM-based code generation show promise for complex optimization, existing methods struggle with the vast optimization space due to insufficient hardware domain knowledge, failing to effectively balance exploration and exploitation. We present KernelBand, a novel framework that formulates kernel optimization as a hierarchical multi-armed bandit problem, enabling LLM agents to strategically navigate the optimization space by treating kernel selection and optimization strategy application as sequential decision-making processes. Our approach leverages hardware profiling information to identify promising optimization strategies and employs runtime behavior clustering to reduce exploration overhead across kernel candidates. Extensive experiments on TritonBench demonstrate that KernelBand significantly outperforms state-of-the-art methods, achieving superior performance with fewer tokens while exhibiting consistent improvement without saturation as computational resources increase."}
{"id": "2511.18539", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18539", "abs": "https://arxiv.org/abs/2511.18539", "authors": ["Lingyu Jiang", "Lingyu Xu", "Peiran Li", "Qianwen Ge", "Dingyi Zhuang", "Shuo Xing", "Wenjing Chen", "Xiangbo Gao", "Ting-Hsuan Chen", "Xueying Zhan", "Xin Zhang", "Ziming Zhang", "Zhengzhong Tu", "Michael Zielewski", "Kazunori Yamada", "Fangzhou Lin"], "title": "TimePre: Bridging Accuracy, Efficiency, and Stability in Probabilistic Time-Series Forecasting", "comment": "15 pages, 5 figures, 6 tables", "summary": "Probabilistic Time-Series Forecasting (PTSF) is critical for uncertainty-aware decision making, but existing generative models, such as diffusion-based approaches, are computationally prohibitive due to expensive iterative sampling. Non-sampling frameworks like Multiple Choice Learning (MCL) offer an efficient alternative, but suffer from severe training instability and hypothesis collapse, which has historically hindered their performance. This problem is dramatically exacerbated when attempting to combine them with modern, efficient MLP-based backbones. To resolve this fundamental incompatibility, we propose TimePre, a novel framework that successfully unifies the efficiency of MLP-based models with the distributional flexibility of the MCL paradigm. The core of our solution is Stabilized Instance Normalization (SIN), a novel normalization layer that explicitly remedies this incompatibility. SIN stabilizes the hybrid architecture by correcting channel-wise statistical shifts, definitively resolving the catastrophic hypothesis collapse. Extensive experiments on six benchmark datasets demonstrate that TimePre achieves new state-of-the-art accuracy on key probabilistic metrics. Critically, TimePre achieves inference speeds orders of magnitude faster than sampling-based models and, unlike prior MCL work, demonstrates stable performance scaling. It thus bridges the long-standing gap between accuracy, efficiency, and stability in probabilistic forecasting."}
{"id": "2511.18871", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18871", "abs": "https://arxiv.org/abs/2511.18871", "authors": ["Jian Lu"], "title": "Periodic Asynchrony: An Effective Method for Accelerating On-Policy Reinforcement Learning", "comment": null, "summary": "Since the introduction of the GRPO algorithm, reinforcement learning (RL) has attracted increasing attention, with growing efforts to reproduce and apply it. However, training efficiency remains a critical challenge. In mainstream RL frameworks, inference and training are typically deployed on the same devices. While this approach reduces costs through resource consolidation, its synchronous execution imposes a computational coupling that prevents concurrent inference and training. In this study, we are returning to the strategy of separating inference and training deployment, and by introducing improvements in the data loader, we transform the conventional synchronous architecture into a periodically asynchronous framework, which allows for demand-driven, independent, and elastic scaling of each component, while the accuracy of the algorithm remains completely equivalent to the synchronization method, with both belonging to the on-policy strategy. It is worth emphasizing that we apply a unified tri-model architecture in the training phase, and we also proposed a shared-prompt attention mask to reduce repetitive computation. In practice, these works have achieved at least a threefold overall performance improvement in RL training on NPU platforms, indicating its potential for widespread application."}
{"id": "2511.18567", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18567", "abs": "https://arxiv.org/abs/2511.18567", "authors": ["Arya Shah", "Vaibhav Tripathi"], "title": "In Search of Goodness: Large Scale Benchmarking of Goodness Functions for the Forward-Forward Algorithm", "comment": "24 pages, 5 tables, 17 figures", "summary": "The Forward-Forward (FF) algorithm offers a biologically plausible alternative to backpropagation, enabling neural networks to learn through local updates. However, FF's efficacy relies heavily on the definition of \"goodness\", which is a scalar measure of neural activity. While current implementations predominantly utilize a simple sum-of-squares metric, it remains unclear if this default choice is optimal. To address this, we benchmarked 21 distinct goodness functions across four standard image datasets (MNIST, FashionMNIST, CIFAR-10, STL-10), evaluating classification accuracy, energy consumption, and carbon footprint. We found that certain alternative goodness functions inspired from various domains significantly outperform the standard baseline. Specifically, \\texttt{game\\_theoretic\\_local} achieved 97.15\\% accuracy on MNIST, \\texttt{softmax\\_energy\\_margin\\_local} reached 82.84\\% on FashionMNIST, and \\texttt{triplet\\_margin\\_local} attained 37.69\\% on STL-10. Furthermore, we observed substantial variability in computational efficiency, highlighting a critical trade-off between predictive performance and environmental cost. These findings demonstrate that the goodness function is a pivotal hyperparameter in FF design. We release our code on \\href{https://github.com/aryashah2k/In-Search-of-Goodness}{Github} for reference and reproducibility."}
{"id": "2511.18890", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18890", "abs": "https://arxiv.org/abs/2511.18890", "authors": ["Yonggan Fu", "Xin Dong", "Shizhe Diao", "Matthijs Van keirsbilck", "Hanrong Ye", "Wonmin Byeon", "Yashaswi Karnati", "Lucas Liebenwein", "Hannah Zhang", "Nikolaus Binder", "Maksim Khadkevich", "Alexander Keller", "Jan Kautz", "Yingyan Celine Lin", "Pavlo Molchanov"], "title": "Nemotron-Flash: Towards Latency-Optimal Hybrid Small Language Models", "comment": "Accepted by NeurIPS 2025", "summary": "Efficient deployment of small language models (SLMs) is essential for numerous real-world applications with stringent latency constraints. While previous work on SLM design has primarily focused on reducing the number of parameters to achieve parameter-optimal SLMs, parameter efficiency does not necessarily translate into proportional real-device speed-ups. This work aims to identify the key determinants of SLMs' real-device latency and offer generalizable principles and methodologies for SLM design and training when real-device latency is the primary consideration. Specifically, we identify two central architectural factors: depth-width ratios and operator choices. The former is crucial for small-batch-size latency, while the latter affects both latency and large-batch-size throughput. In light of this, we first study latency-optimal depth-width ratios, with the key finding that although deep-thin models generally achieve better accuracy under the same parameter budget, they may not lie on the accuracy-latency trade-off frontier. Next, we explore emerging efficient attention alternatives to evaluate their potential as candidate building operators. Using the identified promising operators, we construct an evolutionary search framework to automatically discover latency-optimal combinations of these operators within hybrid SLMs, thereby advancing the accuracy-latency frontier. In addition to architectural improvements, we further enhance SLM training using a weight normalization technique that enables more effective weight updates and improves final convergence. Combining these methods, we introduce a new family of hybrid SLMs, called Nemotron-Flash, which significantly advances the accuracy-efficiency frontier of state-of-the-art SLMs, e.g., achieving over +5.5% average accuracy, 1.3x/1.9x lower latency, and 18.7x/45.6x higher throughput compared to Qwen3-1.7B/0.6B, respectively."}
{"id": "2511.18571", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18571", "abs": "https://arxiv.org/abs/2511.18571", "authors": ["Jiazhen Hong", "Geoffrey Mackellar", "Soheila Ghane"], "title": "SAMBA: Toward a Long-Context EEG Foundation Model via Spatial Embedding and Differential Mamba", "comment": null, "summary": "Long-sequence electroencephalogram (EEG) modeling is essential for developing generalizable EEG representation models. This need arises from the high sampling rate of EEG data and the long recording durations required to capture extended neurological patterns in brain activity. Transformer-based models have shown promise in modeling short sequences of a few seconds; however, their quadratic complexity limits scalability to longer contexts. Moreover, variability in electrode montage across available datasets, along with inter-subject differences in brain signals, pose significant challenges to developing a generalizable and robust foundation model. We propose \\textit{SAMBA}, a self-supervised learning framework with a Mamba-based U-shaped encoder-decoder architecture, which effectively captures long-range temporal dependencies and spatial variability in EEG data. Leveraging the inherent ability of Mamba in processing long context sizes, we introduce: (1) \\textit{Temporal Semantic Random Masking} for semantic-level sequence reconstruction, (2) a \\textit{Multi-Head Differential Mamba} module to suppress redundancy and emphasize salient temporal structures, and (3) a \\textit{Spatial-Adaptive Input Embedding} that learns unified embeddings in a three-dimensional Euclidean space, enabling robustness across devices. Experiments on thirteen EEG datasets across diverse tasks, electrode configurations, and sequence durations demonstrate that SAMBA consistently outperforms state-of-the-art methods while maintaining low memory consumption and inference time. We also show the learned spatial weight maps from our embedding module align closely with task-relevant neurophysiological regions, demonstrating the learnability and interpretability of SAMBA. These results highlight SAMBA's scalability and practical potential as a foundation model for real-time brain-computer interface applications."}
{"id": "2511.18902", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18902", "abs": "https://arxiv.org/abs/2511.18902", "authors": ["Zengjie Hu", "Jiantao Qiu", "Tianyi Bai", "Haojin Yang", "Binhang Yuan", "Qi Jing", "Conghui He", "Wentao Zhang"], "title": "VADE: Variance-Aware Dynamic Sampling via Online Sample-Level Difficulty Estimation for Multimodal RL", "comment": null, "summary": "Group-based policy optimization methods like GRPO and GSPO have become standard for training multimodal models, leveraging group-wise rollouts and relative advantage estimation. However, they suffer from a critical \\emph{gradient vanishing} problem when all responses within a group receive identical rewards, causing advantage estimates to collapse and training signals to diminish. Existing attempts to mitigate this issue fall into two paradigms: filtering-based and sampling-based methods. Filtering-based methods first generate rollouts broadly and then retroactively filter out uninformative groups, leading to substantial computational overhead. Sampling-based methods proactively select effective samples before rollout but rely on static criteria or prior dataset knowledge, lacking real-time adaptability. To address these issues, we propose \\textbf{VADE}, a \\textbf{V}ariance-\\textbf{A}ware \\textbf{D}ynamic sampling framework via online sample-level difficulty \\textbf{E}stimation. Our framework integrates three key components: online sample-level difficulty estimation using Beta distributions, a Thompson sampler that maximizes information gain through the estimated correctness probability, and a two-scale prior decay mechanism that maintains robust estimation under policy evolution. This three components design enables VADE to dynamically select the most informative samples, thereby amplifying training signals while eliminating extra rollout costs. Extensive experiments on multimodal reasoning benchmarks show that VADE consistently outperforms strong baselines in both performance and sample efficiency, while achieving a dramatic reduction in computational overhead. More importantly, our framework can serves as a plug-and-play component to be seamlessly integrated into existing group-based RL algorithms. Code and models are available at https://VADE-RL.github.io."}
{"id": "2511.18593", "categories": ["cs.LG", "eess.SY", "math.SP"], "pdf": "https://arxiv.org/pdf/2511.18593", "abs": "https://arxiv.org/abs/2511.18593", "authors": ["Milad Siami"], "title": "Generative Myopia: Why Diffusion Models Fail at Structure", "comment": null, "summary": "Graph Diffusion Models (GDMs) optimize for statistical likelihood, implicitly acting as \\textbf{frequency filters} that favor abundant substructures over spectrally critical ones. We term this phenomenon \\textbf{Generative Myopia}. In combinatorial tasks like graph sparsification, this leads to the catastrophic removal of ``rare bridges,'' edges that are structurally mandatory ($R_{\\text{eff}} \\approx 1$) but statistically scarce. We prove theoretically and empirically that this failure is driven by \\textbf{Gradient Starvation}: the optimization landscape itself suppresses rare structural signals, rendering them unlearnable regardless of model capacity. To resolve this, we introduce \\textbf{Spectrally-Weighted Diffusion}, which re-aligns the variational objective using Effective Resistance. We demonstrate that spectral priors can be amortized into the training phase with zero inference overhead. Our method eliminates myopia, matching the performance of an optimal Spectral Oracle and achieving \\textbf{100\\% connectivity} on adversarial benchmarks where standard diffusion fails completely (0\\%)."}
{"id": "2511.18903", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18903", "abs": "https://arxiv.org/abs/2511.18903", "authors": ["Kairong Luo", "Zhenbo Sun", "Haodong Wen", "Xinyu Shi", "Jiarui Cui", "Chenyi Dang", "Kaifeng Lyu", "Wenguang Chen"], "title": "How Learning Rate Decay Wastes Your Best Data in Curriculum-Based LLM Pretraining", "comment": null, "summary": "Due to the scarcity of high-quality data, large language models (LLMs) are often trained on mixtures of data with varying quality levels, even after sophisticated data curation. A natural approach to better leverage high-quality data is curriculum-based pretraining, where the model is trained on data sorted in ascending order of quality as determined by a quality metric. However, prior studies have reported limited improvements from such curriculum-based pretraining strategies. This work identifies a critical factor constraining these methods: the incompatibility between the ascending data quality order and the decaying learning rate (LR) schedule. We find that while curriculum-based training substantially outperforms random shuffling when using a constant LR, its advantage diminishes under standard LR decay schedules. Our experiments show this incompatibility can be mitigated by two simple strategies: (1) employing a more moderate LR decay schedule, where the final LR is only moderately smaller than the peak LR, and (2) replacing LR decay with model averaging, i.e., computing a weighted average of the final few checkpoints. By combining these strategies, we improve the average score on a suite of standard benchmarks by 1.64% over random shuffling, without additional data refinement. Validated on 1.5B-parameter models trained over 30B tokens with various data-quality metrics, our findings call for a re-evaluation of curriculum-based LLM pretraining and underscore the potential of co-designing data curricula with optimization methods."}
{"id": "2511.18611", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.18611", "abs": "https://arxiv.org/abs/2511.18611", "authors": ["Mengdi Wang", "Efe Bozkir", "Enkelejda Kasneci"], "title": "CycleSL: Server-Client Cyclical Update Driven Scalable Split Learning", "comment": "The IEEE/CVF Winter Conference on Applications of Computer Vision 2026 (WACV-26)", "summary": "Split learning emerges as a promising paradigm for collaborative distributed model training, akin to federated learning, by partitioning neural networks between clients and a server without raw data exchange. However, sequential split learning suffers from poor scalability, while parallel variants like parallel split learning and split federated learning often incur high server resource overhead due to model duplication and aggregation, and generally exhibit reduced model performance and convergence owing to factors like client drift and lag. To address these limitations, we introduce CycleSL, a novel aggregation-free split learning framework that enhances scalability and performance and can be seamlessly integrated with existing methods. Inspired by alternating block coordinate descent, CycleSL treats server-side training as an independent higher-level machine learning task, resampling client-extracted features (smashed data) to mitigate heterogeneity and drift. It then performs cyclical updates, namely optimizing the server model first, followed by client updates using the updated server for gradient computation. We integrate CycleSL into previous algorithms and benchmark them on five publicly available datasets with non-iid data distribution and partial client attendance. Our empirical findings highlight the effectiveness of CycleSL in enhancing model performance. Our source code is available at https://gitlab.lrz.de/hctl/CycleSL."}
{"id": "2511.18924", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18924", "abs": "https://arxiv.org/abs/2511.18924", "authors": ["Arina Kharlamova", "Jiawen Liu", "Tianyi Zhang", "Xinrui Yang", "Humaid Alqasimi", "Youcheng Sun", "Chun Jason Xue"], "title": "LLM-Driven Kernel Evolution: Automating Driver Updates in Linux", "comment": null, "summary": "Linux kernel evolution breaks drivers through API/ABI changes, semantic shifts, and security-hardening updates. We introduce DRIVEBENCH, an executable corpus of kernel$\\rightarrow$driver co-evolution cases, and AUTODRIVER, a closed-loop, LLM-driven system for automating driver maintenance. The system integrates prompt engineering, multi-agent collaboration, static analysis, and iterative validation to ensure that generated patches are not only syntactically correct but also functionally and semantically consistent with kernel conventions. The corpus spans v5.10-v6.10 with 235 validated cases drawn from 612 candidates. In evaluation across 55 cases, AUTODRIVER achieves 56.4% compilation success; QEMU-based boot verification indicates that compiled patches preserve driver initialization in most instances. By releasing DRIVEBENCH and tooling, we enable reproducible research and a practical route to continuous, safe co-evolution of drivers with the Linux kernel."}
{"id": "2511.18613", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18613", "abs": "https://arxiv.org/abs/2511.18613", "authors": ["Tabish Ali Rather", "S M Mahmudul Hasan Joy", "Nadezda Sukhorukova", "Federico Frascoli"], "title": "KAN vs LSTM Performance in Time Series Forecasting", "comment": "This paper compares Kolmogorov-Arnold Networks (KANs) and LSTMs for forecasting stock prices, highlighting that LSTMs provide superior predictive accuracy while KANs offer better interpretability and efficiency in limited-resource settings. Practical findings and future research directions are discussed", "summary": "This paper compares Kolmogorov-Arnold Networks (KAN) and Long Short-Term Memory networks (LSTM) for forecasting non-deterministic stock price data, evaluating predictive accuracy versus interpretability trade-offs using Root Mean Square Error (RMSE).LSTM demonstrates substantial superiority across all tested prediction horizons, confirming their established effectiveness for sequential data modelling. Standard KAN, while offering theoretical interpretability through the Kolmogorov-Arnold representation theorem, exhibits significantly higher error rates and limited practical applicability for time series forecasting. The results confirm LSTM dominance in accuracy-critical time series applications while identifying computational efficiency as KANs' primary advantage in resource-constrained scenarios where accuracy requirements are less stringent. The findings support LSTM adoption for practical financial forecasting while suggesting that continued research into specialised KAN architectures may yield future improvements."}
{"id": "2511.18930", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18930", "abs": "https://arxiv.org/abs/2511.18930", "authors": ["Salah Eddine Choutri", "Prajwal Chauhan", "Othmane Mazhar", "Saif Eddin Jabari"], "title": "Learning Solution Operators for Partial Differential Equations via Monte Carlo-Type Approximation", "comment": "NeurIPS 2025 Workshop on Machine Learning and the Physical Sciences", "summary": "The Monte Carlo-type Neural Operator (MCNO) introduces a lightweight architecture for learning solution operators for parametric PDEs by directly approximating the kernel integral using a Monte Carlo approach. Unlike Fourier Neural Operators, MCNO makes no spectral or translation-invariance assumptions. The kernel is represented as a learnable tensor over a fixed set of randomly sampled points. This design enables generalization across multiple grid resolutions without relying on fixed global basis functions or repeated sampling during training. Experiments on standard 1D PDE benchmarks show that MCNO achieves competitive accuracy with low computational cost, providing a simple and practical alternative to spectral and graph-based neural operators."}
{"id": "2511.18615", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.18615", "abs": "https://arxiv.org/abs/2511.18615", "authors": ["Jiawei Hu", "Javier A. Barria"], "title": "Bayesian-based Online Label Shift Estimation with Dynamic Dirichlet Priors", "comment": "13 pages, submitted to IEEE journal for possible publication", "summary": "Label shift, a prevalent challenge in supervised learning, arises when the class prior distribution of test data differs from that of training data, leading to significant degradation in classifier performance. To accurately estimate the test priors and enhance classification accuracy, we propose a Bayesian framework for label shift estimation, termed Full Maximum A Posterior Label Shift (FMAPLS), along with its online version, online-FMAPLS. Leveraging batch and online Expectation-Maximization (EM) algorithms, these methods jointly and dynamically optimize Dirichlet hyperparameters $\\boldsymbolα$ and class priors $\\boldsymbolπ$, thereby overcoming the rigid constraints of the existing Maximum A Posterior Label Shift (MAPLS) approach. Moreover, we introduce a linear surrogate function (LSF) to replace gradient-based hyperparameter updates, yielding closed-form solutions that reduce computational complexity while retaining asymptotic equivalence. The online variant substitutes the batch E-step with a stochastic approximation, enabling real-time adaptation to streaming data. Furthermore, our theoretical analysis reveals a fundamental trade-off between online convergence rate and estimation accuracy. Extensive experiments on CIFAR100 and ImageNet datasets under shuffled long-tail and Dirichlet test priors demonstrate that FMAPLS and online-FMAPLS respectively achieve up to 40% and 12% lower KL divergence and substantial improvements in post-shift accuracy over state-of-the-art baselines, particularly under severe class imbalance and distributional uncertainty. These results confirm the robustness, scalability, and suitability of the proposed methods for large-scale and dynamic learning scenarios."}
{"id": "2511.18933", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18933", "abs": "https://arxiv.org/abs/2511.18933", "authors": ["Ryan Wong", "Hosea David Yu Fei Ng", "Dhananjai Sharma", "Glenn Jun Jie Ng", "Kavishvaran Srinivasan"], "title": "Defending Large Language Models Against Jailbreak Exploits with Responsible AI Considerations", "comment": "20 pages including appendix; technical report; NeurIPS 2024 style", "summary": "Large Language Models (LLMs) remain susceptible to jailbreak exploits that bypass safety filters and induce harmful or unethical behavior. This work presents a systematic taxonomy of existing jailbreak defenses across prompt-level, model-level, and training-time interventions, followed by three proposed defense strategies. First, a Prompt-Level Defense Framework detects and neutralizes adversarial inputs through sanitization, paraphrasing, and adaptive system guarding. Second, a Logit-Based Steering Defense reinforces refusal behavior through inference-time vector steering in safety-sensitive layers. Third, a Domain-Specific Agent Defense employs the MetaGPT framework to enforce structured, role-based collaboration and domain adherence. Experiments on benchmark datasets show substantial reductions in attack success rate, achieving full mitigation under the agent-based defense. Overall, this study highlights how jailbreaks pose a significant security threat to LLMs and identifies key intervention points for prevention, while noting that defense strategies often involve trade-offs between safety, performance, and scalability. Code is available at: https://github.com/Kuro0911/CS5446-Project"}
{"id": "2511.18630", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.18630", "abs": "https://arxiv.org/abs/2511.18630", "authors": ["Amin Rakhsha", "Kanika Madan", "Tianyu Zhang", "Amir-massoud Farahmand", "Amir Khasahmadi"], "title": "Majority of the Bests: Improving Best-of-N via Bootstrapping", "comment": null, "summary": "Sampling multiple outputs from a Large Language Model (LLM) and selecting the most frequent (Self-consistency) or highest-scoring (Best-of-N) candidate is a popular approach to achieve higher accuracy in tasks with discrete final answers. Best-of-N (BoN) selects the output with the highest reward, and with perfect rewards, it often achieves near-perfect accuracy. With imperfect rewards from reward models, however, BoN fails to reliably find the correct answer and its performance degrades drastically. We consider the distribution of BoN's outputs and highlight that, although the correct answer does not usually have a probability close to one under imperfect rewards, it is often the most likely outcome. This suggests that the mode of this distribution can be more reliably correct than a sample from it. Based on this idea, we propose Majority-of-the-Bests (MoB), a novel selection mechanism that estimates the output distribution of BoN via bootstrapping and selects its mode. Experimental results across five benchmarks, three different base LLMs, and two reward models demonstrate consistent improvements over BoN in 25 out of 30 setups. We also provide theoretical results for the consistency of the bootstrapping. MoB serves as a simple, yet strong alternative to BoN and self-consistency, and more broadly, motivates further research in more nuanced selection mechanisms."}
{"id": "2511.18936", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18936", "abs": "https://arxiv.org/abs/2511.18936", "authors": ["Santhosh G S", "Saurav Prakash", "Balaraman Ravindran"], "title": "SWAN: Sparse Winnowed Attention for Reduced Inference Memory via Decompression-Free KV-Cache Compression", "comment": null, "summary": "Large Language Models (LLMs) face a significant bottleneck during autoregressive inference due to the massive memory footprint of the Key-Value (KV) cache. Existing compression techniques like token eviction, quantization, or other low-rank methods often risk information loss, have fixed limits, or introduce significant computational overhead from explicit decompression steps. In this work, we introduce SWAN, a novel, fine-tuning-free framework that eliminates this overhead. Our method uses an offline orthogonal matrix to rotate and prune the KV-cache, which is then used directly in the attention computation without any reconstruction. Our extensive experiments demonstrate that SWAN, augmented with a small dense buffer, offers a robust trade-off, maintaining performance close to the uncompressed baseline even at aggressive 50-60% memory savings per-token on KV-cache. A key advantage is its runtime-tunable compression level, allowing operators to dynamically adjust the memory footprint, a flexibility absent in methods requiring fixed offline configurations. This combination of a decompression-free design, high performance under compression, and adaptability makes SWAN a practical and efficient solution for serving LLMs with long contexts."}
{"id": "2511.18631", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18631", "abs": "https://arxiv.org/abs/2511.18631", "authors": ["Kiyan Rezaee", "Morteza Ziabakhsh", "Niloofar Nikfarjam", "Mohammad M. Ghassemi", "Yazdan Rezaee Jouryabi", "Sadegh Eskandari", "Reza Lashgari"], "title": "FOS: A Large-Scale Temporal Graph Benchmark for Scientific Interdisciplinary Link Prediction", "comment": "21 pages, 10 figures", "summary": "Interdisciplinary scientific breakthroughs mostly emerge unexpectedly, and forecasting the formation of novel research fields remains a major challenge. We introduce FOS (Future Of Science), a comprehensive time-aware graph-based benchmark that reconstructs annual co-occurrence graphs of 65,027 research sub-fields (spanning 19 general domains) over the period 1827-2024. In these graphs, edges denote the co-occurrence of two fields in a single publication and are timestamped with the corresponding publication year. Nodes are enriched with semantic embeddings, and edges are characterized by temporal and topological descriptors. We formulate the prediction of new field-pair linkages as a temporal link-prediction task, emphasizing the \"first-time\" connections that signify pioneering interdisciplinary directions. Through extensive experiments, we evaluate a suite of state-of-the-art temporal graph architectures under multiple negative-sampling regimes and show that (i) embedding long-form textual descriptions of fields significantly boosts prediction accuracy, and (ii) distinct model classes excel under different evaluation settings. Case analyses show that top-ranked link predictions on FOS align with field pairings that emerge in subsequent years of academic publications. We publicly release FOS, along with its temporal data splits and evaluation code, to establish a reproducible benchmark for advancing research in predicting scientific frontiers."}
{"id": "2511.18958", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18958", "abs": "https://arxiv.org/abs/2511.18958", "authors": ["Qisen Chai", "Yansong Wang", "Junjie Huang", "Tao Jia"], "title": "Learning to Compress Graphs via Dual Agents for Consistent Topological Robustness Evaluation", "comment": null, "summary": "As graph-structured data grow increasingly large, evaluating their robustness under adversarial attacks becomes computationally expensive and difficult to scale. To address this challenge, we propose to compress graphs into compact representations that preserve both topological structure and robustness profile, enabling efficient and reliable evaluation.We propose Cutter, a dual-agent reinforcement learning framework composed of a Vital Detection Agent (VDA) and a Redundancy Detection Agent (RDA), which collaboratively identify structurally vital and redundant nodes for guided compression. Cutter incorporates three key strategies to enhance learning efficiency and compression quality: trajectory-level reward shaping to transform sparse trajectory returns into dense, policy-equivalent learning signals; prototype-based shaping to guide decisions using behavioral patterns from both highand low-return trajectories; and cross-agent imitation to enable safer and more transferable exploration. Experiments on multiple real-world graphs demonstrate that Cutter generates compressed graphs that retain essential static topological properties and exhibit robustness degradation trends highly consistent with the original graphs under various attack scenarios, thereby significantly improving evaluation efficiency without compromising assessment fidelity."}
{"id": "2511.18632", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18632", "abs": "https://arxiv.org/abs/2511.18632", "authors": ["Jan Benedikt Ruhland", "Doguhan Bahcivan", "Jan-Peter Sowa", "Ali Canbay", "Dominik Heider"], "title": "The Locally Deployable Virtual Doctor: LLM Based Human Interface for Automated Anamnesis and Database Conversion", "comment": null, "summary": "Recent advances in large language models made it possible to achieve high conversational performance with substantially reduced computational demands, enabling practical on-site deployment in clinical environments. Such progress allows for local integration of AI systems that uphold strict data protection and patient privacy requirements, yet their secure implementation in medicine necessitates careful consideration of ethical, regulatory, and technical constraints.\n  In this study, we introduce MedChat, a locally deployable virtual physician framework that integrates an LLM-based medical chatbot with a diffusion-driven avatar for automated and structured anamnesis. The chatbot was fine-tuned using a hybrid corpus of real and synthetically generated medical dialogues, while model efficiency was optimized via Low-Rank Adaptation. A secure and isolated database interface was implemented to ensure complete separation between patient data and the inference process. The avatar component was realized through a conditional diffusion model operating in latent space, trained on researcher video datasets and synchronized with mel-frequency audio features for realistic speech and facial animation.\n  Unlike existing cloud-based systems, this work demonstrates the feasibility of a fully offline, locally deployable LLM-diffusion framework for clinical anamnesis. The autoencoder and diffusion networks exhibited smooth convergence, and MedChat achieved stable fine-tuning with strong generalization to unseen data. The proposed system thus provides a privacy-preserving, resource-efficient foundation for AI-assisted clinical anamnesis, also in low-cost settings."}
{"id": "2511.18977", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18977", "abs": "https://arxiv.org/abs/2511.18977", "authors": ["Xin Yuan", "Siqi Li", "Jiateng Wei", "Chengrui Zhu", "Yanming Wu", "Qingpeng Li", "Jiajun Lv", "Xiaoke Lan", "Jun Chen", "Yong Liu"], "title": "FastForward Pruning: Efficient LLM Pruning via Single-Step Reinforcement Learning", "comment": "5 pages, 2 figures, 4 tables", "summary": "Pruning is an effective method for compressing Large Language Models, but finding an optimal, non-uniform layer-wise sparsity allocation remains a key challenge. While heuristic methods are fast but yield suboptimal performance, more powerful search-based approaches like Reinforcement Learning are often hindered by prohibitive computational costs on large-scale models. To overcome this efficiency barrier, we propose FastForward Pruning. Its core is a decoupled, single-step RL framework that separates policy optimization from the complex budget satisfaction problem. Such a decoupling is crucial for efficiently searching the vast policy space of LLMs. This curriculum-based strategy begins with low-cost, simple tasks and gradually increases in complexity, significantly reducing the search's computational overhead. Evaluated on the LLaMA, Mistral, and OPT model families, our framework discovers pruning policies that achieve superior performance over strong heuristic baselines. Crucially, when compared to other search-based algorithms, our method achieves competitive or superior results at a fraction of the computational cost, demonstrating a clear advantage in search efficiency."}
{"id": "2511.18643", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18643", "abs": "https://arxiv.org/abs/2511.18643", "authors": ["Haojun Xia", "Xiaoxia Wu", "Jisen Li", "Robert Wu", "Junxiong Wang", "Jue Wang", "Chenxi Li", "Aman Singhal", "Alay Dilipbhai Shah", "Alpay Ariyak", "Donglin Zhuang", "Zhongzhu Zhou", "Ben Athiwaratkun", "Zhen Zheng", "Shuaiwen Leon Song"], "title": "Kitty: Accurate and Efficient 2-bit KV Cache Quantization with Dynamic Channel-wise Precision Boost", "comment": null, "summary": "The KV cache is a dominant memory bottleneck for LLM inference. While 4-bit KV quantization preserves accuracy, 2-bit often degrades it, especially on long-context reasoning. We close this gap via an algorithm-system co-design for mixed-precision KV caching: Kitty. On the algorithm side, extensive experiments show that Dynamic Channel-wise Precision Boost -- which ranks Key-cache channels by sensitivity and keeps only a small fraction at higher precision -- maintains near-zero loss in accuracy drop while approaching 2-bit memory. The main challenge is handling dynamic 4-bit channel boosts while keeping the page layout coalesced and the dequantization uniform, with no scattered reads or hard-coded masks. Kitty addresses these issues by decompose each mixed-precision Key page into two tensors with unified 2-bit precision. Based on this, Kitty provides a page-centric KV layout, Triton-compatible page dequantization kernels, and a lightweight runtime pipeline that preserves coalescing and avoids divergence. Across seven tasks and two model families (Qwen3, LLaMA3), Kitty cuts KV memory by nearly 8x with negligible accuracy loss, enabling up to 8x larger batches and 2.1x-4.1x higher throughput under the same memory budget. We release the full implementation of Kitty at https://github.com/Summer-Summer/Kitty."}
{"id": "2511.18987", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18987", "abs": "https://arxiv.org/abs/2511.18987", "authors": ["Donghu Kim"], "title": "Dynamic Mixture of Experts Against Severe Distribution Shifts", "comment": null, "summary": "The challenge of building neural networks that can continuously learn and adapt to evolving data streams is central to the fields of continual learning (CL) and reinforcement learning (RL). This lifelong learning problem is often framed in terms of the plasticity-stability dilemma, focusing on issues like loss of plasticity and catastrophic forgetting. Unlike neural networks, biological brains maintain plasticity through capacity growth, inspiring researchers to explore similar approaches in artificial networks, such as adding capacity dynamically. Prior solutions often lack parameter efficiency or depend on explicit task indices, but Mixture-of-Experts (MoE) architectures offer a promising alternative by specializing experts for distinct distributions. This paper aims to evaluate a DynamicMoE approach for continual and reinforcement learning environments and benchmark its effectiveness against existing network expansion methods."}
{"id": "2511.18660", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.18660", "abs": "https://arxiv.org/abs/2511.18660", "authors": ["Mostafa Mozafari", "Farooq Ahmad Wani", "Maria Sofia Bucarelli", "Fabrizio Silvestri"], "title": "Subtract the Corruption: Training-Data-Free Corrective Machine Unlearning using Task Arithmetic", "comment": null, "summary": "Corrupted training data are ubiquitous. Corrective Machine Unlearning (CMU) seeks to remove the influence of such corruption post-training. Prior CMU typically assumes access to identified corrupted training samples (a ``forget set''). However, in many real-world scenarios the training data are no longer accessible. We formalize \\emph{source-free} CMU, where the original training data are unavailable and, consequently, no forget set of identified corrupted training samples can be specified. Instead, we assume a small proxy (surrogate) set of corrupted samples that reflect the suspected corruption type without needing to be the original training samples. In this stricter setting, methods relying on forget set are ineffective or narrow in scope. We introduce \\textit{Corrective Unlearning in Task Space} (CUTS), a lightweight weight space correction method guided by the proxy set using task arithmetic principles. CUTS treats the clean and the corruption signal as distinct tasks. Specifically, we briefly fine-tune the corrupted model on the proxy to amplify the corruption mechanism in the weight space, compute the difference between the corrupted and fine-tuned weights as a proxy task vector, and subtract a calibrated multiple of this vector to cancel the corruption. Without access to clean data or a forget set, CUTS recovers a large fraction of the lost utility under label noise and, for backdoor triggers, nearly eliminates the attack with minimal damage to utility, outperforming state-of-the-art specialized CMU methods in source-free setting."}
{"id": "2511.19023", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19023", "abs": "https://arxiv.org/abs/2511.19023", "authors": ["Yuting Gao", "Weihao Chen", "Lan Wang", "Ruihan Xu", "Qingpei Guo"], "title": "OrdMoE: Preference Alignment via Hierarchical Expert Group Ranking in Multimodal Mixture-of-Experts LLMs", "comment": null, "summary": "Preference learning has recently emerged as a pivotal strategy for post-training alignment of Multimodal Large Language Models (MLLMs). However, existing approaches predominantly rely on external human-annotated preference data, which is costly and labor-intensive to collect. In this work, we propose OrdMoE, a novel preference alignment framework that bypasses the reliance on external human preferences entirely by leveraging intrinsic signals within Mixture-of-Experts (MoE) architectures. Specifically, we observe that the router's expert selection scores implicitly encode a quality-aware ranking of responses (i.e. higher-scoring experts consistently generate higher-quality outputs). Building on this insight, OrdMoE constructs an internal preference hierarchy by grouping experts into ranked tiers based on their per-token routing scores and activating each tier separately to produce a sequence of responses with increasing quality. This yields a zero-cost, self-supervised preference ordering over generated responses, which can be directly optimized using standard preference learning objectives. Extensive experiments across multiple multimodal benchmarks demnstrate that OrdMoE significantly enhances both alignment and overall performance of multimodal Mixture-of-Experts LLMs, achieving competitive results without requiring any human-annotated preference data."}
{"id": "2511.18670", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18670", "abs": "https://arxiv.org/abs/2511.18670", "authors": ["Rowan Bradbury", "Aniket Srinivasan Ashok", "Sai Ram Kasanagottu", "Gunmay Jhingran", "Shuai Meng"], "title": "Deterministic Continuous Replacement: Fast and Stable Module Replacement in Pretrained Transformers", "comment": "Accepted to NeurIPS 2025 ScaleOPT Workshop; 8 pages; includes figures", "summary": "Replacing modules in pretrained models, especially swapping quadratic self-attention for efficient attention alternatives, poses a hard optimization problem: cold-start reinitialization destabilizes frozen backbones. We isolate this core stability challenge in a controlled study. Deterministic Continuous Replacement (DCR) blends teacher and student outputs with a deterministic, annealed weight. Theoretically, DCR eliminates gate-induced gradient variance inherent to stochastic replacement. In a single-seed study, DCR attains faster convergence and stronger alignment than stochastic gating and distillation baselines on controlled attention replacement, establishing a foundation for heterogeneous operator swaps."}
{"id": "2511.19066", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19066", "abs": "https://arxiv.org/abs/2511.19066", "authors": ["Xiangyu Chang", "Manyi Yao", "Srikanth V. Krishnamurthy", "Christian R. Shelton", "Anirban Chakraborty", "Ananthram Swami", "Samet Oymak", "Amit Roy-Chowdhury"], "title": "Mitigating Participation Imbalance Bias in Asynchronous Federated Learning", "comment": null, "summary": "In Asynchronous Federated Learning (AFL), the central server immediately updates the global model with each arriving client's contribution. As a result, clients perform their local training on different model versions, causing information staleness (delay). In federated environments with non-IID local data distributions, this asynchronous pattern amplifies the adverse effect of client heterogeneity (due to different data distribution, local objectives, etc.), as faster clients contribute more frequent updates, biasing the global model. We term this phenomenon heterogeneity amplification. Our work provides a theoretical analysis that maps AFL design choices to their resulting error sources when heterogeneity amplification occurs. Guided by our analysis, we propose ACE (All-Client Engagement AFL), which mitigates participation imbalance through immediate, non-buffered updates that use the latest information available from all clients. We also introduce a delay-aware variant, ACED, to balance client diversity against update staleness. Experiments on different models for different tasks across diverse heterogeneity and delay settings validate our analysis and demonstrate the robust performance of our approaches."}
{"id": "2511.18671", "categories": ["cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.18671", "abs": "https://arxiv.org/abs/2511.18671", "authors": ["Yan Wang", "Ke Deng", "Yongli Ren"], "title": "Multi-Agent Cross-Entropy Method with Monotonic Nonlinear Critic Decomposition", "comment": null, "summary": "Cooperative multi-agent reinforcement learning (MARL) commonly adopts centralized training with decentralized execution (CTDE), where centralized critics leverage global information to guide decentralized actors. However, centralized-decentralized mismatch (CDM) arises when the suboptimal behavior of one agent degrades others' learning. Prior approaches mitigate CDM through value decomposition, but linear decompositions allow per-agent gradients at the cost of limited expressiveness, while nonlinear decompositions improve representation but require centralized gradients, reintroducing CDM. To overcome this trade-off, we propose the multi-agent cross-entropy method (MCEM), combined with monotonic nonlinear critic decomposition (NCD). MCEM updates policies by increasing the probability of high-value joint actions, thereby excluding suboptimal behaviors. For sample efficiency, we extend off-policy learning with a modified k-step return and Retrace. Analysis and experiments demonstrate that MCEM outperforms state-of-the-art methods across both continuous and discrete action benchmarks."}
{"id": "2511.19087", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19087", "abs": "https://arxiv.org/abs/2511.19087", "authors": ["Ziyun Li", "Ben Dai", "Huancheng Hu", "Henrik Boström", "Soon Hoe Lim"], "title": "EnfoPath: Energy-Informed Analysis of Generative Trajectories in Flow Matching", "comment": "EurIPS 2025 Workshop on Principles of Generative Modeling (PriGM)", "summary": "Flow-based generative models synthesize data by integrating a learned velocity field from a reference distribution to the target data distribution. Prior work has focused on endpoint metrics (e.g., fidelity, likelihood, perceptual quality) while overlooking a deeper question: what do the sampling trajectories reveal? Motivated by classical mechanics, we introduce kinetic path energy (KPE), a simple yet powerful diagnostic that quantifies the total kinetic effort along each generation path of ODE-based samplers. Through comprehensive experiments on CIFAR-10 and ImageNet-256, we uncover two key phenomena: ({i}) higher KPE predicts stronger semantic quality, indicating that semantically richer samples require greater kinetic effort, and ({ii}) higher KPE inversely correlates with data density, with informative samples residing in sparse, low-density regions. Together, these findings reveal that semantically informative samples naturally reside on the sparse frontier of the data distribution, demanding greater generative effort. Our results suggest that trajectory-level analysis offers a physics-inspired and interpretable framework for understanding generation difficulty and sample characteristics."}
{"id": "2511.18689", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18689", "abs": "https://arxiv.org/abs/2511.18689", "authors": ["Kazi Ahmed Asif Fuad", "Lizhong Chen"], "title": "QuantKAN: A Unified Quantization Framework for Kolmogorov Arnold Networks", "comment": null, "summary": "Kolmogorov Arnold Networks (KANs) represent a new class of neural architectures that replace conventional linear transformations and node-based nonlinearities with spline-based function approximations distributed along network edges. Although KANs offer strong expressivity and interpretability, their heterogeneous spline and base branch parameters hinder efficient quantization, which remains unexamined compared to CNNs and Transformers. In this paper, we present QuantKAN, a unified framework for quantizing KANs across both quantization aware training (QAT) and post-training quantization (PTQ) regimes. QuantKAN extends modern quantization algorithms, such as LSQ, LSQ+, PACT, DoReFa, QIL, GPTQ, BRECQ, AdaRound, AWQ, and HAWQ-V2, to spline based layers with branch-specific quantizers for base, spline, and activation components. Through extensive experiments on MNIST, CIFAR 10, and CIFAR 100 across multiple KAN variants (EfficientKAN, FastKAN, PyKAN, and KAGN), we establish the first systematic benchmarks for low-bit spline networks. Our results show that KANs, particularly deeper KAGN variants, are compatible with low-bit quantization but exhibit strong method architecture interactions: LSQ, LSQ+, and PACT preserve near full precision accuracy at 4 bit for shallow KAN MLP and ConvNet models, while DoReFa provides the most stable behavior for deeper KAGN under aggressive low-bit settings. For PTQ, GPTQ and Uniform consistently deliver the strongest overall performance across datasets, with BRECQ highly competitive on simpler regimes such as MNIST. Our proposed QuantKAN framework thus unifies spline learning and quantization, and provides practical tools and guidelines for efficiently deploying KANs in real-world, resource-constrained environments."}
{"id": "2511.19107", "categories": ["cs.LG", "cs.AI", "cs.GT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.19107", "abs": "https://arxiv.org/abs/2511.19107", "authors": ["Robert Bredereck", "Eva Deltl", "Leon Kellerhals", "Jannik Peters"], "title": "The Core in Max-Loss Non-Centroid Clustering Can Be Empty", "comment": null, "summary": "We study core stability in non-centroid clustering under the max-loss objective, where each agent's loss is the maximum distance to other members of their cluster. We prove that for all $k\\geq 3$ there exist metric instances with $n\\ge 9$ agents, with $n$ divisible by $k$, for which no clustering lies in the $α$-core for any $α<2^{\\frac{1}{5}}\\sim 1.148$. The bound is tight for our construction. Using a computer-aided proof, we also identify a two-dimensional Euclidean point set whose associated lower bound is slightly smaller than that of our general construction. This is, to our knowledge, the first impossibility result showing that the core can be empty in non-centroid clustering under the max-loss objective."}
{"id": "2511.18692", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.18692", "abs": "https://arxiv.org/abs/2511.18692", "authors": ["Kichang Yang", "Seonjun Kim", "Minjae Kim", "Nairan Zhang", "Chi Zhang", "Youngki Lee"], "title": "VLM in a flash: I/O-Efficient Sparsification of Vision-Language Model via Neuron Chunking", "comment": null, "summary": "Edge deployment of large Vision-Language Models (VLMs) increasingly relies on flash-based weight offloading, where activation sparsification is used to reduce I/O overhead. However, conventional sparsification remains model-centric, selecting neurons solely by activation magnitude and neglecting how access patterns influence flash performance. We present Neuron Chunking, an I/O-efficient sparsification strategy that operates on chunks (i.e., groups of contiguous neurons in memory) and couples neuron importance with storage access cost. The method models I/O latency through a lightweight abstraction of access contiguity and selects chunks with high utility, defined as neuron importance normalized by estimated latency. By aligning sparsification decisions with the underlying storage behavior, Neuron Chunking improves I/O efficiency by up to 4.65x and 5.76x on Jetson Orin Nano and Jetson AGX Orin, respectively."}
{"id": "2511.19124", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19124", "abs": "https://arxiv.org/abs/2511.19124", "authors": ["Krishang Sharma"], "title": "Uncertainty-Aware Deep Learning Framework for Remaining Useful Life Prediction in Turbofan Engines with Learned Aleatoric Uncertainty", "comment": "10 pages, 2 figures, 3 tables. Submitted to arXiv", "summary": "Accurate Remaining Useful Life (RUL) prediction coupled with uncertainty quantification remains a critical challenge in aerospace prognostics. This research introduces a novel uncertainty-aware deep learning framework that learns aleatoric uncertainty directly through probabilistic modeling, an approach unexplored in existing CMAPSS-based literature. Our hierarchical architecture integrates multi-scale Inception blocks for temporal pattern extraction, bidirectional Long Short-Term Memory networks for sequential modeling, and a dual-level attention mechanism operating simultaneously on sensor and temporal dimensions. The innovation lies in the Bayesian output layer that predicts both mean RUL and variance, enabling the model to learn data-inherent uncertainty. Comprehensive preprocessing employs condition-aware clustering, wavelet denoising, and intelligent feature selection. Experimental validation on NASA CMAPSS benchmarks (FD001-FD004) demonstrates competitive overall performance with RMSE values of 16.22, 19.29, 16.84, and 19.98 respectively. Remarkably, our framework achieves breakthrough critical zone performance (RUL <= 30 cycles) with RMSE of 5.14, 6.89, 5.27, and 7.16, representing 25-40 percent improvements over conventional approaches and establishing new benchmarks for safety-critical predictions. The learned uncertainty provides well-calibrated 95 percent confidence intervals with coverage ranging from 93.5 percent to 95.2 percent, enabling risk-aware maintenance scheduling previously unattainable in CMAPSS literature."}
{"id": "2511.18716", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18716", "abs": "https://arxiv.org/abs/2511.18716", "authors": ["Zesheng Liu", "Maryam Rahnemoonfar"], "title": "GRIT-LP: Graph Transformer with Long-Range Skip Connection and Partitioned Spatial Graphs for Accurate Ice Layer Thickness Prediction", "comment": null, "summary": "Graph transformers have demonstrated remarkable capability on complex spatio-temporal tasks, yet their depth is often limited by oversmoothing and weak long-range dependency modeling. To address these challenges, we introduce GRIT-LP, a graph transformer explicitly designed for polar ice-layer thickness estimation from polar radar imagery. Accurately estimating ice layer thickness is critical for understanding snow accumulation, reconstructing past climate patterns and reducing uncertainties in projections of future ice sheet evolution and sea level rise. GRIT-LP combines an inductive geometric graph learning framework with self-attention mechanism, and introduces two major innovations that jointly address challenges in modeling the spatio-temporal patterns of ice layers: a partitioned spatial graph construction strategy that forms overlapping, fully connected local neighborhoods to preserve spatial coherence and suppress noise from irrelevant long-range links, and a long-range skip connection mechanism within the transformer that improves information flow and mitigates oversmoothing in deeper attention layers. We conducted extensive experiments, demonstrating that GRIT-LP outperforms current state-of-the-art methods with a 24.92\\% improvement in root mean squared error. These results highlight the effectiveness of graph transformers in modeling spatiotemporal patterns by capturing both localized structural features and long-range dependencies across internal ice layers, and demonstrate their potential to advance data-driven understanding of cryospheric processes."}
{"id": "2511.19218", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19218", "abs": "https://arxiv.org/abs/2511.19218", "authors": ["Xurui Li", "Kaisong Song", "Rui Zhu", "Pin-Yu Chen", "Haixu Tang"], "title": "Adversarial Attack-Defense Co-Evolution for LLM Safety Alignment via Tree-Group Dual-Aware Search and Optimization", "comment": null, "summary": "Large Language Models (LLMs) have developed rapidly in web services, delivering unprecedented capabilities while amplifying societal risks. Existing works tend to focus on either isolated jailbreak attacks or static defenses, neglecting the dynamic interplay between evolving threats and safeguards in real-world web contexts. To mitigate these challenges, we propose ACE-Safety (Adversarial Co-Evolution for LLM Safety), a novel framework that jointly optimize attack and defense models by seamlessly integrating two key innovative procedures: (1) Group-aware Strategy-guided Monte Carlo Tree Search (GS-MCTS), which efficiently explores jailbreak strategies to uncover vulnerabilities and generate diverse adversarial samples; (2) Adversarial Curriculum Tree-aware Group Policy Optimization (AC-TGPO), which jointly trains attack and defense LLMs with challenging samples via curriculum reinforcement learning, enabling robust mutual improvement. Evaluations across multiple benchmarks demonstrate that our method outperforms existing attack and defense approaches, and provides a feasible pathway for developing LLMs that can sustainably support responsible AI ecosystems."}
{"id": "2511.18721", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18721", "abs": "https://arxiv.org/abs/2511.18721", "authors": ["Adarsh Kumarappan", "Ayushi Mehrotra"], "title": "Towards Realistic Guarantees: A Probabilistic Certificate for SmoothLLM", "comment": null, "summary": "The SmoothLLM defense provides a certification guarantee against jailbreaking attacks, but it relies on a strict `k-unstable' assumption that rarely holds in practice. This strong assumption can limit the trustworthiness of the provided safety certificate. In this work, we address this limitation by introducing a more realistic probabilistic framework, `(k, $\\varepsilon$)-unstable,' to certify defenses against diverse jailbreaking attacks, from gradient-based (GCG) to semantic (PAIR). We derive a new, data-informed lower bound on SmoothLLM's defense probability by incorporating empirical models of attack success, providing a more trustworthy and practical safety certificate. By introducing the notion of (k, $\\varepsilon$)-unstable, our framework provides practitioners with actionable safety guarantees, enabling them to set certification thresholds that better reflect the real-world behavior of LLMs. Ultimately, this work contributes a practical and theoretically-grounded mechanism to make LLMs more resistant to the exploitation of their safety alignments, a critical challenge in secure AI deployment."}
{"id": "2511.19241", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.19241", "abs": "https://arxiv.org/abs/2511.19241", "authors": ["David Stenger", "Armin Lindicke", "Alexander von Rohr", "Sebastian Trimpe"], "title": "Local Entropy Search over Descent Sequences for Bayesian Optimization", "comment": null, "summary": "Searching large and complex design spaces for a global optimum can be infeasible and unnecessary. A practical alternative is to iteratively refine the neighborhood of an initial design using local optimization methods such as gradient descent. We propose local entropy search (LES), a Bayesian optimization paradigm that explicitly targets the solutions reachable by the descent sequences of iterative optimizers. The algorithm propagates the posterior belief over the objective through the optimizer, resulting in a probability distribution over descent sequences. It then selects the next evaluation by maximizing mutual information with that distribution, using a combination of analytic entropy calculations and Monte-Carlo sampling of descent sequences. Empirical results on high-complexity synthetic objectives and benchmark problems show that LES achieves strong sample efficiency compared to existing local and global Bayesian optimization methods."}
{"id": "2511.18727", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18727", "abs": "https://arxiv.org/abs/2511.18727", "authors": ["Devansh Agarwal", "Maitreyi Chatterjee", "Biplab Chatterjee"], "title": "LogSyn: A Few-Shot LLM Framework for Structured Insight Extraction from Unstructured General Aviation Maintenance Logs", "comment": "Accepted in Proceedings of the 3rd INCOM 2026", "summary": "Aircraft maintenance logs hold valuable safety data but remain underused due to their unstructured text format. This paper introduces LogSyn, a framework that uses Large Language Models (LLMs) to convert these logs into structured, machine-readable data. Using few-shot in-context learning on 6,169 records, LogSyn performs Controlled Abstraction Generation (CAG) to summarize problem-resolution narratives and classify events within a detailed hierarchical ontology. The framework identifies key failure patterns, offering a scalable method for semantic structuring and actionable insight extraction from maintenance logs. This work provides a practical path to improve maintenance workflows and predictive analytics in aviation and related industries."}
{"id": "2511.19253", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19253", "abs": "https://arxiv.org/abs/2511.19253", "authors": ["Boyuan Wu"], "title": "MAESTRO: Multi-Agent Environment Shaping through Task and Reward Optimization", "comment": "Preprint. 16 pages, 6 figures. Preliminary version; extended experiments and analysis forthcoming", "summary": "Cooperative Multi-Agent Reinforcement Learning (MARL) faces two major design bottlenecks: crafting dense reward functions and constructing curricula that avoid local optima in high-dimensional, non-stationary environments. Existing approaches rely on fixed heuristics or use Large Language Models (LLMs) directly in the control loop, which is costly and unsuitable for real-time systems. We propose MAESTRO (Multi-Agent Environment Shaping through Task and Reward Optimization), a framework that moves the LLM outside the execution loop and uses it as an offline training architect. MAESTRO introduces two generative components: (i) a semantic curriculum generator that creates diverse, performance-driven traffic scenarios, and (ii) an automated reward synthesizer that produces executable Python reward functions adapted to evolving curriculum difficulty. These components guide a standard MARL backbone (MADDPG) without increasing inference cost at deployment. We evaluate MAESTRO on large-scale traffic signal control (Hangzhou, 16 intersections) and conduct controlled ablations. Results show that combining LLM-generated curricula with LLM-generated reward shaping yields improved performance and stability. Across four seeds, the full system achieves +4.0% higher mean return (163.26 vs. 156.93) and 2.2% better risk-adjusted performance (Sharpe 1.53 vs. 0.70) over a strong curriculum baseline. These findings highlight LLMs as effective high-level designers for cooperative MARL training."}
{"id": "2511.18728", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18728", "abs": "https://arxiv.org/abs/2511.18728", "authors": ["Maitreyi Chatterjee", "Devansh Agarwal", "Biplab Chatterjee"], "title": "Reinforcement Learning for Self-Healing Material Systems", "comment": "Accepted to INCOM 2026. This is the camera-ready version", "summary": "The transition to autonomous material systems necessitates adaptive control methodologies to maximize structural longevity. This study frames the self-healing process as a Reinforcement Learning (RL) problem within a Markov Decision Process (MDP), enabling agents to autonomously derive optimal policies that efficiently balance structural integrity maintenance against finite resource consumption. A comparative evaluation of discrete-action (Q-learning, DQN) and continuous-action (TD3) agents in a stochastic simulation environment revealed that RL controllers significantly outperform heuristic baselines, achieving near-complete material recovery. Crucially, the TD3 agent utilizing continuous dosage control demonstrated superior convergence speed and stability, underscoring the necessity of fine-grained, proportional actuation in dynamic self-healing applications."}
{"id": "2511.19257", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19257", "abs": "https://arxiv.org/abs/2511.19257", "authors": ["Yingjia Shang", "Yi Liu", "Huimin Wang", "Furong Li", "Wenfang Sun", "Wu Chengyu", "Yefeng Zheng"], "title": "Medusa: Cross-Modal Transferable Adversarial Attacks on Multimodal Medical Retrieval-Augmented Generation", "comment": "Accepted at KDD 2026 First Cycle (full version). Authors marked with * contributed equally. Yi Liu is the lead author", "summary": "With the rapid advancement of retrieval-augmented vision-language models, multimodal medical retrieval-augmented generation (MMed-RAG) systems are increasingly adopted in clinical decision support. These systems enhance medical applications by performing cross-modal retrieval to integrate relevant visual and textual evidence for tasks, e.g., report generation and disease diagnosis. However, their complex architecture also introduces underexplored adversarial vulnerabilities, particularly via visual input perturbations. In this paper, we propose Medusa, a novel framework for crafting cross-modal transferable adversarial attacks on MMed-RAG systems under a black-box setting. Specifically, Medusa formulates the attack as a perturbation optimization problem, leveraging a multi-positive InfoNCE loss (MPIL) to align adversarial visual embeddings with medically plausible but malicious textual targets, thereby hijacking the retrieval process. To enhance transferability, we adopt a surrogate model ensemble and design a dual-loop optimization strategy augmented with invariant risk minimization (IRM). Extensive experiments on two real-world medical tasks, including medical report generation and disease diagnosis, demonstrate that Medusa achieves over 90% average attack success rate across various generation models and retrievers under appropriate parameter configuration, while remaining robust against four mainstream defenses, outperforming state-of-the-art baselines. Our results reveal critical vulnerabilities in the MMed-RAG systems and highlight the necessity of robustness benchmarking in safety-critical medical applications. The code and data are available at https://anonymous.4open.science/r/MMed-RAG-Attack-F05A."}
{"id": "2511.18730", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18730", "abs": "https://arxiv.org/abs/2511.18730", "authors": ["Michael Horton", "Patrick Lucey"], "title": "Large-Scale In-Game Outcome Forecasting for Match, Team and Players in Football using an Axial Transformer Neural Network", "comment": "25 pages, 7 figures, 1 table", "summary": "Football (soccer) is a sport that is characterised by complex game play, where players perform a variety of actions, such as passes, shots, tackles, fouls, in order to score goals, and ultimately win matches. Accurately forecasting the total number of each action that each player will complete during a match is desirable for a variety of applications, including tactical decision-making, sports betting, and for television broadcast commentary and analysis. Such predictions must consider the game state, the ability and skill of the players in both teams, the interactions between the players, and the temporal dynamics of the game as it develops. In this paper, we present a transformer-based neural network that jointly and recurrently predicts the expected totals for thirteen individual actions at multiple time-steps during the match, and where predictions are made for each individual player, each team and at the game-level. The neural network is based on an \\emph{axial transformer} that efficiently captures the temporal dynamics as the game progresses, and the interactions between the players at each time-step. We present a novel axial transformer design that we show is equivalent to a regular sequential transformer, and the design performs well experimentally. We show empirically that the model can make consistent and reliable predictions, and efficiently makes $\\sim$75,000 live predictions at low latency for each game."}
{"id": "2511.19260", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.19260", "abs": "https://arxiv.org/abs/2511.19260", "authors": ["Kyle Verrier", "Achille Nazaret", "Joseph Futoma", "Andrew C. Miller", "Guillermo Sapiro"], "title": "A Nutrition Multimodal Photoplethysmography Language Model", "comment": "21 pages, 2 figures", "summary": "Hunger and satiety dynamics shape dietary behaviors and metabolic health, yet remain difficult to capture in everyday settings. We present a Nutrition Photoplethysmography Language Model (NPLM), integrating continuous photoplethysmography (PPG) from wearables with meal descriptions. NPLM projects PPG into embeddings interpretable by language models, enabling joint reasoning over physiology and meal context. Trained on 19,340 participants and 1.1 million meal-PPG pairs, the model improved daily caloric intake prediction by 11% over text-only baselines, with accuracy maintained when 80% of meal text was removed. In an independent validation study (n=140) with controlled dining and detailed meal information, the model replicated these findings. These results demonstrate the value of integrating physiological measurements from consumer wearables with meal information for noninvasive dietary monitoring at scale."}
{"id": "2511.18732", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.18732", "abs": "https://arxiv.org/abs/2511.18732", "authors": ["Haoming Jia", "Yi Han", "Xiang Wang", "Huizan Wang", "Wei Wu", "Jianming Zheng", "Peikun Xiao"], "title": "OceanForecastBench: A Benchmark Dataset for Data-Driven Global Ocean Forecasting", "comment": null, "summary": "Global ocean forecasting aims to predict key ocean variables such as temperature, salinity, and currents, which is essential for understanding and describing oceanic phenomena. In recent years, data-driven deep learning-based ocean forecast models, such as XiHe, WenHai, LangYa and AI-GOMS, have demonstrated significant potential in capturing complex ocean dynamics and improving forecasting efficiency. Despite these advancements, the absence of open-source, standardized benchmarks has led to inconsistent data usage and evaluation methods. This gap hinders efficient model development, impedes fair performance comparison, and constrains interdisciplinary collaboration. To address this challenge, we propose OceanForecastBench, a benchmark offering three core contributions: (1) A high-quality global ocean reanalysis data over 28 years for model training, including 4 ocean variables across 23 depth levels and 4 sea surface variables. (2) A high-reliability satellite and in-situ observations for model evaluation, covering approximately 100 million locations in the global ocean. (3) An evaluation pipeline and a comprehensive benchmark with 6 typical baseline models, leveraging observations to evaluate model performance from multiple perspectives. OceanForecastBench represents the most comprehensive benchmarking framework currently available for data-driven ocean forecasting, offering an open-source platform for model development, evaluation, and comparison. The dataset and code are publicly available at: https://github.com/Ocean-Intelligent-Forecasting/OceanForecastBench."}
{"id": "2511.19263", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19263", "abs": "https://arxiv.org/abs/2511.19263", "authors": ["Lucas Li", "Jean-Baptiste Puel", "Florence Carton", "Dounya Barrit", "Jhony H. Giraldo"], "title": "Solar-GECO: Perovskite Solar Cell Property Prediction with Geometric-Aware Co-Attention", "comment": "Accepted at the AI for Accelerated Materials Design (AI4Mat) Workshop at NeurIPS 2025. 14 pages, 4 figures", "summary": "Perovskite solar cells are promising candidates for next-generation photovoltaics. However, their performance as multi-scale devices is determined by complex interactions between their constituent layers. This creates a vast combinatorial space of possible materials and device architectures, making the conventional experimental-based screening process slow and expensive. Machine learning models try to address this problem, but they only focus on individual material properties or neglect the important geometric information of the perovskite crystal. To address this problem, we propose to predict perovskite solar cell power conversion efficiency with a geometric-aware co-attention (Solar-GECO) model. Solar-GECO combines a geometric graph neural network (GNN) - that directly encodes the atomic structure of the perovskite absorber - with language model embeddings that process the textual strings representing the chemical compounds of the transport layers and other device components. Solar-GECO also integrates a co-attention module to capture intra-layer dependencies and inter-layer interactions, while a probabilistic regression head predicts both power conversion efficiency (PCE) and its associated uncertainty. Solar-GECO achieves state-of-the-art performance, significantly outperforming several baselines, reducing the mean absolute error (MAE) for PCE prediction from 3.066 to 2.936 compared to semantic GNN (the previous state-of-the-art model). Solar-GECO demonstrates that integrating geometric and textual information provides a more powerful and accurate framework for PCE prediction."}
{"id": "2511.18773", "categories": ["cs.LG", "cs.CV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.18773", "abs": "https://arxiv.org/abs/2511.18773", "authors": ["Senmao Tian", "Xiang Wei", "Shunli Zhang"], "title": "Sampling Control for Imbalanced Calibration in Semi-Supervised Learning", "comment": "Accepted at AAAI 2026", "summary": "Class imbalance remains a critical challenge in semi-supervised learning (SSL), especially when distributional mismatches between labeled and unlabeled data lead to biased classification. Although existing methods address this issue by adjusting logits based on the estimated class distribution of unlabeled data, they often handle model imbalance in a coarse-grained manner, conflating data imbalance with bias arising from varying class-specific learning difficulties. To address this issue, we propose a unified framework, SC-SSL, which suppresses model bias through decoupled sampling control. During training, we identify the key variables for sampling control under ideal conditions. By introducing a classifier with explicit expansion capability and adaptively adjusting sampling probabilities across different data distributions, SC-SSL mitigates feature-level imbalance for minority classes. In the inference phase, we further analyze the weight imbalance of the linear classifier and apply post-hoc sampling control with an optimization bias vector to directly calibrate the logits. Extensive experiments across various benchmark datasets and distribution settings validate the consistency and state-of-the-art performance of SC-SSL."}
{"id": "2511.19264", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2511.19264", "abs": "https://arxiv.org/abs/2511.19264", "authors": ["Amirtha Varshini A S", "Duminda S. Ranasinghe", "Hok Hei Tam"], "title": "Interpreting GFlowNets for Drug Discovery: Extracting Actionable Insights for Medicinal Chemistry", "comment": "13 pages, 7 figures. Accepted for presentation at NeurIPS 2025 WiML Workshop and Molecular Machine Learning Conference (MoML) 2025", "summary": "Generative Flow Networks, or GFlowNets, offer a promising framework for molecular design, but their internal decision policies remain opaque. This limits adoption in drug discovery, where chemists require clear and interpretable rationales for proposed structures. We present an interpretability framework for SynFlowNet, a GFlowNet trained on documented chemical reactions and purchasable starting materials that generates both molecules and the synthetic routes that produce them. Our approach integrates three complementary components. Gradient based saliency combined with counterfactual perturbations identifies which atomic environments influence reward and how structural edits change molecular outcomes. Sparse autoencoders reveal axis aligned latent factors that correspond to physicochemical properties such as polarity, lipophilicity, and molecular size. Motif probes show that functional groups including aromatic rings and halogens are explicitly encoded and linearly decodable from the internal embeddings. Together, these results expose the chemical logic inside SynFlowNet and provide actionable and mechanistic insight that supports transparent and controllable molecular design."}
{"id": "2511.18777", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18777", "abs": "https://arxiv.org/abs/2511.18777", "authors": ["Chenhong Zhou", "Jie Chen", "Zaifeng Yang"], "title": "SAOT: An Enhanced Locality-Aware Spectral Transformer for Solving PDEs", "comment": "Accepted to AAAI 2026 (Main Technical Track)", "summary": "Neural operators have shown great potential in solving a family of Partial Differential Equations (PDEs) by modeling the mappings between input and output functions. Fourier Neural Operator (FNO) implements global convolutions via parameterizing the integral operators in Fourier space. However, it often results in over-smoothing solutions and fails to capture local details and high-frequency components. To address these limitations, we investigate incorporating the spatial-frequency localization property of Wavelet transforms into the Transformer architecture. We propose a novel Wavelet Attention (WA) module with linear computational complexity to efficiently learn locality-aware features. Building upon WA, we further develop the Spectral Attention Operator Transformer (SAOT), a hybrid spectral Transformer framework that integrates WA's localized focus with the global receptive field of Fourier-based Attention (FA) through a gated fusion block. Experimental results demonstrate that WA significantly mitigates the limitations of FA and outperforms existing Wavelet-based neural operators by a large margin. By integrating the locality-aware and global spectral representations, SAOT achieves state-of-the-art performance on six operator learning benchmarks and exhibits strong discretization-invariant ability."}
{"id": "2511.19283", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19283", "abs": "https://arxiv.org/abs/2511.19283", "authors": ["Ndaka. A", "Avila-Acosta. F", "Mbula-Ndaka. H", "Amera. C", "Chauke. S", "Majiwa. E"], "title": "Data Flows and Colonial Regimes in Africa: A Critical Analysis of the Colonial Futurities Embedded in AI Ecosystems", "comment": "12 pages", "summary": "This chapter seeks to frame the elemental and invisible problems of AI and big data in the African context by examining digital sites and infrastructure through the lens of power and interests. It will present reflections on how these sites are using AI recommendation algorithms to recreate new digital societies in the region, how they have the potential to propagate algorithmic colonialism and negative gender norms, and what this means for the regional sustainable development agenda. The chapter proposes adopting business models that embrace response-ability and consider the existence of alternative socio-material worlds of AI. These reflections will mainly come from ongoing discussions with Kenyan social media users in this authors' user space talks, personal experiences and six months of active participant observations done by the authors."}
{"id": "2511.18783", "categories": ["cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2511.18783", "abs": "https://arxiv.org/abs/2511.18783", "authors": ["Renchu Guan", "Xuyang Li", "Yachao Zhang", "Wei Pang", "Fausto Giunchiglia", "Ximing Li", "Yonghao Liu", "Xiaoyue Feng"], "title": "Hypergraph Contrastive Learning for both Homophilic and Heterophilic Hypergraphs", "comment": null, "summary": "Hypergraphs, as a generalization of traditional graphs, naturally capture high-order relationships. In recent years, hypergraph neural networks (HNNs) have been widely used to capture complex high-order relationships. However, most existing hypergraph neural network methods inherently rely on the homophily assumption, which often does not hold in real-world scenarios that exhibit significant heterophilic structures. To address this limitation, we propose \\textbf{HONOR}, a novel unsupervised \\textbf{H}ypergraph c\\textbf{ON}trastive learning framework suitable for both hom\\textbf{O}philic and hete\\textbf{R}ophilic hypergraphs. Specifically, HONOR explicitly models the heterophilic relationships between hyperedges and nodes through two complementary mechanisms: a prompt-based hyperedge feature construction strategy that maintains global semantic consistency while suppressing local noise, and an adaptive attention aggregation module that dynamically captures the diverse local contributions of nodes to hyperedges. Combined with high-pass filtering, these designs enable HONOR to fully exploit heterophilic connection patterns, yielding more discriminative and robust node and hyperedge representations. Theoretically, we demonstrate the superior generalization ability and robustness of HONOR. Empirically, extensive experiments further validate that HONOR consistently outperforms state-of-the-art baselines under both homophilic and heterophilic datasets."}
{"id": "2511.19299", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19299", "abs": "https://arxiv.org/abs/2511.19299", "authors": ["James R. M. Black", "Moritz S. Hanke", "Aaron Maiwald", "Tina Hernandez-Boussard", "Oliver M. Crook", "Jaspreet Pannu"], "title": "Open-weight genome language model safeguards: Assessing robustness via adversarial fine-tuning", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: Biosecurity Safeguards for Generative AI", "summary": "Novel deep learning architectures are increasingly being applied to biological data, including genetic sequences. These models, referred to as genomic language mod- els (gLMs), have demonstrated impressive predictive and generative capabilities, raising concerns that such models may also enable misuse, for instance via the generation of genomes for human-infecting viruses. These concerns have catalyzed calls for risk mitigation measures. The de facto mitigation of choice is filtering of pretraining data (i.e., removing viral genomic sequences from training datasets) in order to limit gLM performance on virus-related tasks. However, it is not currently known how robust this approach is for securing open-source models that can be fine-tuned using sensitive pathogen data. Here, we evaluate a state-of-the-art gLM, Evo 2, and perform fine-tuning using sequences from 110 harmful human-infecting viruses to assess the rescue of misuse-relevant predictive capabilities. The fine- tuned model exhibited reduced perplexity on unseen viral sequences relative to 1) the pretrained model and 2) a version fine-tuned on bacteriophage sequences. The model fine-tuned on human-infecting viruses also identified immune escape variants from SARS-CoV-2 (achieving an AUROC of 0.6), despite having no expo- sure to SARS-CoV-2 sequences during fine-tuning. This work demonstrates that data exclusion might be circumvented by fine-tuning approaches that can, to some degree, rescue misuse-relevant capabilities of gLMs. We highlight the need for safety frameworks for gLMs and outline further work needed on evaluations and mitigation measures to enable the safe deployment of gLMs."}
{"id": "2511.18789", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.18789", "abs": "https://arxiv.org/abs/2511.18789", "authors": ["Haichen Hu", "David Simchi-Levi"], "title": "Doubly Wild Refitting: Model-Free Evaluation of High Dimensional Black-Box Predictions under Convex Losses", "comment": null, "summary": "We study the problem of excess risk evaluation for empirical risk minimization (ERM) under general convex loss functions. Our contribution is an efficient refitting procedure that computes the excess risk and provides high-probability upper bounds under the fixed-design setting. Assuming only black-box access to the training algorithm and a single dataset, we begin by generating two sets of artificially modified pseudo-outcomes termed wild response, created by stochastically perturbing the gradient vectors with carefully chosen scaling. Using these two pseudo-labeled datasets, we then refit the black-box procedure twice to obtain two corresponding wild predictors. Finally, leveraging the original predictor, the two wild predictors, and the constructed wild responses, we derive an efficient excess risk upper bound. A key feature of our analysis is that it requires no prior knowledge of the complexity of the underlying function class. As a result, the method is essentially model-free and holds significant promise for theoretically evaluating modern opaque machine learning system--such as deep nerral networks and generative model--where traditional capacity-based learning theory becomes infeasible due to the extreme complexity of the hypothesis class."}
{"id": "2511.19355", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19355", "abs": "https://arxiv.org/abs/2511.19355", "authors": ["Franklin Cardenoso", "Wouter Caarls"], "title": "Leveraging LLMs for reward function design in reinforcement learning control tasks", "comment": null, "summary": "The challenge of designing effective reward functions in reinforcement learning (RL) represents a significant bottleneck, often requiring extensive human expertise and being time-consuming. Previous work and recent advancements in large language models (LLMs) have demonstrated their potential for automating the generation of reward functions. However, existing methodologies often require preliminary evaluation metrics, human-engineered feedback for the refinement process, or the use of environmental source code as context. To address these limitations, this paper introduces LEARN-Opt (LLM-based Evaluator and Analyzer for Reward functioN Optimization). This LLM-based, fully autonomous, and model-agnostic framework eliminates the need for preliminary metrics and environmental source code as context to generate, execute, and evaluate reward function candidates from textual descriptions of systems and task objectives. LEARN-Opt's main contribution lies in its ability to autonomously derive performance metrics directly from the system description and the task objective, enabling unsupervised evaluation and selection of reward functions. Our experiments indicate that LEARN-Opt achieves performance comparable to or better to that of state-of-the-art methods, such as EUREKA, while requiring less prior knowledge. We find that automated reward design is a high-variance problem, where the average-case candidate fails, requiring a multi-run approach to find the best candidates. Finally, we show that LEARN-Opt can unlock the potential of low-cost LLMs to find high-performing candidates that are comparable to, or even better than, those of larger models. This demonstrated performance affirms its potential to generate high-quality reward functions without requiring any preliminary human-defined metrics, thereby reducing engineering overhead and enhancing generalizability."}
{"id": "2511.18829", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18829", "abs": "https://arxiv.org/abs/2511.18829", "authors": ["Kanav Arora", "Girish Narayanswamy", "Shwetak Patel", "Richard Li"], "title": "Towards Characterizing Knowledge Distillation of PPG Heart Rate Estimation Models", "comment": "To be published in: 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: Learning from Time Series for Health", "summary": "Heart rate estimation from photoplethysmography (PPG) signals generated by wearable devices such as smartwatches and fitness trackers has significant implications for the health and well-being of individuals. Although prior work has demonstrated deep learning models with strong performance in the heart rate estimation task, in order to deploy these models on wearable devices, these models must also adhere to strict memory and latency constraints. In this work, we explore and characterize how large pre-trained PPG models may be distilled to smaller models appropriate for real-time inference on the edge. We evaluate four distillation strategies through comprehensive sweeps of teacher and student model capacities: (1) hard distillation, (2) soft distillation, (3) decoupled knowledge distillation (DKD), and (4) feature distillation. We present a characterization of the resulting scaling laws describing the relationship between model size and performance. This early investigation lays the groundwork for practical and predictable methods for building edge-deployable models for physiological sensing."}
{"id": "2511.19390", "categories": ["cs.LG", "astro-ph.SR", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.19390", "abs": "https://arxiv.org/abs/2511.19390", "authors": ["Rudy Morel", "Francesco Pio Ramunno", "Jeff Shen", "Alberto Bietti", "Kyunghyun Cho", "Miles Cranmer", "Siavash Golkar", "Olexandr Gugnin", "Geraud Krawezik", "Tanya Marwah", "Michael McCabe", "Lucas Meyer", "Payel Mukhopadhyay", "Ruben Ohana", "Liam Parker", "Helen Qu", "François Rozet", "K. D. Leka", "François Lanusse", "David Fouhey", "Shirley Ho"], "title": "Predicting partially observable dynamical systems via diffusion models with a multiscale inference scheme", "comment": null, "summary": "Conditional diffusion models provide a natural framework for probabilistic prediction of dynamical systems and have been successfully applied to fluid dynamics and weather prediction. However, in many settings, the available information at a given time represents only a small fraction of what is needed to predict future states, either due to measurement uncertainty or because only a small fraction of the state can be observed. This is true for example in solar physics, where we can observe the Sun's surface and atmosphere, but its evolution is driven by internal processes for which we lack direct measurements. In this paper, we tackle the probabilistic prediction of partially observable, long-memory dynamical systems, with applications to solar dynamics and the evolution of active regions. We show that standard inference schemes, such as autoregressive rollouts, fail to capture long-range dependencies in the data, largely because they do not integrate past information effectively. To overcome this, we propose a multiscale inference scheme for diffusion models, tailored to physical processes. Our method generates trajectories that are temporally fine-grained near the present and coarser as we move farther away, which enables capturing long-range temporal dependencies without increasing computational cost. When integrated into a diffusion model, we show that our inference scheme significantly reduces the bias of the predicted distributions and improves rollout stability."}
{"id": "2511.18830", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18830", "abs": "https://arxiv.org/abs/2511.18830", "authors": ["Fang Wang", "Paolo Ceravolo", "Ernesto Damiani"], "title": "Leveraging Duration Pseudo-Embeddings in Multilevel LSTM and GCN Hypermodels for Outcome-Oriented PPM", "comment": "12 pages", "summary": "Existing deep learning models for Predictive Process Monitoring (PPM) struggle with temporal irregularities, particularly stochastic event durations and overlapping timestamps, limiting their adaptability across heterogeneous datasets. We propose a dual input neural network strategy that separates event and sequence attributes, using a duration-aware pseudo-embedding matrix to transform temporal importance into compact, learnable representations. This design is implemented across two baseline families: B-LSTM and B-GCN, and their duration-aware variants D-LSTM and D-GCN. All models incorporate self-tuned hypermodels for adaptive architecture selection. Experiments on balanced and imbalanced outcome prediction tasks show that duration pseudo-embedding inputs consistently improve generalization, reduce model complexity, and enhance interpretability. Our results demonstrate the benefits of explicit temporal encoding and provide a flexible design for robust, real-world PPM applications."}
{"id": "2511.19413", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19413", "abs": "https://arxiv.org/abs/2511.19413", "authors": ["Zhaolong Su", "Wang Lu", "Hao Chen", "Sharon Li", "Jindong Wang"], "title": "UniGame: Turning a Unified Multimodal Model Into Its Own Adversary", "comment": null, "summary": "Unified Multimodal Models (UMMs) have shown impressive performance in both understanding and generation with a single architecture. However, UMMs still exhibit a fundamental inconsistency: understanding favors compact embeddings, whereas generation favors reconstruction-rich representations. This structural trade-off produces misaligned decision boundaries, degraded cross-modal coherence, and heightened vulnerability under distributional and adversarial shifts. In this paper, we present UniGame, a self-adversarial post-training framework that directly targets the inconsistencies. By applying a lightweight perturber at the shared token interface, UniGame enables the generation branch to actively seek and challenge fragile understanding, turning the model itself into its own adversary. Experiments demonstrate that UniGame significantly improves the consistency (+4.6%). Moreover, it also achieves substantial improvements in understanding (+3.6%), generation (+0.02), out-of-distribution and adversarial robustness (+4.8% and +6.2% on NaturalBench and AdVQA). The framework is architecture-agnostic, introduces less than 1% additional parameters, and is complementary to existing post-training methods. These results position adversarial self-play as a general and effective principle for enhancing the coherence, stability, and unified competence of future multimodal foundation models. The official code is available at: https://github.com/AIFrontierLab/UniGame"}
{"id": "2511.18835", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18835", "abs": "https://arxiv.org/abs/2511.18835", "authors": ["Fang Wang", "Lance Kosca", "Adrienne Kosca", "Marko Gacesa", "Ernesto Damiani"], "title": "Auto-ML Graph Neural Network Hypermodels for Outcome Prediction in Event-Sequence Data", "comment": "6 pages", "summary": "This paper introduces HGNN(O), an AutoML GNN hypermodel framework for outcome prediction on event-sequence data. Building on our earlier work on graph convolutional network hypermodels, HGNN(O) extends four architectures-One Level, Two Level, Two Level Pseudo Embedding, and Two Level Embedding-across six canonical GNN operators. A self-tuning mechanism based on Bayesian optimization with pruning and early stopping enables efficient adaptation over architectures and hyperparameters without manual configuration. Empirical evaluation on both balanced and imbalanced event logs shows that HGNN(O) achieves accuracy exceeding 0.98 on the Traffic Fines dataset and weighted F1 scores up to 0.86 on the Patients dataset without explicit imbalance handling. These results demonstrate that the proposed AutoML-GNN approach provides a robust and generalizable benchmark for outcome prediction in complex event-sequence data."}
{"id": "2511.19422", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.19422", "abs": "https://arxiv.org/abs/2511.19422", "authors": ["David Jiahao Fu", "Aryan Gupta", "Aaron Councilman", "David Grove", "Yu-Xiong Wang", "Vikram Adve"], "title": "SLMFix: Leveraging Small Language Models for Error Fixing with Reinforcement Learning", "comment": null, "summary": "Recent advancements in large language models (LLMs) have shown very impressive capabilities in code generation across many programming languages. However, even state-of-the-art LLMs generate programs that contains syntactic errors and fail to complete the given tasks, especially for low-resource programming languages (LRPLs). In addition, high training cost makes finetuning LLMs unaffordable with constrained computational resources, further undermining the effectiveness of LLMs for code generation. In this work, we propose SLMFix, a novel code generation pipeline that leverages a small language model (SLM) finetuned using reinforcement learning (RL) techniques to fix syntactic errors in LLM-generated programs to improve the quality of LLM-generated programs for domain-specific languages (DSLs). In specific, we applied RL on the SLM for the program repair task using a reward calculated using both a static validator and a static semantic similarity metric. Our experimental results demonstrate the effectiveness and generalizability of our approach across multiple DSLs, achieving more than 95% pass rate on the static validator. Notably, SLMFix brings substantial improvement to the base model and outperforms supervised finetuning approach even for 7B models on a LRPL, showing the potential of our approach as an alternative to traditional finetuning approaches."}
{"id": "2511.18841", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.18841", "abs": "https://arxiv.org/abs/2511.18841", "authors": ["Mincheol Jeon", "Euinam Huh"], "title": "Federated style aware transformer aggregation of representations", "comment": null, "summary": "Personalized Federated Learning (PFL) faces persistent challenges, including domain heterogeneity from diverse client data, data imbalance due to skewed participation, and strict communication constraints. Traditional federated learning often lacks personalization, as a single global model cannot capture client-specific characteristics, leading to biased predictions and poor generalization, especially for clients with highly divergent data distributions.\n  To address these issues, we propose FedSTAR, a style-aware federated learning framework that disentangles client-specific style factors from shared content representations. FedSTAR aggregates class-wise prototypes using a Transformer-based attention mechanism, allowing the server to adaptively weight client contributions while preserving personalization.\n  Furthermore, by exchanging compact prototypes and style vectors instead of full model parameters, FedSTAR significantly reduces communication overhead. Experimental results demonstrate that combining content-style disentanglement with attention-driven prototype aggregation improves personalization and robustness in heterogeneous environments without increasing communication cost."}
{"id": "2511.19427", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19427", "abs": "https://arxiv.org/abs/2511.19427", "authors": ["Jayanaka L. Dantanarayana", "Savini Kashmira", "Thakee Nathees", "Zichen Zhang", "Krisztian Flautner", "Lingjia Tang", "Jason Mars"], "title": "Prompt Less, Smile More: MTP with Semantic Engineering in Lieu of Prompt Engineering", "comment": null, "summary": "AI-Integrated programming is emerging as a foundational paradigm for building intelligent systems with large language models (LLMs). Recent approaches such as Meaning Typed Programming (MTP) automate prompt generation by leveraging the semantics already present in code. However, many real-world applications depend on contextual cues, developer intent, and domain-specific reasoning that extend beyond what static code semantics alone can express. To address this limitation, we introduce Semantic Engineering, a lightweight method for enriching program semantics so that LLM-based systems can more accurately reflect developer intent without requiring full manual prompt design. We present Semantic Context Annotations (SemTexts), a language-level mechanism that allows developers to embed natural-language context directly into program constructs. Integrated into the Jac programming language, Semantic Engineering extends MTP to incorporate these enriched semantics during prompt generation. We further introduce a benchmark suite designed to reflect realistic AI-Integrated application scenarios. Our evaluation shows that Semantic Engineering substantially improves prompt fidelity, achieving performance comparable to Prompt Engineering while requiring significantly less developer effort."}
{"id": "2511.18846", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18846", "abs": "https://arxiv.org/abs/2511.18846", "authors": ["Yubo Wang", "Hui He", "Chaoxi Niu", "Zhendong Niu"], "title": "WaveTuner: Comprehensive Wavelet Subband Tuning for Time Series Forecasting", "comment": null, "summary": "Due to the inherent complexity, temporal patterns in real-world time series often evolve across multiple intertwined scales, including long-term periodicity, short-term fluctuations, and abrupt regime shifts. While existing literature has designed many sophisticated decomposition approaches based on the time or frequency domain to partition trend-seasonality components and high-low frequency components, an alternative line of approaches based on the wavelet domain has been proposed to provide a unified multi-resolution representation with precise time-frequency localization. However, most wavelet-based methods suffer from a persistent bias toward recursively decomposing only low-frequency components, severely underutilizing subtle yet informative high-frequency components that are pivotal for precise time series forecasting. To address this problem, we propose WaveTuner, a Wavelet decomposition framework empowered by full-spectrum subband Tuning for time series forecasting. Concretely, WaveTuner comprises two key modules: (i) Adaptive Wavelet Refinement module, that transforms time series into time-frequency coefficients, utilizes an adaptive router to dynamically assign subband weights, and generates subband-specific embeddings to support refinement; and (ii) Multi-Branch Specialization module, that employs multiple functional branches, each instantiated as a flexible Kolmogorov-Arnold Network (KAN) with a distinct functional order to model a specific spectral subband. Equipped with these modules, WaveTuner comprehensively tunes global trends and local variations within a unified time-frequency framework. Extensive experiments on eight real-world datasets demonstrate WaveTuner achieves state-of-the-art forecasting performance in time series forecasting."}
{"id": "2511.18859", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18859", "abs": "https://arxiv.org/abs/2511.18859", "authors": ["Bo Jiang", "Weijun Zhao", "Beibei Wang", "Xiao Wang", "Jin Tang"], "title": "Robust and Generalizable GNN Fine-Tuning via Uncertainty-aware Adapter Learning", "comment": null, "summary": "Recently, fine-tuning large-scale pre-trained GNNs has yielded remarkable attention in adapting pre-trained GNN models for downstream graph learning tasks. One representative fine-tuning method is to exploit adapter (termed AdapterGNN) which aims to 'augment' the pre-trained model by inserting a lightweight module to make the 'augmented' model better adapt to the downstream tasks. However, graph data may contain various types of noise in downstream tasks, such as noisy edges and ambiguous node attributes. Existing AdapterGNNs are often prone to graph noise and exhibit limited generalizability. How to enhance the robustness and generalization ability of GNNs' fine tuning remains an open problem. In this paper, we show that the above problem can be well addressed by integrating uncertainty learning into the GNN adapter. We propose the Uncertainty-aware Adapter (UAdapterGNN) that fortifies pre-trained GNN models against noisy graph data in the fine-tuning process. Specifically, in contrast to regular AdapterGNN, our UAdapterGNN exploits Gaussian probabilistic adapter to augment the pre-trained GNN model. In this way, when the graph contains various noises,our method can automatically absorb the effects of changes in the variances of the Gaussian distribution, thereby significantly enhancing the model's robustness. Also, UAdapterGNN can further improve the generalization ability of the model on the downstream tasks. Extensive experiments on several benchmarks demonstrate the effectiveness, robustness and high generalization ability of the proposed UAdapterGNN method."}
{"id": "2511.18868", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18868", "abs": "https://arxiv.org/abs/2511.18868", "authors": ["Dezhi Ran", "Shuxiao Xie", "Mingfang Ji", "Ziyue Hua", "Mengzhou Wu", "Yuan Cao", "Yuzhe Guo", "Yu Hao", "Linyi Li", "Yitao Hu", "Tao Xie"], "title": "KernelBand: Boosting LLM-based Kernel Optimization with a Hierarchical and Hardware-aware Multi-armed Bandit", "comment": "Work in progress", "summary": "High quality kernels are critical for reducing training and inference costs of Large Language Models (LLMs), yet they traditionally require significant expertise in hardware architecture and software optimization. While recent advances in LLM-based code generation show promise for complex optimization, existing methods struggle with the vast optimization space due to insufficient hardware domain knowledge, failing to effectively balance exploration and exploitation. We present KernelBand, a novel framework that formulates kernel optimization as a hierarchical multi-armed bandit problem, enabling LLM agents to strategically navigate the optimization space by treating kernel selection and optimization strategy application as sequential decision-making processes. Our approach leverages hardware profiling information to identify promising optimization strategies and employs runtime behavior clustering to reduce exploration overhead across kernel candidates. Extensive experiments on TritonBench demonstrate that KernelBand significantly outperforms state-of-the-art methods, achieving superior performance with fewer tokens while exhibiting consistent improvement without saturation as computational resources increase."}
{"id": "2511.18871", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18871", "abs": "https://arxiv.org/abs/2511.18871", "authors": ["Jian Lu"], "title": "Periodic Asynchrony: An Effective Method for Accelerating On-Policy Reinforcement Learning", "comment": null, "summary": "Since the introduction of the GRPO algorithm, reinforcement learning (RL) has attracted increasing attention, with growing efforts to reproduce and apply it. However, training efficiency remains a critical challenge. In mainstream RL frameworks, inference and training are typically deployed on the same devices. While this approach reduces costs through resource consolidation, its synchronous execution imposes a computational coupling that prevents concurrent inference and training. In this study, we are returning to the strategy of separating inference and training deployment, and by introducing improvements in the data loader, we transform the conventional synchronous architecture into a periodically asynchronous framework, which allows for demand-driven, independent, and elastic scaling of each component, while the accuracy of the algorithm remains completely equivalent to the synchronization method, with both belonging to the on-policy strategy. It is worth emphasizing that we apply a unified tri-model architecture in the training phase, and we also proposed a shared-prompt attention mask to reduce repetitive computation. In practice, these works have achieved at least a threefold overall performance improvement in RL training on NPU platforms, indicating its potential for widespread application."}
{"id": "2511.18887", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18887", "abs": "https://arxiv.org/abs/2511.18887", "authors": ["Hyeong-Gun Joo", "Songnam Hong", "Seunghwan Lee", "Dong-Joon Shin"], "title": "Hi-SAFE: Hierarchical Secure Aggregation for Lightweight Federated Learning", "comment": "currently submitted and awaiting review at the IEEE Internet of Things Journal", "summary": "Federated learning (FL) faces challenges in ensuring both privacy and communication efficiency, particularly in resource-constrained environments such as Internet of Things (IoT) and edge networks. While sign-based methods, such as sign stochastic gradient descent with majority voting (SIGNSGD-MV), offer substantial bandwidth savings, they remain vulnerable to inference attacks due to exposure of gradient signs. Existing secure aggregation techniques are either incompatible with sign-based methods or incur prohibitive overhead. To address these limitations, we propose Hi-SAFE, a lightweight and cryptographically secure aggregation framework for sign-based FL. Our core contribution is the construction of efficient majority vote polynomials for SIGNSGD-MV, derived from Fermat's Little Theorem. This formulation represents the majority vote as a low-degree polynomial over a finite field, enabling secure evaluation that hides intermediate values and reveals only the final result. We further introduce a hierarchical subgrouping strategy that ensures constant multiplicative depth and bounded per-user complexity, independent of the number of users n."}
{"id": "2511.18890", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18890", "abs": "https://arxiv.org/abs/2511.18890", "authors": ["Yonggan Fu", "Xin Dong", "Shizhe Diao", "Matthijs Van keirsbilck", "Hanrong Ye", "Wonmin Byeon", "Yashaswi Karnati", "Lucas Liebenwein", "Hannah Zhang", "Nikolaus Binder", "Maksim Khadkevich", "Alexander Keller", "Jan Kautz", "Yingyan Celine Lin", "Pavlo Molchanov"], "title": "Nemotron-Flash: Towards Latency-Optimal Hybrid Small Language Models", "comment": "Accepted by NeurIPS 2025", "summary": "Efficient deployment of small language models (SLMs) is essential for numerous real-world applications with stringent latency constraints. While previous work on SLM design has primarily focused on reducing the number of parameters to achieve parameter-optimal SLMs, parameter efficiency does not necessarily translate into proportional real-device speed-ups. This work aims to identify the key determinants of SLMs' real-device latency and offer generalizable principles and methodologies for SLM design and training when real-device latency is the primary consideration. Specifically, we identify two central architectural factors: depth-width ratios and operator choices. The former is crucial for small-batch-size latency, while the latter affects both latency and large-batch-size throughput. In light of this, we first study latency-optimal depth-width ratios, with the key finding that although deep-thin models generally achieve better accuracy under the same parameter budget, they may not lie on the accuracy-latency trade-off frontier. Next, we explore emerging efficient attention alternatives to evaluate their potential as candidate building operators. Using the identified promising operators, we construct an evolutionary search framework to automatically discover latency-optimal combinations of these operators within hybrid SLMs, thereby advancing the accuracy-latency frontier. In addition to architectural improvements, we further enhance SLM training using a weight normalization technique that enables more effective weight updates and improves final convergence. Combining these methods, we introduce a new family of hybrid SLMs, called Nemotron-Flash, which significantly advances the accuracy-efficiency frontier of state-of-the-art SLMs, e.g., achieving over +5.5% average accuracy, 1.3x/1.9x lower latency, and 18.7x/45.6x higher throughput compared to Qwen3-1.7B/0.6B, respectively."}
{"id": "2511.18902", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18902", "abs": "https://arxiv.org/abs/2511.18902", "authors": ["Zengjie Hu", "Jiantao Qiu", "Tianyi Bai", "Haojin Yang", "Binhang Yuan", "Qi Jing", "Conghui He", "Wentao Zhang"], "title": "VADE: Variance-Aware Dynamic Sampling via Online Sample-Level Difficulty Estimation for Multimodal RL", "comment": null, "summary": "Group-based policy optimization methods like GRPO and GSPO have become standard for training multimodal models, leveraging group-wise rollouts and relative advantage estimation. However, they suffer from a critical \\emph{gradient vanishing} problem when all responses within a group receive identical rewards, causing advantage estimates to collapse and training signals to diminish. Existing attempts to mitigate this issue fall into two paradigms: filtering-based and sampling-based methods. Filtering-based methods first generate rollouts broadly and then retroactively filter out uninformative groups, leading to substantial computational overhead. Sampling-based methods proactively select effective samples before rollout but rely on static criteria or prior dataset knowledge, lacking real-time adaptability. To address these issues, we propose \\textbf{VADE}, a \\textbf{V}ariance-\\textbf{A}ware \\textbf{D}ynamic sampling framework via online sample-level difficulty \\textbf{E}stimation. Our framework integrates three key components: online sample-level difficulty estimation using Beta distributions, a Thompson sampler that maximizes information gain through the estimated correctness probability, and a two-scale prior decay mechanism that maintains robust estimation under policy evolution. This three components design enables VADE to dynamically select the most informative samples, thereby amplifying training signals while eliminating extra rollout costs. Extensive experiments on multimodal reasoning benchmarks show that VADE consistently outperforms strong baselines in both performance and sample efficiency, while achieving a dramatic reduction in computational overhead. More importantly, our framework can serves as a plug-and-play component to be seamlessly integrated into existing group-based RL algorithms. Code and models are available at https://VADE-RL.github.io."}
{"id": "2511.18903", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18903", "abs": "https://arxiv.org/abs/2511.18903", "authors": ["Kairong Luo", "Zhenbo Sun", "Haodong Wen", "Xinyu Shi", "Jiarui Cui", "Chenyi Dang", "Kaifeng Lyu", "Wenguang Chen"], "title": "How Learning Rate Decay Wastes Your Best Data in Curriculum-Based LLM Pretraining", "comment": null, "summary": "Due to the scarcity of high-quality data, large language models (LLMs) are often trained on mixtures of data with varying quality levels, even after sophisticated data curation. A natural approach to better leverage high-quality data is curriculum-based pretraining, where the model is trained on data sorted in ascending order of quality as determined by a quality metric. However, prior studies have reported limited improvements from such curriculum-based pretraining strategies. This work identifies a critical factor constraining these methods: the incompatibility between the ascending data quality order and the decaying learning rate (LR) schedule. We find that while curriculum-based training substantially outperforms random shuffling when using a constant LR, its advantage diminishes under standard LR decay schedules. Our experiments show this incompatibility can be mitigated by two simple strategies: (1) employing a more moderate LR decay schedule, where the final LR is only moderately smaller than the peak LR, and (2) replacing LR decay with model averaging, i.e., computing a weighted average of the final few checkpoints. By combining these strategies, we improve the average score on a suite of standard benchmarks by 1.64% over random shuffling, without additional data refinement. Validated on 1.5B-parameter models trained over 30B tokens with various data-quality metrics, our findings call for a re-evaluation of curriculum-based LLM pretraining and underscore the potential of co-designing data curricula with optimization methods."}
{"id": "2511.18930", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18930", "abs": "https://arxiv.org/abs/2511.18930", "authors": ["Salah Eddine Choutri", "Prajwal Chauhan", "Othmane Mazhar", "Saif Eddin Jabari"], "title": "Learning Solution Operators for Partial Differential Equations via Monte Carlo-Type Approximation", "comment": "NeurIPS 2025 Workshop on Machine Learning and the Physical Sciences", "summary": "The Monte Carlo-type Neural Operator (MCNO) introduces a lightweight architecture for learning solution operators for parametric PDEs by directly approximating the kernel integral using a Monte Carlo approach. Unlike Fourier Neural Operators, MCNO makes no spectral or translation-invariance assumptions. The kernel is represented as a learnable tensor over a fixed set of randomly sampled points. This design enables generalization across multiple grid resolutions without relying on fixed global basis functions or repeated sampling during training. Experiments on standard 1D PDE benchmarks show that MCNO achieves competitive accuracy with low computational cost, providing a simple and practical alternative to spectral and graph-based neural operators."}
{"id": "2511.18936", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18936", "abs": "https://arxiv.org/abs/2511.18936", "authors": ["Santhosh G S", "Saurav Prakash", "Balaraman Ravindran"], "title": "SWAN: Sparse Winnowed Attention for Reduced Inference Memory via Decompression-Free KV-Cache Compression", "comment": null, "summary": "Large Language Models (LLMs) face a significant bottleneck during autoregressive inference due to the massive memory footprint of the Key-Value (KV) cache. Existing compression techniques like token eviction, quantization, or other low-rank methods often risk information loss, have fixed limits, or introduce significant computational overhead from explicit decompression steps. In this work, we introduce SWAN, a novel, fine-tuning-free framework that eliminates this overhead. Our method uses an offline orthogonal matrix to rotate and prune the KV-cache, which is then used directly in the attention computation without any reconstruction. Our extensive experiments demonstrate that SWAN, augmented with a small dense buffer, offers a robust trade-off, maintaining performance close to the uncompressed baseline even at aggressive 50-60% memory savings per-token on KV-cache. A key advantage is its runtime-tunable compression level, allowing operators to dynamically adjust the memory footprint, a flexibility absent in methods requiring fixed offline configurations. This combination of a decompression-free design, high performance under compression, and adaptability makes SWAN a practical and efficient solution for serving LLMs with long contexts."}
{"id": "2511.18940", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.18940", "abs": "https://arxiv.org/abs/2511.18940", "authors": ["Sanjeev Manivannan", "Chandrashekar Lakshminarayan"], "title": "Geometry-Aware Deep Congruence Networks for Manifold Learning in Cross-Subject Motor Imagery", "comment": "10 pages, 2 figures", "summary": "Cross-subject motor-imagery decoding remains a major challenge in EEG-based brain-computer interfaces due to strong subject variability and the curved geometry of covariance matrices on the symmetric positive definite (SPD) manifold. We address the zero-shot cross-subject setting, where no target-subject labels or adaptation are allowed, by introducing novel geometry-aware preprocessing modules and deep congruence networks that operate directly on SPD covariance matrices. Our preprocessing modules, DCR and RiFU, extend Riemannian Alignment by improving action separation while reducing subject-specific distortions. We further propose two manifold classifiers, SPD-DCNet and RiFUNet, which use hierarchical congruence transforms to learn discriminative, subject-invariant covariance representations. On the BCI-IV 2a benchmark, our framework improves cross-subject accuracy by 3-4% over the strongest classical baselines, demonstrating the value of geometry-aware transformations for robust EEG decoding."}
{"id": "2511.18945", "categories": ["cs.LG", "cs.IT"], "pdf": "https://arxiv.org/pdf/2511.18945", "abs": "https://arxiv.org/abs/2511.18945", "authors": ["German Gritsai", "Megan Richards", "Maxime Méloux", "Kyunghyun Cho", "Maxime Peyrard"], "title": "MIST: Mutual Information Via Supervised Training", "comment": null, "summary": "We propose a fully data-driven approach to designing mutual information (MI) estimators. Since any MI estimator is a function of the observed sample from two random variables, we parameterize this function with a neural network (MIST) and train it end-to-end to predict MI values. Training is performed on a large meta-dataset of 625,000 synthetic joint distributions with known ground-truth MI. To handle variable sample sizes and dimensions, we employ a two-dimensional attention scheme ensuring permutation invariance across input samples. To quantify uncertainty, we optimize a quantile regression loss, enabling the estimator to approximate the sampling distribution of MI rather than return a single point estimate. This research program departs from prior work by taking a fully empirical route, trading universal theoretical guarantees for flexibility and efficiency. Empirically, the learned estimators largely outperform classical baselines across sample sizes and dimensions, including on joint distributions unseen during training. The resulting quantile-based intervals are well-calibrated and more reliable than bootstrap-based confidence intervals, while inference is orders of magnitude faster than existing neural baselines. Beyond immediate empirical gains, this framework yields trainable, fully differentiable estimators that can be embedded into larger learning pipelines. Moreover, exploiting MI's invariance to invertible transformations, meta-datasets can be adapted to arbitrary data modalities via normalizing flows, enabling flexible training for diverse target meta-distributions."}
{"id": "2511.18958", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18958", "abs": "https://arxiv.org/abs/2511.18958", "authors": ["Qisen Chai", "Yansong Wang", "Junjie Huang", "Tao Jia"], "title": "Learning to Compress Graphs via Dual Agents for Consistent Topological Robustness Evaluation", "comment": null, "summary": "As graph-structured data grow increasingly large, evaluating their robustness under adversarial attacks becomes computationally expensive and difficult to scale. To address this challenge, we propose to compress graphs into compact representations that preserve both topological structure and robustness profile, enabling efficient and reliable evaluation.We propose Cutter, a dual-agent reinforcement learning framework composed of a Vital Detection Agent (VDA) and a Redundancy Detection Agent (RDA), which collaboratively identify structurally vital and redundant nodes for guided compression. Cutter incorporates three key strategies to enhance learning efficiency and compression quality: trajectory-level reward shaping to transform sparse trajectory returns into dense, policy-equivalent learning signals; prototype-based shaping to guide decisions using behavioral patterns from both highand low-return trajectories; and cross-agent imitation to enable safer and more transferable exploration. Experiments on multiple real-world graphs demonstrate that Cutter generates compressed graphs that retain essential static topological properties and exhibit robustness degradation trends highly consistent with the original graphs under various attack scenarios, thereby significantly improving evaluation efficiency without compromising assessment fidelity."}
{"id": "2511.18960", "categories": ["cs.LG", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18960", "abs": "https://arxiv.org/abs/2511.18960", "authors": ["Lei Xiao", "Jifeng Li", "Juntao Gao", "Feiyang Ye", "Yan Jin", "Jingjing Qian", "Jing Zhang", "Yong Wu", "Xiaoyuan Yu"], "title": "AVA-VLA: Improving Vision-Language-Action models with Active Visual Attention", "comment": "18 pages, 10 figures", "summary": "Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in embodied AI tasks. However, existing VLA models, often built upon Vision-Language Models (VLMs), typically process dense visual inputs independently at each timestep. This approach implicitly models the task as a Markov Decision Process (MDP). However, this history-agnostic design is suboptimal for effective visual token processing in dynamic sequential decision-making, as it fails to leverage the context of history. To address this limitation, we reformulate the problem from a Partially Observable Markov Decision Process (POMDP) perspective and propose a novel framework named AVA-VLA. Inspired by the POMDP that the action generation should be conditioned on the belief state. AVA-VLA introduces Active Visual Attention (AVA) to dynamically modulate visual processing. It achieves this by leveraging the recurrent state, which is a neural approximation of the agent's belief state derived from the previous decision step. Specifically, the AVA module uses the recurrent state to compute the soft weights to actively process task-relevant visual tokens based on its historical context. Comprehensive evaluations demonstrate that AVA-VLA achieves state-of-the-art performance across popular robotic benchmarks, including LIBERO and CALVIN. Furthermore, real-world deployments on a dual-arm robot platform validate the framework's practical applicability and robust sim-to-real transferability."}
{"id": "2511.18977", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18977", "abs": "https://arxiv.org/abs/2511.18977", "authors": ["Xin Yuan", "Siqi Li", "Jiateng Wei", "Chengrui Zhu", "Yanming Wu", "Qingpeng Li", "Jiajun Lv", "Xiaoke Lan", "Jun Chen", "Yong Liu"], "title": "FastForward Pruning: Efficient LLM Pruning via Single-Step Reinforcement Learning", "comment": "5 pages, 2 figures, 4 tables", "summary": "Pruning is an effective method for compressing Large Language Models, but finding an optimal, non-uniform layer-wise sparsity allocation remains a key challenge. While heuristic methods are fast but yield suboptimal performance, more powerful search-based approaches like Reinforcement Learning are often hindered by prohibitive computational costs on large-scale models. To overcome this efficiency barrier, we propose FastForward Pruning. Its core is a decoupled, single-step RL framework that separates policy optimization from the complex budget satisfaction problem. Such a decoupling is crucial for efficiently searching the vast policy space of LLMs. This curriculum-based strategy begins with low-cost, simple tasks and gradually increases in complexity, significantly reducing the search's computational overhead. Evaluated on the LLaMA, Mistral, and OPT model families, our framework discovers pruning policies that achieve superior performance over strong heuristic baselines. Crucially, when compared to other search-based algorithms, our method achieves competitive or superior results at a fraction of the computational cost, demonstrating a clear advantage in search efficiency."}
{"id": "2511.18987", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18987", "abs": "https://arxiv.org/abs/2511.18987", "authors": ["Donghu Kim"], "title": "Dynamic Mixture of Experts Against Severe Distribution Shifts", "comment": null, "summary": "The challenge of building neural networks that can continuously learn and adapt to evolving data streams is central to the fields of continual learning (CL) and reinforcement learning (RL). This lifelong learning problem is often framed in terms of the plasticity-stability dilemma, focusing on issues like loss of plasticity and catastrophic forgetting. Unlike neural networks, biological brains maintain plasticity through capacity growth, inspiring researchers to explore similar approaches in artificial networks, such as adding capacity dynamically. Prior solutions often lack parameter efficiency or depend on explicit task indices, but Mixture-of-Experts (MoE) architectures offer a promising alternative by specializing experts for distinct distributions. This paper aims to evaluate a DynamicMoE approach for continual and reinforcement learning environments and benchmark its effectiveness against existing network expansion methods."}
{"id": "2511.19019", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19019", "abs": "https://arxiv.org/abs/2511.19019", "authors": ["Nguyen Duc Minh Quang", "Chang Liu", "Huy-Trung Nguyen", "Shuangyang Li", "Derrick Wing Kwan Ng", "Wei Xiang"], "title": "3D Dynamic Radio Map Prediction Using Vision Transformers for Low-Altitude Wireless Networks", "comment": "7 pages, 4 figures, submitted to IEEE ICC 2026", "summary": "Low-altitude wireless networks (LAWN) are rapidly expanding with the growing deployment of unmanned aerial vehicles (UAVs) for logistics, surveillance, and emergency response. Reliable connectivity remains a critical yet challenging task due to three-dimensional (3D) mobility, time-varying user density, and limited power budgets. The transmit power of base stations (BSs) fluctuates dynamically according to user locations and traffic demands, leading to a highly non-stationary 3D radio environment. Radio maps (RMs) have emerged as an effective means to characterize spatial power distributions and support radio-aware network optimization. However, most existing works construct static or offline RMs, overlooking real-time power variations and spatio-temporal dependencies in multi-UAV networks. To overcome this limitation, we propose a {3D dynamic radio map (3D-DRM)} framework that learns and predicts the spatio-temporal evolution of received power. Specially, a Vision Transformer (ViT) encoder extracts high-dimensional spatial representations from 3D RMs, while a Transformer-based module models sequential dependencies to predict future power distributions. Experiments unveil that 3D-DRM accurately captures fast-varying power dynamics and substantially outperforms baseline models in both RM reconstruction and short-term prediction."}
{"id": "2511.19023", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19023", "abs": "https://arxiv.org/abs/2511.19023", "authors": ["Yuting Gao", "Weihao Chen", "Lan Wang", "Ruihan Xu", "Qingpei Guo"], "title": "OrdMoE: Preference Alignment via Hierarchical Expert Group Ranking in Multimodal Mixture-of-Experts LLMs", "comment": null, "summary": "Preference learning has recently emerged as a pivotal strategy for post-training alignment of Multimodal Large Language Models (MLLMs). However, existing approaches predominantly rely on external human-annotated preference data, which is costly and labor-intensive to collect. In this work, we propose OrdMoE, a novel preference alignment framework that bypasses the reliance on external human preferences entirely by leveraging intrinsic signals within Mixture-of-Experts (MoE) architectures. Specifically, we observe that the router's expert selection scores implicitly encode a quality-aware ranking of responses (i.e. higher-scoring experts consistently generate higher-quality outputs). Building on this insight, OrdMoE constructs an internal preference hierarchy by grouping experts into ranked tiers based on their per-token routing scores and activating each tier separately to produce a sequence of responses with increasing quality. This yields a zero-cost, self-supervised preference ordering over generated responses, which can be directly optimized using standard preference learning objectives. Extensive experiments across multiple multimodal benchmarks demnstrate that OrdMoE significantly enhances both alignment and overall performance of multimodal Mixture-of-Experts LLMs, achieving competitive results without requiring any human-annotated preference data."}
{"id": "2511.19037", "categories": ["cs.LG", "math.PR"], "pdf": "https://arxiv.org/pdf/2511.19037", "abs": "https://arxiv.org/abs/2511.19037", "authors": ["Zimo Yan", "Zheng Xie", "Chang Liu", "Yuan Wang"], "title": "Resolving Node Identifiability in Graph Neural Processes via Laplacian Spectral Encodings", "comment": null, "summary": "Message passing graph neural networks are widely used for learning on graphs, yet their expressive power is limited by the one-dimensional Weisfeiler-Lehman test and can fail to distinguish structurally different nodes. We provide rigorous theory for a Laplacian positional encoding that is invariant to eigenvector sign flips and to basis rotations within eigenspaces. We prove that this encoding yields node identifiability from a constant number of observations and establishes a sample-complexity separation from architectures constrained by the Weisfeiler-Lehman test. The analysis combines a monotone link between shortest-path and diffusion distance, spectral trilateration with a constant set of anchors, and quantitative spectral injectivity with logarithmic embedding size. As an instantiation, pairing this encoding with a neural-process style decoder yields significant gains on a drug-drug interaction task on chemical graphs, improving both the area under the ROC curve and the F1 score and demonstrating the practical benefits of resolving theoretical expressiveness limitations with principled positional information."}
{"id": "2511.19066", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19066", "abs": "https://arxiv.org/abs/2511.19066", "authors": ["Xiangyu Chang", "Manyi Yao", "Srikanth V. Krishnamurthy", "Christian R. Shelton", "Anirban Chakraborty", "Ananthram Swami", "Samet Oymak", "Amit Roy-Chowdhury"], "title": "Mitigating Participation Imbalance Bias in Asynchronous Federated Learning", "comment": null, "summary": "In Asynchronous Federated Learning (AFL), the central server immediately updates the global model with each arriving client's contribution. As a result, clients perform their local training on different model versions, causing information staleness (delay). In federated environments with non-IID local data distributions, this asynchronous pattern amplifies the adverse effect of client heterogeneity (due to different data distribution, local objectives, etc.), as faster clients contribute more frequent updates, biasing the global model. We term this phenomenon heterogeneity amplification. Our work provides a theoretical analysis that maps AFL design choices to their resulting error sources when heterogeneity amplification occurs. Guided by our analysis, we propose ACE (All-Client Engagement AFL), which mitigates participation imbalance through immediate, non-buffered updates that use the latest information available from all clients. We also introduce a delay-aware variant, ACED, to balance client diversity against update staleness. Experiments on different models for different tasks across diverse heterogeneity and delay settings validate our analysis and demonstrate the robust performance of our approaches."}
{"id": "2511.19087", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19087", "abs": "https://arxiv.org/abs/2511.19087", "authors": ["Ziyun Li", "Ben Dai", "Huancheng Hu", "Henrik Boström", "Soon Hoe Lim"], "title": "EnfoPath: Energy-Informed Analysis of Generative Trajectories in Flow Matching", "comment": "EurIPS 2025 Workshop on Principles of Generative Modeling (PriGM)", "summary": "Flow-based generative models synthesize data by integrating a learned velocity field from a reference distribution to the target data distribution. Prior work has focused on endpoint metrics (e.g., fidelity, likelihood, perceptual quality) while overlooking a deeper question: what do the sampling trajectories reveal? Motivated by classical mechanics, we introduce kinetic path energy (KPE), a simple yet powerful diagnostic that quantifies the total kinetic effort along each generation path of ODE-based samplers. Through comprehensive experiments on CIFAR-10 and ImageNet-256, we uncover two key phenomena: ({i}) higher KPE predicts stronger semantic quality, indicating that semantically richer samples require greater kinetic effort, and ({ii}) higher KPE inversely correlates with data density, with informative samples residing in sparse, low-density regions. Together, these findings reveal that semantically informative samples naturally reside on the sparse frontier of the data distribution, demanding greater generative effort. Our results suggest that trajectory-level analysis offers a physics-inspired and interpretable framework for understanding generation difficulty and sample characteristics."}
{"id": "2511.19090", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19090", "abs": "https://arxiv.org/abs/2511.19090", "authors": ["Shenghan Zhao", "Yuzhen Lin", "Ximeng Yang", "Qiaochu Lu", "Haozhong Xue", "Gaozhe Jiang"], "title": "Optimization of Deep Learning Models for Dynamic Market Behavior Prediction", "comment": null, "summary": "The advent of financial technology has witnessed a surge in the utilization of deep learning models to anticipate consumer conduct, a trend that has demonstrated considerable potential in enhancing lending strategies and bolstering market efficiency. We study multi-horizon demand forecasting on e-commerce transactions using the UCI Online Retail II dataset. Unlike prior versions of this manuscript that mixed financial-loan narratives with retail data, we focus exclusively on retail market behavior and define a clear prediction target: per SKU daily demand (or revenue) for horizons H=1,7,14. We present a hybrid sequence model that combines multi-scale temporal convolutions, a gated recurrent module, and time-aware self-attention. The model is trained with standard regression losses and evaluated under MAE, RMSE, sMAPE, MASE, and Theil's U_2 with strict time-based splits to prevent leakage. We benchmark against ARIMA/Prophet, LSTM/GRU, LightGBM, and state-of-the-art Transformer forecasters (TFT, Informer, Autoformer, N-BEATS). Results show consistent accuracy gains and improved robustness on peak/holiday periods. We further provide ablations and statistical significance tests to ensure the reliability of improvements, and we release implementation details to facilitate reproducibility."}
{"id": "2511.19103", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19103", "abs": "https://arxiv.org/abs/2511.19103", "authors": ["Dora Krekovic", "Mario Kusek", "Ivana Podnar Zarko", "Danh Le-Phuoc"], "title": "Edge-Based Predictive Data Reduction for Smart Agriculture: A Lightweight Approach to Efficient IoT Communication", "comment": "Accepted for presentation and publication in the proceedings of the IEEE Annual Congress on Artificial Intelligence of Things (IEEE AIoT 2025)", "summary": "The rapid growth of IoT devices has led to an enormous amount of sensor data that requires transmission to cloud servers for processing, resulting in excessive network congestion, increased latency and high energy consumption. This is particularly problematic in resource-constrained and remote environments where bandwidth is limited, and battery-dependent devices further emphasize the problem. Moreover, in domains such as agriculture, consecutive sensor readings often have minimal variation, making continuous data transmission inefficient and unnecessarily resource intensive. To overcome these challenges, we propose an analytical prediction algorithm designed for edge computing environments and validated through simulation. The proposed solution utilizes a predictive filter at the network edge that forecasts the next sensor data point and triggers data transmission only when the deviation from the predicted value exceeds a predefined tolerance. A complementary cloud-based model ensures data integrity and overall system consistency. This dual-model strategy effectively reduces communication overhead and demonstrates potential for improving energy efficiency by minimizing redundant transmissions. In addition to reducing communication load, our approach leverages both in situ and satellite observations from the same locations to enhance model robustness. It also supports cross-site generalization, enabling models trained in one region to be effectively deployed elsewhere without retraining. This makes our solution highly scalable, energy-aware, and well-suited for optimizing sensor data transmission in remote and bandwidth-constrained IoT environments."}
{"id": "2511.19107", "categories": ["cs.LG", "cs.AI", "cs.GT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.19107", "abs": "https://arxiv.org/abs/2511.19107", "authors": ["Robert Bredereck", "Eva Deltl", "Leon Kellerhals", "Jannik Peters"], "title": "The Core in Max-Loss Non-Centroid Clustering Can Be Empty", "comment": null, "summary": "We study core stability in non-centroid clustering under the max-loss objective, where each agent's loss is the maximum distance to other members of their cluster. We prove that for all $k\\geq 3$ there exist metric instances with $n\\ge 9$ agents, with $n$ divisible by $k$, for which no clustering lies in the $α$-core for any $α<2^{\\frac{1}{5}}\\sim 1.148$. The bound is tight for our construction. Using a computer-aided proof, we also identify a two-dimensional Euclidean point set whose associated lower bound is slightly smaller than that of our general construction. This is, to our knowledge, the first impossibility result showing that the core can be empty in non-centroid clustering under the max-loss objective."}
{"id": "2511.19124", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19124", "abs": "https://arxiv.org/abs/2511.19124", "authors": ["Krishang Sharma"], "title": "Uncertainty-Aware Deep Learning Framework for Remaining Useful Life Prediction in Turbofan Engines with Learned Aleatoric Uncertainty", "comment": "10 pages, 2 figures, 3 tables. Submitted to arXiv", "summary": "Accurate Remaining Useful Life (RUL) prediction coupled with uncertainty quantification remains a critical challenge in aerospace prognostics. This research introduces a novel uncertainty-aware deep learning framework that learns aleatoric uncertainty directly through probabilistic modeling, an approach unexplored in existing CMAPSS-based literature. Our hierarchical architecture integrates multi-scale Inception blocks for temporal pattern extraction, bidirectional Long Short-Term Memory networks for sequential modeling, and a dual-level attention mechanism operating simultaneously on sensor and temporal dimensions. The innovation lies in the Bayesian output layer that predicts both mean RUL and variance, enabling the model to learn data-inherent uncertainty. Comprehensive preprocessing employs condition-aware clustering, wavelet denoising, and intelligent feature selection. Experimental validation on NASA CMAPSS benchmarks (FD001-FD004) demonstrates competitive overall performance with RMSE values of 16.22, 19.29, 16.84, and 19.98 respectively. Remarkably, our framework achieves breakthrough critical zone performance (RUL <= 30 cycles) with RMSE of 5.14, 6.89, 5.27, and 7.16, representing 25-40 percent improvements over conventional approaches and establishing new benchmarks for safety-critical predictions. The learned uncertainty provides well-calibrated 95 percent confidence intervals with coverage ranging from 93.5 percent to 95.2 percent, enabling risk-aware maintenance scheduling previously unattainable in CMAPSS literature."}
{"id": "2511.19152", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.19152", "abs": "https://arxiv.org/abs/2511.19152", "authors": ["Prateek Garg", "Bhavya Kohli", "Sunita Sarawagi"], "title": "Masked Diffusion Models are Secretly Learned-Order Autoregressive Models", "comment": "Accepted at EurIPS 2025 Workshop on Principles of Generative Modeling (PriGM)", "summary": "Masked Diffusion Models (MDMs) have emerged as one of the most promising paradigms for generative modeling over discrete domains. It is known that MDMs effectively train to decode tokens in a random order, and that this ordering has significant performance implications in practice. This observation raises a fundamental question: can we design a training framework that optimizes for a favorable decoding order? We answer this in the affirmative, showing that the continuous-time variational objective of MDMs, when equipped with multivariate noise schedules, can identify and optimize for a decoding order during training. We establish a direct correspondence between decoding order and the multivariate noise schedule and show that this setting breaks invariance of the MDM objective to the noise schedule. Furthermore, we prove that the MDM objective decomposes precisely into a weighted auto-regressive losses over these orders, which establishes them as auto-regressive models with learnable orders."}
{"id": "2511.19165", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19165", "abs": "https://arxiv.org/abs/2511.19165", "authors": ["Fabian Schramm", "Nicolas Perrin-Gilbert", "Justin Carpentier"], "title": "First-order Sobolev Reinforcement Learning", "comment": "Workshop paper at Differentiable Systems and Scientific Machine Learning, EurIPS 2025", "summary": "We propose a refinement of temporal-difference learning that enforces first-order Bellman consistency: the learned value function is trained to match not only the Bellman targets in value but also their derivatives with respect to states and actions. By differentiating the Bellman backup through differentiable dynamics, we obtain analytically consistent gradient targets. Incorporating these into the critic objective using a Sobolev-type loss encourages the critic to align with both the value and local geometry of the target function. This first-order TD matching principle can be seamlessly integrated into existing algorithms, such as Q-learning or actor-critic methods (e.g., DDPG, SAC), potentially leading to faster critic convergence and more stable policy gradients without altering their overall structure."}
{"id": "2511.19168", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.19168", "abs": "https://arxiv.org/abs/2511.19168", "authors": ["Deyi Ji", "Yuekui Yang", "Liqun Liu", "Peng Shu", "Haiyang Wu", "Shaogang Tang", "Xudong Chen", "Shaoping Ma", "Tianrun Chen", "Lanyun Zhu"], "title": "RAVEN++: Pinpointing Fine-Grained Violations in Advertisement Videos with Active Reinforcement Reasoning", "comment": "EMNLP 2025 (Oral, Industry Track)", "summary": "Advertising (Ad) is a cornerstone of the digital economy, yet the moderation of video advertisements remains a significant challenge due to their complexity and the need for precise violation localization. While recent advancements, such as the RAVEN model, have improved coarse-grained violation detection, critical gaps persist in fine-grained understanding, explainability, and generalization. To address these limitations, we propose RAVEN++, a novel framework that introduces three key innovations: 1) Active Reinforcement Learning (RL), which dynamically adapts training to samples of varying difficulty; 2) Fine-Grained Violation Understanding, achieved through hierarchical reward functions and reasoning distillation; and 3) Progressive Multi-Stage Training, which systematically combines knowledge injection, curriculum-based passive RL, and active RL. Extensive experiments on both public and proprietary datasets, on both offline scenarios and online deployed A/B Testing, demonstrate that RAVEN++ outperforms general-purpose LLMs and specialized models like RAVEN in terms of fine-grained violation understanding, reasoning capabilities, and generalization ability."}
{"id": "2511.19176", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.19176", "abs": "https://arxiv.org/abs/2511.19176", "authors": ["Jeeho Shin", "Kyungho Kim", "Kijung Shin"], "title": "From Raw Features to Effective Embeddings: A Three-Stage Approach for Multimodal Recipe Recommendation", "comment": null, "summary": "Recipe recommendation has become an essential task in web-based food platforms. A central challenge is effectively leveraging rich multimodal features beyond user-recipe interactions. Our analysis shows that even simple uses of multimodal signals yield competitive performance, suggesting that systematic enhancement of these signals is highly promising. We propose TESMR, a 3-stage framework for recipe recommendation that progressively refines raw multimodal features into effective embeddings through: (1) content-based enhancement using foundation models with multimodal comprehension, (2) relation-based enhancement via message propagation over user-recipe interactions, and (3) learning-based enhancement through contrastive learning with learnable embeddings. Experiments on two real-world datasets show that TESMR outperforms existing methods, achieving 7-15% higher Recall@10."}
{"id": "2511.19240", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19240", "abs": "https://arxiv.org/abs/2511.19240", "authors": ["Minxin Chen"], "title": "Empirical Comparison of Forgetting Mechanisms for UCB-based Algorithms on a Data-Driven Simulation Platform", "comment": null, "summary": "Many real-world bandit problems involve non-stationary reward distributions, where the optimal decision may shift due to evolving environments. However, the performance of some typical Multi-Armed Bandit (MAB) models such as Upper Confidence Bound (UCB) algorithms degrades significantly in non-stationary environments where reward distributions change over time. To address this limitation, this paper introduces and evaluates FDSW-UCB, a novel dual-view algorithm that integrates a discount-based long-term perspective with a sliding-window-based short-term view. A data-driven semi-synthetic simulation platform, built upon the MovieLens-1M and Open Bandit datasets, is developed to test algorithm adaptability under abrupt and gradual drift scenarios. Experimental results demonstrate that a well-configured sliding-window mechanism (SW-UCB) is robust, while the widely used discounting method (D-UCB) suffers from a fundamental learning failure, leading to linear regret. Crucially, the proposed FDSW-UCB, when employing an optimistic aggregation strategy, achieves superior performance in dynamic settings, highlighting that the ensemble strategy itself is a decisive factor for success."}
{"id": "2511.19241", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.19241", "abs": "https://arxiv.org/abs/2511.19241", "authors": ["David Stenger", "Armin Lindicke", "Alexander von Rohr", "Sebastian Trimpe"], "title": "Local Entropy Search over Descent Sequences for Bayesian Optimization", "comment": null, "summary": "Searching large and complex design spaces for a global optimum can be infeasible and unnecessary. A practical alternative is to iteratively refine the neighborhood of an initial design using local optimization methods such as gradient descent. We propose local entropy search (LES), a Bayesian optimization paradigm that explicitly targets the solutions reachable by the descent sequences of iterative optimizers. The algorithm propagates the posterior belief over the objective through the optimizer, resulting in a probability distribution over descent sequences. It then selects the next evaluation by maximizing mutual information with that distribution, using a combination of analytic entropy calculations and Monte-Carlo sampling of descent sequences. Empirical results on high-complexity synthetic objectives and benchmark problems show that LES achieves strong sample efficiency compared to existing local and global Bayesian optimization methods."}
{"id": "2511.19253", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19253", "abs": "https://arxiv.org/abs/2511.19253", "authors": ["Boyuan Wu"], "title": "MAESTRO: Multi-Agent Environment Shaping through Task and Reward Optimization", "comment": "Preprint. 16 pages, 6 figures. Preliminary version; extended experiments and analysis forthcoming", "summary": "Cooperative Multi-Agent Reinforcement Learning (MARL) faces two major design bottlenecks: crafting dense reward functions and constructing curricula that avoid local optima in high-dimensional, non-stationary environments. Existing approaches rely on fixed heuristics or use Large Language Models (LLMs) directly in the control loop, which is costly and unsuitable for real-time systems. We propose MAESTRO (Multi-Agent Environment Shaping through Task and Reward Optimization), a framework that moves the LLM outside the execution loop and uses it as an offline training architect. MAESTRO introduces two generative components: (i) a semantic curriculum generator that creates diverse, performance-driven traffic scenarios, and (ii) an automated reward synthesizer that produces executable Python reward functions adapted to evolving curriculum difficulty. These components guide a standard MARL backbone (MADDPG) without increasing inference cost at deployment. We evaluate MAESTRO on large-scale traffic signal control (Hangzhou, 16 intersections) and conduct controlled ablations. Results show that combining LLM-generated curricula with LLM-generated reward shaping yields improved performance and stability. Across four seeds, the full system achieves +4.0% higher mean return (163.26 vs. 156.93) and 2.2% better risk-adjusted performance (Sharpe 1.53 vs. 0.70) over a strong curriculum baseline. These findings highlight LLMs as effective high-level designers for cooperative MARL training."}
{"id": "2511.19260", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.19260", "abs": "https://arxiv.org/abs/2511.19260", "authors": ["Kyle Verrier", "Achille Nazaret", "Joseph Futoma", "Andrew C. Miller", "Guillermo Sapiro"], "title": "A Nutrition Multimodal Photoplethysmography Language Model", "comment": "21 pages, 2 figures", "summary": "Hunger and satiety dynamics shape dietary behaviors and metabolic health, yet remain difficult to capture in everyday settings. We present a Nutrition Photoplethysmography Language Model (NPLM), integrating continuous photoplethysmography (PPG) from wearables with meal descriptions. NPLM projects PPG into embeddings interpretable by language models, enabling joint reasoning over physiology and meal context. Trained on 19,340 participants and 1.1 million meal-PPG pairs, the model improved daily caloric intake prediction by 11% over text-only baselines, with accuracy maintained when 80% of meal text was removed. In an independent validation study (n=140) with controlled dining and detailed meal information, the model replicated these findings. These results demonstrate the value of integrating physiological measurements from consumer wearables with meal information for noninvasive dietary monitoring at scale."}
{"id": "2511.19263", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19263", "abs": "https://arxiv.org/abs/2511.19263", "authors": ["Lucas Li", "Jean-Baptiste Puel", "Florence Carton", "Dounya Barrit", "Jhony H. Giraldo"], "title": "Solar-GECO: Perovskite Solar Cell Property Prediction with Geometric-Aware Co-Attention", "comment": "Accepted at the AI for Accelerated Materials Design (AI4Mat) Workshop at NeurIPS 2025. 14 pages, 4 figures", "summary": "Perovskite solar cells are promising candidates for next-generation photovoltaics. However, their performance as multi-scale devices is determined by complex interactions between their constituent layers. This creates a vast combinatorial space of possible materials and device architectures, making the conventional experimental-based screening process slow and expensive. Machine learning models try to address this problem, but they only focus on individual material properties or neglect the important geometric information of the perovskite crystal. To address this problem, we propose to predict perovskite solar cell power conversion efficiency with a geometric-aware co-attention (Solar-GECO) model. Solar-GECO combines a geometric graph neural network (GNN) - that directly encodes the atomic structure of the perovskite absorber - with language model embeddings that process the textual strings representing the chemical compounds of the transport layers and other device components. Solar-GECO also integrates a co-attention module to capture intra-layer dependencies and inter-layer interactions, while a probabilistic regression head predicts both power conversion efficiency (PCE) and its associated uncertainty. Solar-GECO achieves state-of-the-art performance, significantly outperforming several baselines, reducing the mean absolute error (MAE) for PCE prediction from 3.066 to 2.936 compared to semantic GNN (the previous state-of-the-art model). Solar-GECO demonstrates that integrating geometric and textual information provides a more powerful and accurate framework for PCE prediction."}
{"id": "2511.19264", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2511.19264", "abs": "https://arxiv.org/abs/2511.19264", "authors": ["Amirtha Varshini A S", "Duminda S. Ranasinghe", "Hok Hei Tam"], "title": "Interpreting GFlowNets for Drug Discovery: Extracting Actionable Insights for Medicinal Chemistry", "comment": "13 pages, 7 figures. Accepted for presentation at NeurIPS 2025 WiML Workshop and Molecular Machine Learning Conference (MoML) 2025", "summary": "Generative Flow Networks, or GFlowNets, offer a promising framework for molecular design, but their internal decision policies remain opaque. This limits adoption in drug discovery, where chemists require clear and interpretable rationales for proposed structures. We present an interpretability framework for SynFlowNet, a GFlowNet trained on documented chemical reactions and purchasable starting materials that generates both molecules and the synthetic routes that produce them. Our approach integrates three complementary components. Gradient based saliency combined with counterfactual perturbations identifies which atomic environments influence reward and how structural edits change molecular outcomes. Sparse autoencoders reveal axis aligned latent factors that correspond to physicochemical properties such as polarity, lipophilicity, and molecular size. Motif probes show that functional groups including aromatic rings and halogens are explicitly encoded and linearly decodable from the internal embeddings. Together, these results expose the chemical logic inside SynFlowNet and provide actionable and mechanistic insight that supports transparent and controllable molecular design."}
{"id": "2511.19265", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19265", "abs": "https://arxiv.org/abs/2511.19265", "authors": ["Bianka Kowalska", "Halina Kwaśnicka"], "title": "Unboxing the Black Box: Mechanistic Interpretability for Algorithmic Understanding of Neural Networks", "comment": null, "summary": "The black box nature of deep neural networks poses a significant challenge for the deployment of transparent and trustworthy artificial intelligence (AI) systems. With the growing presence of AI in society, it becomes increasingly important to develop methods that can explain and interpret the decisions made by these systems. To address this, mechanistic interpretability (MI) emerged as a promising and distinctive research program within the broader field of explainable artificial intelligence (XAI). MI is the process of studying the inner computations of neural networks and translating them into human-understandable algorithms. It encompasses reverse engineering techniques aimed at uncovering the computational algorithms implemented by neural networks. In this article, we propose a unified taxonomy of MI approaches and provide a detailed analysis of key techniques, illustrated with concrete examples and pseudo-code. We contextualize MI within the broader interpretability landscape, comparing its goals, methods, and insights to other strands of XAI. Additionally, we trace the development of MI as a research area, highlighting its conceptual roots and the accelerating pace of recent work. We argue that MI holds significant potential to support a more scientific understanding of machine learning systems -- treating models not only as tools for solving tasks, but also as systems to be studied and understood. We hope to invite new researchers into the field of mechanistic interpretability."}
{"id": "2511.19267", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19267", "abs": "https://arxiv.org/abs/2511.19267", "authors": ["Manish Singh", "Arpita Dayama"], "title": "Leveraging Spatiotemporal Graph Neural Networks for Multi-Store Sales Forecasting", "comment": "6 pages, 4 figures, 1 table", "summary": "This work evaluates the effectiveness of spatiotemporal Graph Neural Networks (GNNs) for multi-store retail sales forecasting and compares their performance against ARIMA, LSTM, and XGBoost baselines. Using weekly sales data from 45 Walmart stores, we construct a relational forecasting framework that models inter-store dependencies through a learned adaptive graph. The proposed STGNN predicts log-differenced sales and reconstructs final values through a residual path, enabling stable training and improved generalisation. Experiments show that STGNN achieves the lowest overall forecasting error, outperforming all baselines in Normalised Total Absolute Error, P90 MAPE, and variance of MAPE across stores. Analysis of the learned adjacency matrix reveals meaningful functional store clusters and high-influence nodes that emerge without geographic metadata. These results demonstrate that relational structure significantly improves forecast quality in interconnected retail environments and establishes STGNNs as a robust modelling choice for multi-store demand prediction."}
{"id": "2511.19269", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.19269", "abs": "https://arxiv.org/abs/2511.19269", "authors": ["Minseo Kim", "Chenfeng Xu", "Coleman Hooper", "Harman Singh", "Ben Athiwaratkun", "Ce Zhang", "Kurt Keutzer", "Amir Gholami"], "title": "CDLM: Consistency Diffusion Language Models For Faster Sampling", "comment": "18 pages, 6 figures", "summary": "Diffusion Language Models (DLMs) offer a promising parallel generation paradigm but suffer from slow inference due to numerous refinement steps and the inability to use standard KV caching. We introduce CDLM (Consistency Diffusion Language Models), a training-based acceleration method that simultaneously tackles both bottlenecks. CDLM integrates consistency modeling to drastically reduce the number of required sampling steps by enabling multi-token finalization. Furthermore, we enforce a block-wise causal attention mask during fine-tuning, making the model fully compatible with KV caching. Experiments show CDLM achieves 3.6x-14.5x lower latency while maintaining competitive accuracy on math and coding tasks. The full training and evaluation code is available at https://github.com/SqueezeAILab/CDLM."}
{"id": "2511.19272", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19272", "abs": "https://arxiv.org/abs/2511.19272", "authors": ["Felix Birkel"], "title": "Tiny-TSM: Efficiently Training a Lightweight SOTA Time Series Foundation Model", "comment": null, "summary": "We present Tiny-TSM, a time series foundation model characterized by small scale, economical training, and state-of-the-art performance. It comprises 23M total parameters, trained on a single A100 GPU in less than a week using a new synthetic data generation and data augmentation pipeline (SynthTS). Without any neural architecture search, hyperparameter tuning, or scaling up model size, Tiny-TSM achieves state-of-the-art performance on a wide range of time series benchmark datasets, often outperforming much larger models and even matching the performance of much larger, industrial-scale, likely highly tuned foundation models. Specifically, Tiny-TSM outperforms all other time series foundation models we evaluated on medium- and long-term forecasting tasks under MSE loss, while short-term accuracy is still competitive with state-of-the-art models.\n  We also introduce a causal input normalization scheme that enables time series models to be trained with dense next-token prediction loss, significantly accelerating convergence speed and reducing training time.\n  All experiments were conducted on a single A100 GPU, illustrating the practicality of the proposed approach in a resource-constrained setting."}
{"id": "2511.19273", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19273", "abs": "https://arxiv.org/abs/2511.19273", "authors": ["Kunal Dumbre", "Lei Jiao", "Ole-Christoffer Granmo"], "title": "Scalable Bayesian Network Structure Learning Using Tsetlin Machine to Constrain the Search Space", "comment": null, "summary": "The PC algorithm is a widely used method in causal inference for learning the structure of Bayesian networks. Despite its popularity, the PC algorithm suffers from significant time complexity, particularly as the size of the dataset increases, which limits its applicability in large-scale real-world problems. In this study, we propose a novel approach that utilises the Tsetlin Machine (TM) to construct Bayesian structures more efficiently. Our method leverages the most significant literals extracted from the TM and performs conditional independence (CI) tests on these selected literals instead of the full set of variables, resulting in a considerable reduction in computational time. We implemented our approach and compared it with various state-of-the-art methods. Our evaluation includes categorical datasets from the bnlearn repository, such as Munin1, Hepar2. The findings indicate that the proposed TM-based method not only reduces computational complexity but also maintains competitive accuracy in causal discovery, making it a viable alternative to traditional PC algorithm implementations by offering improved efficiency without compromising performance."}
{"id": "2511.19277", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19277", "abs": "https://arxiv.org/abs/2511.19277", "authors": ["Brittany V. Lancellotti", "Jordan M. Malof", "Aaron Davitt", "Gavin McCormick", "Shelby Anderson", "Pol Carbó-Mestre", "Gary Collins", "Verity Crane", "Zoheyr Doctor", "George Ebri", "Kevin Foster", "Trey M. Gowdy", "Michael Guzzardi", "John Heal", "Heather Hunter", "David Kroodsma", "Khandekar Mahammad Galib", "Paul J. Markakis", "Gavin McDonald", "Daniel P. Moore", "Eric D. Nguyen", "Sabina Parvu", "Michael Pekala", "Christine D. Piatko", "Amy Piscopo", "Mark Powell", "Krsna Raniga", "Elizabeth P. Reilly", "Michael Robinette", "Ishan Saraswat", "Patrick Sicurello", "Isabella Söldner-Rembold", "Raymond Song", "Charlotte Underwood", "Kyle Bradbury"], "title": "Closing Gaps in Emissions Monitoring with Climate TRACE", "comment": null, "summary": "Global greenhouse gas emissions estimates are essential for monitoring and mitigation planning. Yet most datasets lack one or more characteristics that enhance their actionability, such as accuracy, global coverage, high spatial and temporal resolution, and frequent updates. To address these gaps, we present Climate TRACE (climatetrace.org), an open-access platform delivering global emissions estimates with enhanced detail, coverage, and timeliness. Climate TRACE synthesizes existing emissions data, prioritizing accuracy, coverage, and resolution, and fills gaps using sector-specific estimation approaches. The dataset is the first to provide globally comprehensive emissions estimates for individual sources (e.g., individual power plants) for all anthropogenic emitting sectors. The dataset spans January 1, 2021, to the present, with a two-month reporting lag and monthly updates. The open-access platform enables non-technical audiences to engage with detailed emissions datasets for most subnational governments worldwide. Climate TRACE supports data-driven climate action at scales where decisions are made, representing a major breakthrough for emissions accounting and mitigation."}
{"id": "2511.19279", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.19279", "abs": "https://arxiv.org/abs/2511.19279", "authors": ["Victor Rambaud", "Salvador Mascarenhas", "Yair Lakretz"], "title": "MapFormer: Self-Supervised Learning of Cognitive Maps with Input-Dependent Positional Embeddings", "comment": "19 pages (29 with appendix), 8 figures", "summary": "A cognitive map is an internal model which encodes the abstract relationships among entities in the world, giving humans and animals the flexibility to adapt to new situations, with a strong out-of-distribution (OOD) generalization that current AI systems still do not possess. To bridge this gap, we introduce MapFormers, new architectures based on Transformer models, which can learn cognitive maps from observational data and perform path integration in parallel, in a self-supervised manner. Cognitive maps are learned in the model by disentangling structural relationships in the inputs from their specific content, a property that can be achieved naturally by updating the positional encoding in Transformers with input-dependent matrices. We developed two variants of MapFormers that unify absolute and relative positional encoding to model episodic (EM) and working memory (WM), respectively. We tested MapFormers on several tasks, including a classic 2D navigation task, showing that our models can learn a cognitive map of the underlying space and generalize OOD (e.g., to longer sequences) with near-perfect performance, unlike current architectures. Together, these results demonstrate the superiority of models designed to learn a cognitive map, and the importance of introducing a structural bias for structure-content disentanglement, which can be achieved in Transformers with input-dependent positional encoding. MapFormers have broad applications in both neuroscience and AI, by explaining the neural mechanisms giving rise to cognitive maps, while allowing these relation models to be learned at scale."}
{"id": "2511.19299", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19299", "abs": "https://arxiv.org/abs/2511.19299", "authors": ["James R. M. Black", "Moritz S. Hanke", "Aaron Maiwald", "Tina Hernandez-Boussard", "Oliver M. Crook", "Jaspreet Pannu"], "title": "Open-weight genome language model safeguards: Assessing robustness via adversarial fine-tuning", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: Biosecurity Safeguards for Generative AI", "summary": "Novel deep learning architectures are increasingly being applied to biological data, including genetic sequences. These models, referred to as genomic language mod- els (gLMs), have demonstrated impressive predictive and generative capabilities, raising concerns that such models may also enable misuse, for instance via the generation of genomes for human-infecting viruses. These concerns have catalyzed calls for risk mitigation measures. The de facto mitigation of choice is filtering of pretraining data (i.e., removing viral genomic sequences from training datasets) in order to limit gLM performance on virus-related tasks. However, it is not currently known how robust this approach is for securing open-source models that can be fine-tuned using sensitive pathogen data. Here, we evaluate a state-of-the-art gLM, Evo 2, and perform fine-tuning using sequences from 110 harmful human-infecting viruses to assess the rescue of misuse-relevant predictive capabilities. The fine- tuned model exhibited reduced perplexity on unseen viral sequences relative to 1) the pretrained model and 2) a version fine-tuned on bacteriophage sequences. The model fine-tuned on human-infecting viruses also identified immune escape variants from SARS-CoV-2 (achieving an AUROC of 0.6), despite having no expo- sure to SARS-CoV-2 sequences during fine-tuning. This work demonstrates that data exclusion might be circumvented by fine-tuning approaches that can, to some degree, rescue misuse-relevant capabilities of gLMs. We highlight the need for safety frameworks for gLMs and outline further work needed on evaluations and mitigation measures to enable the safe deployment of gLMs."}
{"id": "2511.19328", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19328", "abs": "https://arxiv.org/abs/2511.19328", "authors": ["Rohan Saha", "Farzane Aminmansour", "Alona Fyshe"], "title": "Understanding the Staged Dynamics of Transformers in Learning Latent Structure", "comment": "Preprint", "summary": "While transformers can discover latent structure from context, the dynamics of how they acquire different components of the latent structure remain poorly understood. In this work, we use the Alchemy benchmark, to investigate the dynamics of latent structure learning. We train a small decoder-only transformer on three task variants: 1) inferring missing rules from partial contextual information, 2) composing simple rules to solve multi-step sequences, and 3) decomposing complex multi-step examples to infer intermediate steps. By factorizing each task into interpretable events, we show that the model acquires capabilities in discrete stages, first learning the coarse grained rules, before learning the complete latent structure. We also identify a crucial asymmetry, where the model can compose fundamental rules robustly, but struggles to decompose complex examples to discover the fundamental rules. These findings offer new insights into understanding how a transformer model learns latent structures, providing a granular view of how these capabilities evolve during training."}
{"id": "2511.19330", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19330", "abs": "https://arxiv.org/abs/2511.19330", "authors": ["Dominik Luszczynski"], "title": "Targeted Manipulation: Slope-Based Attacks on Financial Time-Series Data", "comment": "13 pages, 6 figures, 4 tables, preprint; Total including Appendix: 21 pages, 11 figures, 7 tables", "summary": "A common method of attacking deep learning models is through adversarial attacks, which occur when an attacker specifically modifies the input of a model to produce an incorrect result. Adversarial attacks have been deeply investigated in the image domain; however, there is less research in the time-series domain and very little for forecasting financial data. To address these concerns, this study aims to build upon previous research on adversarial attacks for time-series data by introducing two new slope-based methods aimed to alter the trends of the predicted stock forecast generated by an N-HiTS model. Compared to the normal N-HiTS predictions, the two new slope-based methods, the General Slope Attack and Least-Squares Slope Attack, can manipulate N-HiTS predictions by doubling the slope. These new slope attacks can bypass standard security mechanisms, such as a discriminator that filters real and perturbed inputs, reducing a 4-layered CNN's specificity to 28% and accuracy to 57%. Furthermore, the slope based methods were incorporated into a GAN architecture as a means of generating realistic synthetic data, while simultaneously fooling the model. Finally, this paper also proposes a sample malware designed to inject an adversarial attack in the model inference library, proving that ML-security research should not only focus on making the model safe, but also securing the entire pipeline."}
{"id": "2511.19344", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19344", "abs": "https://arxiv.org/abs/2511.19344", "authors": ["Hari Chandana Kuchibhotla", "K S Ananth", "Vineeth N Balasubramanian"], "title": "Annotation-Free Class-Incremental Learning", "comment": "18 pages, 6 figures", "summary": "Despite significant progress in continual learning ranging from architectural novelty to clever strategies for mitigating catastrophic forgetting most existing methods rest on a strong but unrealistic assumption the availability of labeled data throughout the learning process. In real-world scenarios, however, data often arrives sequentially and without annotations, rendering conventional approaches impractical. In this work, we revisit the fundamental assumptions of continual learning and ask: Can current systems adapt when labels are absent and tasks emerge incrementally over time? To this end, we introduce Annotation-Free Class-Incremental Learning (AFCIL), a more realistic and challenging paradigm where unlabeled data arrives continuously, and the learner must incrementally acquire new classes without any supervision. To enable effective learning under AFCIL, we propose CrossWorld CL, a Cross Domain World Guided Continual Learning framework that incorporates external world knowledge as a stable auxiliary source. The method retrieves semantically related ImageNet classes for each downstream category, maps downstream and ImageNet features through a cross domain alignment strategy and finally introduce a novel replay strategy. This design lets the model uncover semantic structure without annotations while keeping earlier knowledge intact. Across four datasets, CrossWorld-CL surpasses CLIP baselines and existing continual and unlabeled learning methods, underscoring the benefit of world knowledge for annotation free continual learning."}
{"id": "2511.19350", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.19350", "abs": "https://arxiv.org/abs/2511.19350", "authors": ["Nikita Neveditsin", "Pawan Lingras", "Vijay Mago"], "title": "Scalable Parameter-Light Spectral Method for Clustering Short Text Embeddings with a Cohesion-Based Evaluation Metric", "comment": null, "summary": "Clustering short text embeddings is a foundational task in natural language processing, yet remains challenging due to the need to specify the number of clusters in advance. We introduce a scalable spectral method that estimates the number of clusters directly from the structure of the Laplacian eigenspectrum, constructed using cosine similarities and guided by an adaptive sampling strategy. This sampling approach enables our estimator to efficiently scale to large datasets without sacrificing reliability. To support intrinsic evaluation of cluster quality without ground-truth labels, we propose the Cohesion Ratio, a simple and interpretable evaluation metric that quantifies how much intra-cluster similarity exceeds the global similarity background. It has an information-theoretic motivation inspired by mutual information, and in our experiments it correlates closely with extrinsic measures such as normalized mutual information and homogeneity. Extensive experiments on six short-text datasets and four modern embedding models show that standard algorithms like K-Means and HAC, when guided by our estimator, significantly outperform popular parameter-light methods such as HDBSCAN, OPTICS, and Leiden. These results demonstrate the practical value of our spectral estimator and Cohesion Ratio for unsupervised organization and evaluation of short text data. Implementation of our estimator of k and Cohesion Ratio, along with code for reproducing the experiments, is available at https://anonymous.4open.science/r/towards_clustering-0C2E."}
{"id": "2511.19355", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19355", "abs": "https://arxiv.org/abs/2511.19355", "authors": ["Franklin Cardenoso", "Wouter Caarls"], "title": "Leveraging LLMs for reward function design in reinforcement learning control tasks", "comment": null, "summary": "The challenge of designing effective reward functions in reinforcement learning (RL) represents a significant bottleneck, often requiring extensive human expertise and being time-consuming. Previous work and recent advancements in large language models (LLMs) have demonstrated their potential for automating the generation of reward functions. However, existing methodologies often require preliminary evaluation metrics, human-engineered feedback for the refinement process, or the use of environmental source code as context. To address these limitations, this paper introduces LEARN-Opt (LLM-based Evaluator and Analyzer for Reward functioN Optimization). This LLM-based, fully autonomous, and model-agnostic framework eliminates the need for preliminary metrics and environmental source code as context to generate, execute, and evaluate reward function candidates from textual descriptions of systems and task objectives. LEARN-Opt's main contribution lies in its ability to autonomously derive performance metrics directly from the system description and the task objective, enabling unsupervised evaluation and selection of reward functions. Our experiments indicate that LEARN-Opt achieves performance comparable to or better to that of state-of-the-art methods, such as EUREKA, while requiring less prior knowledge. We find that automated reward design is a high-variance problem, where the average-case candidate fails, requiring a multi-run approach to find the best candidates. Finally, we show that LEARN-Opt can unlock the potential of low-cost LLMs to find high-performing candidates that are comparable to, or even better than, those of larger models. This demonstrated performance affirms its potential to generate high-quality reward functions without requiring any preliminary human-defined metrics, thereby reducing engineering overhead and enhancing generalizability."}
{"id": "2511.19359", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19359", "abs": "https://arxiv.org/abs/2511.19359", "authors": ["Ariel Fargion", "Lahav Dabah", "Tom Tirer"], "title": "Enhancing Conformal Prediction via Class Similarity", "comment": null, "summary": "Conformal Prediction (CP) has emerged as a powerful statistical framework for high-stakes classification applications. Instead of predicting a single class, CP generates a prediction set, guaranteed to include the true label with a pre-specified probability. The performance of different CP methods is typically assessed by their average prediction set size. In setups where the classes can be partitioned into semantic groups, e.g., diseases that require similar treatment, users can benefit from prediction sets that are not only small on average, but also contain a small number of semantically different groups. This paper begins by addressing this problem and ultimately offers a widely applicable tool for boosting any CP method on any dataset. First, given a class partition, we propose augmenting the CP score function with a term that penalizes predictions with out-of-group errors. We theoretically analyze this strategy and prove its advantages for group-related metrics. Surprisingly, we show mathematically that, for common class partitions, it can also reduce the average set size of any CP score function. Our analysis reveals the class similarity factors behind this improvement and motivates us to propose a model-specific variant, which does not require any human semantic partition and can further reduce the prediction set size. Finally, we present an extensive empirical study, encompassing prominent CP methods, multiple models, and several datasets, which demonstrates that our class-similarity-based approach consistently enhances CP methods."}
{"id": "2511.19364", "categories": ["cs.LG", "astro-ph.IM", "gr-qc", "quant-ph"], "pdf": "https://arxiv.org/pdf/2511.19364", "abs": "https://arxiv.org/abs/2511.19364", "authors": ["Carlos Ruiz-Gonzalez", "Sören Arlt", "Sebastian Lehner", "Arturs Berzins", "Yehonathan Drori", "Rana X Adhikari", "Johannes Brandstetter", "Mario Krenn"], "title": "Neural surrogates for designing gravitational wave detectors", "comment": "20 pages, 7 figures, 4 tables", "summary": "Physics simulators are essential in science and engineering, enabling the analysis, control, and design of complex systems. In experimental sciences, they are increasingly used to automate experimental design, often via combinatorial search and optimization. However, as the setups grow more complex, the computational cost of traditional, CPU-based simulators becomes a major limitation. Here, we show how neural surrogate models can significantly reduce reliance on such slow simulators while preserving accuracy. Taking the design of interferometric gravitational wave detectors as a representative example, we train a neural network to surrogate the gravitational wave physics simulator Finesse, which was developed by the LIGO community. Despite that small changes in physical parameters can change the output by orders of magnitudes, the model rapidly predicts the quality and feasibility of candidate designs, allowing an efficient exploration of large design spaces. Our algorithm loops between training the surrogate, inverse designing new experiments, and verifying their properties with the slow simulator for further training. Assisted by auto-differentiation and GPU parallelism, our method proposes high-quality experiments much faster than direct optimization. Solutions that our algorithm finds within hours outperform designs that take five days for the optimizer to reach. Though shown in the context of gravitational wave detectors, our framework is broadly applicable to other domains where simulator bottlenecks hinder optimization and discovery."}
{"id": "2511.19368", "categories": ["cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.19368", "abs": "https://arxiv.org/abs/2511.19368", "authors": ["Tianyang Duan", "Zongyuan Zhang", "Zheng Lin", "Songxiao Guo", "Xiuxian Guan", "Guangyu Wu", "Zihan Fang", "Haotian Meng", "Xia Du", "Ji-Zhe Zhou", "Heming Cui", "Jun Luo", "Yue Gao"], "title": "LLM-Driven Stationarity-Aware Expert Demonstrations for Multi-Agent Reinforcement Learning in Mobile Systems", "comment": "15 pages, 9 figures", "summary": "Multi-agent reinforcement learning (MARL) has been increasingly adopted in many real-world applications. While MARL enables decentralized deployment on resource-constrained edge devices, it suffers from severe non-stationarity due to the synchronous updates of agent policies. This non stationarity results in unstable training and poor policy con vergence, especially as the number of agents increases. In this paper, we propose RELED, a scalable MARL framework that integrates large language model (LLM)-driven expert demonstrations with autonomous agent exploration. RELED incorporates a Stationarity-Aware Expert Demonstration module, which leverages theoretical non-stationarity bounds to enhance the quality of LLM-generated expert trajectories, thus providing high reward and training-stable samples for each agent. Moreover, a Hybrid Expert-Agent Policy Optimization module adaptively balances each agent's learning from both expert-generated and agent-generated trajectories, accelerating policy convergence and improving generalization. Extensive experiments with real city networks based on OpenStreetMap demonstrate that RELED achieves superior performance compared to state-of-the-art MARL methods."}
{"id": "2511.19379", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19379", "abs": "https://arxiv.org/abs/2511.19379", "authors": ["Srishti Gupta", "Yashasvee Taiwade"], "title": "Efficiency vs. Fidelity: A Comparative Analysis of Diffusion Probabilistic Models and Flow Matching on Low-Resource Hardware", "comment": null, "summary": "Denoising Diffusion Probabilistic Models (DDPMs) have established a new state-of-the-art in generative image synthesis, yet their deployment is hindered by significant computational overhead during inference, often requiring up to 1,000 iterative steps. This study presents a rigorous comparative analysis of DDPMs against the emerging Flow Matching (Rectified Flow) paradigm, specifically isolating their geometric and efficiency properties on low-resource hardware. By implementing both frameworks on a shared Time-Conditioned U-Net backbone using the MNIST dataset, we demonstrate that Flow Matching significantly outperforms Diffusion in efficiency. Our geometric analysis reveals that Flow Matching learns a highly rectified transport path (Curvature $\\mathcal{C} \\approx 1.02$), which is near-optimal, whereas Diffusion trajectories remain stochastic and tortuous ($\\mathcal{C} \\approx 3.45$). Furthermore, we establish an ``efficiency frontier'' at $N=10$ function evaluations, where Flow Matching retains high fidelity while Diffusion collapses. Finally, we show via numerical sensitivity analysis that the learned vector field is sufficiently linear to render high-order ODE solvers (Runge-Kutta 4) unnecessary, validating the use of lightweight Euler solvers for edge deployment. \\textbf{This work concludes that Flow Matching is the superior algorithmic choice for real-time, resource-constrained generative tasks.}"}
{"id": "2511.19390", "categories": ["cs.LG", "astro-ph.SR", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.19390", "abs": "https://arxiv.org/abs/2511.19390", "authors": ["Rudy Morel", "Francesco Pio Ramunno", "Jeff Shen", "Alberto Bietti", "Kyunghyun Cho", "Miles Cranmer", "Siavash Golkar", "Olexandr Gugnin", "Geraud Krawezik", "Tanya Marwah", "Michael McCabe", "Lucas Meyer", "Payel Mukhopadhyay", "Ruben Ohana", "Liam Parker", "Helen Qu", "François Rozet", "K. D. Leka", "François Lanusse", "David Fouhey", "Shirley Ho"], "title": "Predicting partially observable dynamical systems via diffusion models with a multiscale inference scheme", "comment": null, "summary": "Conditional diffusion models provide a natural framework for probabilistic prediction of dynamical systems and have been successfully applied to fluid dynamics and weather prediction. However, in many settings, the available information at a given time represents only a small fraction of what is needed to predict future states, either due to measurement uncertainty or because only a small fraction of the state can be observed. This is true for example in solar physics, where we can observe the Sun's surface and atmosphere, but its evolution is driven by internal processes for which we lack direct measurements. In this paper, we tackle the probabilistic prediction of partially observable, long-memory dynamical systems, with applications to solar dynamics and the evolution of active regions. We show that standard inference schemes, such as autoregressive rollouts, fail to capture long-range dependencies in the data, largely because they do not integrate past information effectively. To overcome this, we propose a multiscale inference scheme for diffusion models, tailored to physical processes. Our method generates trajectories that are temporally fine-grained near the present and coarser as we move farther away, which enables capturing long-range temporal dependencies without increasing computational cost. When integrated into a diffusion model, we show that our inference scheme significantly reduces the bias of the predicted distributions and improves rollout stability."}
{"id": "2511.19405", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19405", "abs": "https://arxiv.org/abs/2511.19405", "authors": ["Dereck Piche", "Mohammed Muqeeth", "Milad Aghajohari", "Juan Duque", "Michael Noukhovitch", "Aaron Courville"], "title": "Learning Robust Social Strategies with Large Language Models", "comment": null, "summary": "As agentic AI becomes more widespread, agents with distinct and possibly conflicting goals will interact in complex ways. These multi-agent interactions pose a fundamental challenge, particularly in social dilemmas, where agents' individual incentives can undermine collective welfare. While reinforcement learning (RL) has been effective for aligning large language models (LLMs) in the single-agent regime, prior small-network results suggest that standard RL in multi-agent settings often converges to defecting, self-interested policies. We show the same effect in LLMs: despite cooperative priors, RL-trained LLM agents develop opportunistic behavior that can exploit even advanced closed-source models. To address this tendency of RL to converge to poor equilibria, we adapt a recent opponent-learning awareness algorithm, Advantage Alignment, to fine-tune LLMs toward multi-agent cooperation and non-exploitability. We then introduce a group-relative baseline that simplifies advantage computation in iterated games, enabling multi-agent training at LLM scale. We also contribute a novel social dilemma environment, Trust and Split, which requires natural language communication to achieve high collective welfare. Across a wide range of social dilemmas, policies learned with Advantage Alignment achieve higher collective payoffs while remaining robust against exploitation by greedy agents."}
{"id": "2511.19413", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19413", "abs": "https://arxiv.org/abs/2511.19413", "authors": ["Zhaolong Su", "Wang Lu", "Hao Chen", "Sharon Li", "Jindong Wang"], "title": "UniGame: Turning a Unified Multimodal Model Into Its Own Adversary", "comment": null, "summary": "Unified Multimodal Models (UMMs) have shown impressive performance in both understanding and generation with a single architecture. However, UMMs still exhibit a fundamental inconsistency: understanding favors compact embeddings, whereas generation favors reconstruction-rich representations. This structural trade-off produces misaligned decision boundaries, degraded cross-modal coherence, and heightened vulnerability under distributional and adversarial shifts. In this paper, we present UniGame, a self-adversarial post-training framework that directly targets the inconsistencies. By applying a lightweight perturber at the shared token interface, UniGame enables the generation branch to actively seek and challenge fragile understanding, turning the model itself into its own adversary. Experiments demonstrate that UniGame significantly improves the consistency (+4.6%). Moreover, it also achieves substantial improvements in understanding (+3.6%), generation (+0.02), out-of-distribution and adversarial robustness (+4.8% and +6.2% on NaturalBench and AdVQA). The framework is architecture-agnostic, introduces less than 1% additional parameters, and is complementary to existing post-training methods. These results position adversarial self-play as a general and effective principle for enhancing the coherence, stability, and unified competence of future multimodal foundation models. The official code is available at: https://github.com/AIFrontierLab/UniGame"}
{"id": "2511.19428", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19428", "abs": "https://arxiv.org/abs/2511.19428", "authors": ["Shangyuan Tong", "Nanye Ma", "Saining Xie", "Tommi Jaakkola"], "title": "Flow Map Distillation Without Data", "comment": null, "summary": "State-of-the-art flow models achieve remarkable quality but require slow, iterative sampling. To accelerate this, flow maps can be distilled from pre-trained teachers, a procedure that conventionally requires sampling from an external dataset. We argue that this data-dependency introduces a fundamental risk of Teacher-Data Mismatch, as a static dataset may provide an incomplete or even misaligned representation of the teacher's full generative capabilities. This leads us to question whether this reliance on data is truly necessary for successful flow map distillation. In this work, we explore a data-free alternative that samples only from the prior distribution, a distribution the teacher is guaranteed to follow by construction, thereby circumventing the mismatch risk entirely. To demonstrate the practical viability of this philosophy, we introduce a principled framework that learns to predict the teacher's sampling path while actively correcting for its own compounding errors to ensure high fidelity. Our approach surpasses all data-based counterparts and establishes a new state-of-the-art by a significant margin. Specifically, distilling from SiT-XL/2+REPA, our method reaches an impressive FID of 1.45 on ImageNet 256x256, and 1.49 on ImageNet 512x512, both with only 1 sampling step. We hope our work establishes a more robust paradigm for accelerating generative models and motivates the broader adoption of flow map distillation without data."}
{"id": "2511.17643", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17643", "abs": "https://arxiv.org/abs/2511.17643", "authors": ["Yayan Qiu", "Sean Hanna"], "title": "Fluid Grey 2: How Well Does Generative Adversarial Network Learn Deeper Topology Structure in Architecture That Matches Images?", "comment": null, "summary": "Taking into account the regional characteristics of intrinsic and extrinsic properties of space is an essential issue in architectural design and urban renewal, which is often achieved step by step using image and graph-based GANs. However, each model nesting and data conversion may cause information loss, and it is necessary to streamline the tools to facilitate architects and users to participate in the design. Therefore, this study hopes to prove that I2I GAN also has the potential to recognize topological relationships autonomously. Therefore, this research proposes a method for quickly detecting the ability of pix2pix to learn topological relationships, which is achieved by adding two Grasshopper-based detection modules before and after GAN. At the same time, quantitative data is provided and its learning process is visualized, and changes in different input modes such as greyscale and RGB affect its learning efficiency. There are two innovations in this paper: 1) It proves that pix2pix can automatically learn spatial topological relationships and apply them to architectural design. 2) It fills the gap in detecting the performance of Image-based Generation GAN from a topological perspective. Moreover, the detection method proposed in this study takes a short time and is simple to operate. The two detection modules can be widely used for customizing image datasets with the same topological structure and for batch detection of topological relationships of images. In the future, this paper may provide a theoretical foundation and data support for the application of architectural design and urban renewal that use GAN to preserve spatial topological characteristics."}
{"id": "2511.17939", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17939", "abs": "https://arxiv.org/abs/2511.17939", "authors": ["Yuchen Ying", "Yiyang Dai", "Wenda Li", "Wenjie Huang", "Rui Wang", "Tongya Zheng", "Yu Wang", "Hanyang Yuan", "Mingli Song"], "title": "Neural Graph Navigation for Intelligent Subgraph Matching", "comment": "Under review at AAAI 2026", "summary": "Subgraph matching, a cornerstone of relational pattern detection in domains ranging from biochemical systems to social network analysis, faces significant computational challenges due to the dramatically growing search space. Existing methods address this problem within a filtering-ordering-enumeration framework, in which the enumeration stage recursively matches the query graph against the candidate subgraphs of the data graph. However, the lack of awareness of subgraph structural patterns leads to a costly brute-force enumeration, thereby critically motivating the need for intelligent navigation in subgraph matching. To address this challenge, we propose Neural Graph Navigation (NeuGN), a neuro-heuristic framework that transforms brute-force enumeration into neural-guided search by integrating neural navigation mechanisms into the core enumeration process. By preserving heuristic-based completeness guarantees while incorporating neural intelligence, NeuGN significantly reduces the \\textit{First Match Steps} by up to 98.2\\% compared to state-of-the-art methods across six real-world datasets."}
{"id": "2511.17959", "categories": ["cs.CR", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17959", "abs": "https://arxiv.org/abs/2511.17959", "authors": ["Yuhao Wu", "Ke Yang", "Franziska Roesner", "Tadayoshi Kohno", "Ning Zhang", "Umar Iqbal"], "title": "Towards Automating Data Access Permissions in AI Agents", "comment": "Accepted by the IEEE Symposium on Security and Privacy (S&P) 2026", "summary": "As AI agents attempt to autonomously act on users' behalf, they raise transparency and control issues. We argue that permission-based access control is indispensable in providing meaningful control to the users, but conventional permission models are inadequate for the automated agentic execution paradigm. We therefore propose automated permission management for AI agents. Our key idea is to conduct a user study to identify the factors influencing users' permission decisions and to encode these factors into an ML-based permission management assistant capable of predicting users' future decisions. We find that participants' permission decisions are influenced by communication context but importantly individual preferences tend to remain consistent within contexts, and align with those of other participants. Leveraging these insights, we develop a permission prediction model achieving 85.1% accuracy overall and 94.4% for high-confidence predictions. We find that even without using permission history, our model achieves an accuracy of 66.9%, and a slight increase of training samples (i.e., 1-4) can substantially increase the accuracy by 10.8%."}
{"id": "2511.17977", "categories": ["cs.SE", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.17977", "abs": "https://arxiv.org/abs/2511.17977", "authors": ["Kuangxiangzi Liu", "Dhiman Chakraborty", "Alexander Liggesmeyer", "Andreas Zeller"], "title": "Synthesizing Precise Protocol Specs from Natural Language for Effective Test Generation", "comment": null, "summary": "Safety- and security-critical systems have to be thoroughly tested against their specifications. The state of practice is to have _natural language_ specifications, from which test cases are derived manually - a process that is slow, error-prone, and difficult to scale. _Formal_ specifications, on the other hand, are well-suited for automated test generation, but are tedious to write and maintain. In this work, we propose a two-stage pipeline that uses large language models (LLMs) to bridge the gap: First, we extract _protocol elements_ from natural-language specifications; second, leveraging a protocol implementation, we synthesize and refine a formal _protocol specification_ from these elements, which we can then use to massively test further implementations.\n  We see this two-stage approach to be superior to end-to-end LLM-based test generation, as 1. it produces an _inspectable specification_ that preserves traceability to the original text; 2. the generation of actual test cases _no longer requires an LLM_; 3. the resulting formal specs are _human-readable_, and can be reviewed, version-controlled, and incrementally refined; and 4. over time, we can build a _corpus_ of natural-language-to-formal-specification mappings that can be used to further train and refine LLMs for more automatic translations.\n  Our prototype, AUTOSPEC, successfully demonstrated the feasibility of our approach on five widely used _internet protocols_ (SMTP, POP3, IMAP, FTP, and ManageSieve) by applying its methods on their _RFC specifications_ written in natural-language, and the recent _I/O grammar_ formalism for protocol specification and fuzzing. In its evaluation, AUTOSPEC recovers on average 92.8% of client and 80.2% of server message types, and achieves 81.5% message acceptance across diverse, real-world systems."}
{"id": "2511.18025", "categories": ["cs.CR", "cs.IT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18025", "abs": "https://arxiv.org/abs/2511.18025", "authors": ["Yifan Luo", "Meng Zhang", "Jin Xu", "Junting Chen", "Jianwei Huang"], "title": "Correlated-Sequence Differential Privacy", "comment": "11 pages, 5 figures. Published in 2025 34th International Conference on Computer Communications and Networks (ICCCN), IEEE, August 2025", "summary": "Data streams collected from multiple sources are rarely independent. Values evolve over time and influence one another across sequences. These correlations improve prediction in healthcare, finance, and smart-city control yet violate the record-independence assumption built into most Differential Privacy (DP) mechanisms. To restore rigorous privacy guarantees without sacrificing utility, we introduce Correlated-Sequence Differential Privacy (CSDP), a framework specifically designed for preserving privacy in correlated sequential data. CSDP addresses two linked challenges: quantifying the extra information an attacker gains from joint temporal and cross-sequence links, and adding just enough noise to hide that information while keeping the data useful. We model multivariate streams as a Coupling Markov Chain, yielding the derived loose leakage bound expressed with a few spectral terms and revealing a counterintuitive result: stronger coupling can actually decrease worst-case leakage by dispersing perturbations across sequences. Guided by these bounds, we build the Freshness-Regulated Adaptive Noise (FRAN) mechanism--combining data aging, correlation-aware sensitivity scaling, and Laplace noise--that runs in linear time. Tests on two-sequence datasets show that CSDP improves the privacy-utility trade-off by approximately 50% over existing correlated-DP methods and by two orders of magnitude compared to the standard DP approach."}
{"id": "2511.18038", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18038", "abs": "https://arxiv.org/abs/2511.18038", "authors": ["Xiaoke Han", "Hong Zhu"], "title": "MASTEST: A LLM-Based Multi-Agent System For RESTful API Tests", "comment": "14 Page of main text plus 4 pages of appendix", "summary": "Testing RESTful API is increasingly important in quality assurance of cloud-native applications. Recent advances in machine learning (ML) techniques have demonstrated that various testing activities can be performed automatically by large language models (LLMs) with reasonable accuracy. This paper develops a multi-agent system called MASTEST that combines LLM-based and programmed agents to form a complete tool chain that covers the whole workflow of API test starting from generating unit and system test scenarios from API specification in the OpenAPI Swagger format, to generating of Pytest test scripts, executing test scripts to interact with web services, to analysing web service response messages to determine test correctness and calculate test coverage. The system also supports the incorporation of human testers in reviewing and correcting LLM generated test artefacts to ensure the quality of testing activities. MASTEST system is evaluated on two LLMs, GPT-4o and DeepSeek V3.1 Reasoner with five public APIs. The performances of LLMs on various testing activities are measured by a wide range of metrics, including unit and system test scenario coverage and API operation coverage for the quality of generated test scenarios, data type correctness, status code coverage and script syntax correctness for the quality of LLM generated test scripts, as well as bug detection ability and usability of LLM generated test scenarios and scripts. Experiment results demonstrated that both DeepSeek and GPT-4o achieved a high overall performance. DeepSeek excels in data type correctness and status code detection, while GPT-4o performs best in API operation coverage. For both models, LLM generated test scripts maintained 100\\% syntax correctness and only required minimal manual edits for semantic correctness. These findings indicate the effectiveness and feasibility of MASTEST."}
{"id": "2511.18098", "categories": ["cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18098", "abs": "https://arxiv.org/abs/2511.18098", "authors": ["More Aayush Babasaheb", "Shamik Sural"], "title": "Towards Harnessing the Power of LLMs for ABAC Policy Mining", "comment": null, "summary": "This paper presents an empirical investigation into the capabilities of Large Language Models (LLMs) to perform automated Attribute-based Access Control (ABAC) policy mining. While ABAC provides fine-grained, context-aware access management, the increasing number and complexity of access policies can make their formulation and evaluation rather challenging. To address the task of synthesizing concise yet accurate policies, we evaluate the performance of some of the state-of-the-art LLMs, specifically Google Gemini (Flash and Pro) and OpenAI ChatGPT, as potential policy mining engines. An experimental framework was developed in Python to generate randomized access data parameterized by varying numbers of subjects, objects, and initial policy sets. The baseline policy sets, which govern permission decisions between subjects and objects, serve as the ground truth for comparison. Each LLM-generated policy was evaluated against the baseline policy using standard performance metrics. The results indicate that LLMs can effectively infer compact and valid ABAC policies for small-scale scenarios. However, as the system size increases, characterized by higher numbers of subjects and objects, LLM outputs exhibit declining accuracy and precision, coupled with significant increase in the size of policy generated, which is beyond the optimal size. These findings highlight both the promise and limitations of current LLM architectures for scalable policy mining in access control domains. Future work will explore hybrid approaches that combine prompt optimization with classical rule mining algorithms to improve scalability and interpretability in complex ABAC environments."}
{"id": "2511.18319", "categories": ["cs.AI", "cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.18319", "abs": "https://arxiv.org/abs/2511.18319", "authors": ["Xian Yeow Lee", "Lasitha Vidyaratne", "Gregory Sin", "Ahmed Farahat", "Chetan Gupta"], "title": "Weakly-supervised Latent Models for Task-specific Visual-Language Control", "comment": null, "summary": "Autonomous inspection in hazardous environments requires AI agents that can interpret high-level goals and execute precise control. A key capability for such agents is spatial grounding, for example when a drone must center a detected object in its camera view to enable reliable inspection. While large language models provide a natural interface for specifying goals, using them directly for visual control achieves only 58\\% success in this task. We envision that equipping agents with a world model as a tool would allow them to roll out candidate actions and perform better in spatially grounded settings, but conventional world models are data and compute intensive. To address this, we propose a task-specific latent dynamics model that learns state-specific action-induced shifts in a shared latent space using only goal-state supervision. The model leverages global action embeddings and complementary training losses to stabilize learning. In experiments, our approach achieves 71\\% success and generalizes to unseen images and instructions, highlighting the potential of compact, domain-specific latent dynamics models for spatial alignment in autonomous inspection."}
{"id": "2511.18364", "categories": ["cs.AI", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18364", "abs": "https://arxiv.org/abs/2511.18364", "authors": ["Marvin Hofer", "Erhard Rahm"], "title": "KGpipe: Generation and Evaluation of Pipelines for Data Integration into Knowledge Graphs", "comment": "15 KG pipelines (9 single source, 6 multi source)", "summary": "Building high-quality knowledge graphs (KGs) from diverse sources requires combining methods for information extraction, data transformation, ontology mapping, entity matching, and data fusion. Numerous methods and tools exist for each of these tasks, but support for combining them into reproducible and effective end-to-end pipelines is still lacking. We present a new framework, KGpipe for defining and executing integration pipelines that can combine existing tools or LLM (Large Language Model) functionality. To evaluate different pipelines and the resulting KGs, we propose a benchmark to integrate heterogeneous data of different formats (RDF, JSON, text) into a seed KG. We demonstrate the flexibility of KGpipe by running and comparatively evaluating several pipelines integrating sources of the same or different formats using selected performance and quality metrics."}
{"id": "2511.18517", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18517", "abs": "https://arxiv.org/abs/2511.18517", "authors": ["Khanh Gia Bui"], "title": "Foundations of Artificial Intelligence Frameworks: Notion and Limits of AGI", "comment": "49 pages, 4 pictures", "summary": "Within the limited scope of this paper, we argue that artificial general intelligence cannot emerge from current neural network paradigms regardless of scale, nor is such an approach healthy for the field at present. Drawing on various notions, discussions, present-day developments and observations, current debates and critiques, experiments, and so on in between philosophy, including the Chinese Room Argument and Gödelian argument, neuroscientific ideas, computer science, the theoretical consideration of artificial intelligence, and learning theory, we address conceptually that neural networks are architecturally insufficient for genuine understanding. They operate as static function approximators of a limited encoding framework - a 'sophisticated sponge' exhibiting complex behaviours without structural richness that constitute intelligence. We critique the theoretical foundations the field relies on and created of recent times; for example, an interesting heuristic as neural scaling law (as an example, arXiv:2001.08361 ) made prominent in a wrong way of interpretation, The Universal Approximation Theorem addresses the wrong level of abstraction and, in parts, partially, the question of current architectures lacking dynamic restructuring capabilities. We propose a framework distinguishing existential facilities (computational substrate) from architectural organization (interpretive structures), and outline principles for what genuine machine intelligence would require, and furthermore, a conceptual method of structuralizing the richer framework on which the principle of neural network system takes hold."}
{"id": "2511.18633", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18633", "abs": "https://arxiv.org/abs/2511.18633", "authors": ["Yildiz Culcu"], "title": "Bridging Philosophy and Machine Learning: A Structuralist Framework for Classifying Neural Network Representations", "comment": "7 pages, 1 figure, 1 table. Developed from the author's bachelor thesis but substantially revised and reformulated for research publication", "summary": "Machine learning models increasingly function as representational systems, yet the philosoph- ical assumptions underlying their internal structures remain largely unexamined. This paper develops a structuralist decision framework for classifying the implicit ontological commitments made in machine learning research on neural network representations. Using a modified PRISMA protocol, a systematic review of the last two decades of literature on representation learning and interpretability is conducted. Five influential papers are analysed through three hierarchical criteria derived from structuralist philosophy of science: entity elimination, source of structure, and mode of existence. The results reveal a pronounced tendency toward structural idealism, where learned representations are treated as model-dependent constructions shaped by architec- ture, data priors, and training dynamics. Eliminative and non-eliminative structuralist stances appear selectively, while structural realism is notably absent. The proposed framework clarifies conceptual tensions in debates on interpretability, emergence, and epistemic trust in machine learning, and offers a rigorous foundation for future interdisciplinary work between philosophy of science and machine learning."}
{"id": "2511.18653", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18653", "abs": "https://arxiv.org/abs/2511.18653", "authors": ["Nuo Xu", "Zhaoting Gong", "Ran Ran", "Jinwei Tang", "Wujie Wen", "Caiwen Ding"], "title": "FHE-Agent: Automating CKKS Configuration for Practical Encrypted Inference via an LLM-Guided Agentic Framework", "comment": null, "summary": "Fully Homomorphic Encryption (FHE), particularly the CKKS scheme, is a promising enabler for privacy-preserving MLaaS, but its practical deployment faces a prohibitive barrier: it heavily relies on domain expertise. Configuring CKKS involves a tightly coupled space of ring dimensions, modulus chains, and packing layouts. Without deep cryptographic knowledge to navigate these interactions, practitioners are restricted to compilers that rely on fixed heuristics. These \"one-shot\" tools often emit rigid configurations that are either severely over-provisioned in latency or fail to find a feasible solution entirely for deeper networks.\n  We present FHE-Agent, an agentic framework that automates this expert reasoning process. By coupling a Large Language Model (LLM) controller with a deterministic tool suite, FHE-Agent decomposes the search into global parameter selection and layer-wise bottleneck repair. The agents operate within a multi-fidelity workflow, pruning invalid regimes using cheap static analysis and reserving expensive encrypted evaluations for the most promising candidates.\n  We instantiate FHE-Agent on the Orion compiler and evaluate it on standard benchmarks (MLP, LeNet, LoLa) and deeper architectures (AlexNet). FHE-Agent consistently achieves better precision and lower latency than naïve search strategies. Crucially, it automatically discovers feasible, 128-bit secure configurations for complex models where baseline heuristics and one-shot prompts fail to produce a valid setup."}
{"id": "2511.18739", "categories": ["cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.18739", "abs": "https://arxiv.org/abs/2511.18739", "authors": ["Kaixiang Yang", "Jiarong Liu", "Yupeng Song", "Shuanghua Yang", "Yujue Zhou"], "title": "A Problem-Oriented Taxonomy of Evaluation Metrics for Time Series Anomaly Detection", "comment": null, "summary": "Time series anomaly detection is widely used in IoT and cyber-physical systems, yet its evaluation remains challenging due to diverse application objectives and heterogeneous metric assumptions. This study introduces a problem-oriented framework that reinterprets existing metrics based on the specific evaluation challenges they are designed to address, rather than their mathematical forms or output structures. We categorize over twenty commonly used metrics into six dimensions: 1) basic accuracy-driven evaluation; 2) timeliness-aware reward mechanisms; 3) tolerance to labeling imprecision; 4) penalties reflecting human-audit cost; 5) robustness against random or inflated scores; and 6) parameter-free comparability for cross-dataset benchmarking. Comprehensive experiments are conducted to examine metric behavior under genuine, random, and oracle detection scenarios. By comparing their resulting score distributions, we quantify each metric's discriminative ability -- its capability to distinguish meaningful detections from random noise. The results show that while most event-level metrics exhibit strong separability, several widely used metrics (e.g., NAB, Point-Adjust) demonstrate limited resistance to random-score inflation. These findings reveal that metric suitability must be inherently task-dependent and aligned with the operational objectives of IoT applications. The proposed framework offers a unified analytical perspective for understanding existing metrics and provides practical guidance for selecting or developing more context-aware, robust, and fair evaluation methodologies for time series anomaly detection."}
{"id": "2511.18793", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18793", "abs": "https://arxiv.org/abs/2511.18793", "authors": ["Yejing Wang", "Shengyu Zhou", "Jinyu Lu", "Ziwei Liu", "Langming Liu", "Maolin Wang", "Wenlin Zhang", "Feng Li", "Wenbo Su", "Pengjie Wang", "Jian Xu", "Xiangyu Zhao"], "title": "NEZHA: A Zero-sacrifice and Hyperspeed Decoding Architecture for Generative Recommendations", "comment": null, "summary": "Generative Recommendation (GR), powered by Large Language Models (LLMs), represents a promising new paradigm for industrial recommender systems. However, their practical application is severely hindered by high inference latency, which makes them infeasible for high-throughput, real-time services and limits their overall business impact. While Speculative Decoding (SD) has been proposed to accelerate the autoregressive generation process, existing implementations introduce new bottlenecks: they typically require separate draft models and model-based verifiers, requiring additional training and increasing the latency overhead. In this paper, we address these challenges with NEZHA, a novel architecture that achieves hyperspeed decoding for GR systems without sacrificing recommendation quality. Specifically, NEZHA integrates a nimble autoregressive draft head directly into the primary model, enabling efficient self-drafting. This design, combined with a specialized input prompt structure, preserves the integrity of sequence-to-sequence generation. Furthermore, to tackle the critical problem of hallucination, a major source of performance degradation, we introduce an efficient, model-free verifier based on a hash set. We demonstrate the effectiveness of NEZHA through extensive experiments on public datasets and have successfully deployed the system on Taobao since October 2025, driving the billion-level advertising revenue and serving hundreds of millions of daily active users."}
{"id": "2511.18874", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.MA", "cs.RO", "cs.SI"], "pdf": "https://arxiv.org/pdf/2511.18874", "abs": "https://arxiv.org/abs/2511.18874", "authors": ["Yuzhi Chen", "Yuanchang Xie", "Lei Zhao", "Pan Liu", "Yajie Zou", "Chen Wang"], "title": "GContextFormer: A global context-aware hybrid multi-head attention approach with scaled additive aggregation for multimodal trajectory prediction", "comment": null, "summary": "Multimodal trajectory prediction generates multiple plausible future trajectories to address vehicle motion uncertainty from intention ambiguity and execution variability. However, HD map-dependent models suffer from costly data acquisition, delayed updates, and vulnerability to corrupted inputs, causing prediction failures. Map-free approaches lack global context, with pairwise attention over-amplifying straight patterns while suppressing transitional patterns, resulting in motion-intention misalignment. This paper proposes GContextFormer, a plug-and-play encoder-decoder architecture with global context-aware hybrid attention and scaled additive aggregation achieving intention-aligned multimodal prediction without map reliance. The Motion-Aware Encoder builds scene-level intention prior via bounded scaled additive aggregation over mode-embedded trajectory tokens and refines per-mode representations under shared global context, mitigating inter-mode suppression and promoting intention alignment. The Hierarchical Interaction Decoder decomposes social reasoning into dual-pathway cross-attention: a standard pathway ensures uniform geometric coverage over agent-mode pairs while a neighbor-context-enhanced pathway emphasizes salient interactions, with gating module mediating their contributions to maintain coverage-focus balance. Experiments on eight highway-ramp scenarios from TOD-VT dataset show GContextFormer outperforms state-of-the-art baselines. Compared to existing transformer models, GContextFormer achieves greater robustness and concentrated improvements in high-curvature and transition zones via spatial distributions. Interpretability is achieved through motion mode distinctions and neighbor context modulation exposing reasoning attribution. The modular architecture supports extensibility toward cross-domain multimodal reasoning tasks. Source: https://fenghy-chen.github.io/sources/."}
{"id": "2511.19100", "categories": ["cs.AI", "cs.FL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19100", "abs": "https://arxiv.org/abs/2511.19100", "authors": ["Chih-Duo Hong", "Hongjian Jiang", "Anthony W. Lin", "Oliver Markgraf", "Julian Parsert", "Tony Tan"], "title": "Extracting Robust Register Automata from Neural Networks over Data Sequences", "comment": null, "summary": "Automata extraction is a method for synthesising interpretable surrogates for black-box neural models that can be analysed symbolically. Existing techniques assume a finite input alphabet, and thus are not directly applicable to data sequences drawn from continuous domains. We address this challenge with deterministic register automata (DRAs), which extend finite automata with registers that store and compare numeric values. Our main contribution is a framework for robust DRA extraction from black-box models: we develop a polynomial-time robustness checker for DRAs with a fixed number of registers, and combine it with passive and active automata learning algorithms. This combination yields surrogate DRAs with statistical robustness and equivalence guarantees. As a key application, we use the extracted automata to assess the robustness of neural networks: for a given sequence and distance metric, the DRA either certifies local robustness or produces a concrete counterexample. Experiments on recurrent neural networks and transformer architectures show that our framework reliably learns accurate automata and enables principled robustness evaluation. Overall, our results demonstrate that robust DRA extraction effectively bridges neural network interpretability and formal reasoning without requiring white-box access to the underlying network."}
{"id": "2511.19256", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19256", "abs": "https://arxiv.org/abs/2511.19256", "authors": ["Hang Ding", "Xue Wang", "Tian Zhou", "Tao Yao"], "title": "SimDiff: Simpler Yet Better Diffusion Model for Time Series Point Forecasting", "comment": "Accepted by AAAI 2026", "summary": "Diffusion models have recently shown promise in time series forecasting, particularly for probabilistic predictions. However, they often fail to achieve state-of-the-art point estimation performance compared to regression-based methods. This limitation stems from difficulties in providing sufficient contextual bias to track distribution shifts and in balancing output diversity with the stability and precision required for point forecasts. Existing diffusion-based approaches mainly focus on full-distribution modeling under probabilistic frameworks, often with likelihood maximization objectives, while paying little attention to dedicated strategies for high-accuracy point estimation. Moreover, other existing point prediction diffusion methods frequently rely on pre-trained or jointly trained mature models for contextual bias, sacrificing the generative flexibility of diffusion models.\n  To address these challenges, we propose SimDiff, a single-stage, end-to-end framework. SimDiff employs a single unified Transformer network carefully tailored to serve as both denoiser and predictor, eliminating the need for external pre-trained or jointly trained regressors. It achieves state-of-the-art point estimation performance by leveraging intrinsic output diversity and improving mean squared error accuracy through multiple inference ensembling. Key innovations, including normalization independence and the median-of-means estimator, further enhance adaptability and stability. Extensive experiments demonstrate that SimDiff significantly outperforms existing methods in time series point forecasting."}
{"id": "2511.19257", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19257", "abs": "https://arxiv.org/abs/2511.19257", "authors": ["Yingjia Shang", "Yi Liu", "Huimin Wang", "Furong Li", "Wenfang Sun", "Wu Chengyu", "Yefeng Zheng"], "title": "Medusa: Cross-Modal Transferable Adversarial Attacks on Multimodal Medical Retrieval-Augmented Generation", "comment": "Accepted at KDD 2026 First Cycle (full version). Authors marked with * contributed equally. Yi Liu is the lead author", "summary": "With the rapid advancement of retrieval-augmented vision-language models, multimodal medical retrieval-augmented generation (MMed-RAG) systems are increasingly adopted in clinical decision support. These systems enhance medical applications by performing cross-modal retrieval to integrate relevant visual and textual evidence for tasks, e.g., report generation and disease diagnosis. However, their complex architecture also introduces underexplored adversarial vulnerabilities, particularly via visual input perturbations. In this paper, we propose Medusa, a novel framework for crafting cross-modal transferable adversarial attacks on MMed-RAG systems under a black-box setting. Specifically, Medusa formulates the attack as a perturbation optimization problem, leveraging a multi-positive InfoNCE loss (MPIL) to align adversarial visual embeddings with medically plausible but malicious textual targets, thereby hijacking the retrieval process. To enhance transferability, we adopt a surrogate model ensemble and design a dual-loop optimization strategy augmented with invariant risk minimization (IRM). Extensive experiments on two real-world medical tasks, including medical report generation and disease diagnosis, demonstrate that Medusa achieves over 90% average attack success rate across various generation models and retrievers under appropriate parameter configuration, while remaining robust against four mainstream defenses, outperforming state-of-the-art baselines. Our results reveal critical vulnerabilities in the MMed-RAG systems and highlight the necessity of robustness benchmarking in safety-critical medical applications. The code and data are available at https://anonymous.4open.science/r/MMed-RAG-Attack-F05A."}
{"id": "2511.19262", "categories": ["cs.AI", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2511.19262", "abs": "https://arxiv.org/abs/2511.19262", "authors": ["Przemyslaw Chojecki"], "title": "Psychometric Tests for AI Agents and Their Moduli Space", "comment": null, "summary": "We develop a moduli-theoretic view of psychometric test batteries for AI agents and connect it explicitly to the AAI score developed previously. First, we make precise the notion of an AAI functional on a battery and set out axioms that any reasonable autonomy/general intelligence score should satisfy. Second, we show that the composite index ('AAI-Index') defined previously is a special case of our AAI functional. Third, we introduce the notion of a cognitive core of an agent relative to a battery and define the associated AAI$_{\\textrm{core}}$ score as the restriction of an AAI functional to that core. Finally, we use these notions to describe invariants of batteries under evaluation-preserving symmetries and outline how moduli of equivalent batteries are organized."}
{"id": "2511.19304", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19304", "abs": "https://arxiv.org/abs/2511.19304", "authors": ["Jiayi Zhang", "Yiran Peng", "Fanqi Kong", "Yang Cheng", "Yifan Wu", "Zhaoyang Yu", "Jinyu Xiang", "Jianhao Ruan", "Jinlin Wang", "Maojia Song", "HongZhang Liu", "Xiangru Tang", "Bang Liu", "Chenglin Wu", "Yuyu Luo"], "title": "AutoEnv: Automated Environments for Measuring Cross-Environment Agent Learning", "comment": null, "summary": "Humans naturally adapt to diverse environments by learning underlying rules across worlds with different dynamics, observations, and reward structures. In contrast, existing agents typically demonstrate improvements via self-evolving within a single domain, implicitly assuming a fixed environment distribution. Cross-environment learning has remained largely unmeasured: there is no standard collection of controllable, heterogeneous environments, nor a unified way to represent how agents learn. We address these gaps in two steps. First, we propose AutoEnv, an automated framework that treats environments as factorizable distributions over transitions, observations, and rewards, enabling low-cost (4.12 USD on average) generation of heterogeneous worlds. Using AutoEnv, we construct AutoEnv-36, a dataset of 36 environments with 358 validated levels, on which seven language models achieve 12-49% normalized reward, demonstrating the challenge of AutoEnv-36. Second, we formalize agent learning as a component-centric process driven by three stages of Selection, Optimization, and Evaluation applied to an improvable agent component. Using this formulation, we design eight learning methods and evaluate them on AutoEnv-36. Empirically, the gain of any single learning method quickly decrease as the number of environments increases, revealing that fixed learning methods do not scale across heterogeneous environments. Environment-adaptive selection of learning methods substantially improves performance but exhibits diminishing returns as the method space expands. These results highlight both the necessity and the current limitations of agent learning for scalable cross-environment generalization, and position AutoEnv and AutoEnv-36 as a testbed for studying cross-environment agent learning. The code is avaiable at https://github.com/FoundationAgents/AutoEnv."}
{"id": "2511.19314", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19314", "abs": "https://arxiv.org/abs/2511.19314", "authors": ["Jaewoo Lee", "Archiki Prasad", "Justin Chih-Yao Chen", "Zaid Khan", "Elias Stengel-Eskin", "Mohit Bansal"], "title": "PRInTS: Reward Modeling for Long-Horizon Information Seeking", "comment": "18 pages, code: https://github.com/G-JWLee/PRInTS", "summary": "Information-seeking is a core capability for AI agents, requiring them to gather and reason over tool-generated information across long trajectories. However, such multi-step information-seeking tasks remain challenging for agents backed by language models. While process reward models (PRMs) can guide agents by ranking candidate steps at test-time, existing PRMs, designed for short reasoning with binary judgment, cannot capture richer dimensions of information-seeking steps, such as tool interactions and reasoning over tool outputs, nor handle the rapidly growing context in long-horizon tasks. To address these limitations, we introduce PRInTS, a generative PRM trained with dual capabilities: (1) dense scoring based on the PRM's reasoning across multiple step quality dimensions (e.g., interpretation of tool outputs, tool call informativeness) and (2) trajectory summarization that compresses the growing context while preserving essential information for step evaluation. Extensive evaluations across FRAMES, GAIA (levels 1-3), and WebWalkerQA (easy-hard) benchmarks on multiple models, along with ablations, reveal that best-of-n sampling with PRInTS enhances information-seeking abilities of open-source models as well as specialized agents, matching or surpassing the performance of frontier models with a much smaller backbone agent and outperforming other strong reward modeling baselines."}
